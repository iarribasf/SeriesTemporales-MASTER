---
title: "Series Temporales: Procesos ARIMA"
subtitle: "Máster de Bioestadística (Modelización Estadística)"
author: "Iván Arribas (Depto. Análisis Económico. Universitat de València)"
toc: true
toc-title: Índice
number-sections: true
bibliography: references.bib
crossref:
  fig-title: Figura
  tbl-title: Tabla
  fig-prefix: Figura
  tbl-prefix: Tabla
---

```{r}
#| label: chunk_setup
#| echo: false
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      comment = "",
                      fig.align = "center", 
                      fig.show = "hold",
                      fig.height = 4,
                      fig.width = 8,
                      out.width = "80%") 
```

```{r}
#| label: librerias
#| echo: false
library(forecast)
library(ggplot2); theme_set(theme_bw())
library(tseries)
library(lmtest)
```


# Introducción

Los **modelos ARIMA** han mostrado ser uno se los métodos de ajuste de series temporales más valiosos desde que fueran formalizados en 1976 por Box y Jenkins [@BoxJenkins76]. Además, dieron las pautas a seguir en el ajuste de una serie temporal para alcanzar buenas predicciones (véase epígrafe 5).

En este tema y el siguiente, definiremos estos procesos y aprenderemos a identificarlos, estimarlos y hacer predicciones.

**Los modelos ARIMA son ahora el tronco de una amplia familia de procesos** que requieren menos hipótesis para su aplicación: ARCH, GARCH, NGARCH,  IGARCH, EGARCH, GARCH-M, QGARCH, GJR-GARCH, TGARCH, fGARCH...

**Los modelos ARIMA y los métodos de Alisado Exponencial son complementarios**:

* Los modelos de Alisado lineales son casos especiales de modelos Arima,
* Los modelos de Alisado no lineales no tienen su contrapartida en modelos Arima
* Muchos modelos Arima no tiene contrapartida en los modelos de Alisado.  

\
\

# Operador Retardo

Definimos el **operador retardo** $L$ como $Ly_t = y_{t-1}$, es decir, retrasa un periodo la serie. En inglés se denomina *lag operator* (L) o *backward shift* (B)

Así, se tiene que
$$L^k y_t  = y_{t-k}.$$
y por tanto que

$$
\begin{aligned}
  \nabla y_t & =  y_t - y_{t-1} = y_t - Ly_t = (1-L)y_t \\
  \nabla^d y_t & =  (1-L)^d y_t \\
  \nabla_m y_t & =  (1-L^m) y_t \\
  \nabla_m^D y_t & =  (1-L^m)^D y_t
\end{aligned}
$$

La @tbl-operadorretardo muestra un sencillo ejemplo del efecto del operador retardo sobre la serie $y_t$

```{r}
#| echo: false
#| label: tbl-operadorretardo
#| tbl-cap: "Ejemplo de aplicación del operador retardo"
knitr::kable(data.frame(y = 1:7, lag1_y = c(NA, 1:6), lag2_y = c(NA,NA,1:5), lag3_y = c(NA,NA,NA,1:4)))
```

\
\

# Hipótesis

\

## Sobre el proceso estocástico

A lo largo de este tema asumiremos que:

* $\{y_t\}_{t=1}^T$ es una realización de un proceso estocástico desconocido.
    
* El proceso estocástico es **estacionario en sentido amplio**:
$$E[y_t]  = \mu < \infty \;\;\; \forall t,$$
$$Cov[y_t, y_{t-k}]  = \gamma_k  \;\;\; \forall k.$$
     
* El proceso estocástico es **ergódico**, o su condición suficiente: 
$$\lim_{k \rightarrow \infty} \gamma_k  = 0.$$

\

## Sobre el vector de residuos

También asumiremos que el residuo del modelo $\{\varepsilon_t\}_{t=1}^T$ es **ruido blanco**:

* Media cero: $E[\varepsilon_t]=0$

\vspace{0.3cm}

* Varianza constante (homocedástico): $E[\varepsilon_t^2]=\sigma^2$

\vspace{0.3cm}

* Incorrelación: $E[\varepsilon_t \cdot \varepsilon_{s}]=0 \;\;\; t \neq s$

\vspace{0.3cm}

* Distribución Normal: $\varepsilon_t \sim N$

\vspace{0.3cm}

Es decir, $\varepsilon_t \sim N(0,\sigma^2)$ i.i.d.

\
\

# Procesos ARIMA

ARIMA surge de combinar las siglas de tres procesos diferentes: **AR** de AutoRegresive, **I** de Integrated y **MA** de Moving Average. Veamos cada uno de estos tres conceptos por separado y luego su combinación. 

\

## Procesos autorregresivos AR(p)

### Definición {.unnumbered}

El modelo general **autorregresivo de orden p**, $y_t \sim AR(p)$ viene definido por
$$y_t=c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \varepsilon_t,$$
que usando el operador retardo queda
$$(1 - \phi_1 L - \phi_2 L^2 - \ldots - \phi_p L^p)y_t = c + \varepsilon_t$$

### Propiedades {.unnumbered}

El proceso es **estacionario** si quedan fuera del círculo de radio la unidad todas las raíces del polinomio autorregresivo
$$\Phi_p(z) = 1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p.$$

Es **invertible** siempre.

* Podemos transformar el proceso AR(p) en un proceso donde $y_t$ depende de la suma infinita de errores pasados, MA($\infty$).
* Si conocemos las p primeras autocorrelaciones, podemos estimar los p parámetros del modelo. Por ejemplo, para un proceso AR(2) se verifica que:
$$\rho_1 = \phi_1 + \phi_2 \rho_1$$
$$\rho_2 = \phi_1 \rho_1 + \phi_2$$

  Estas ecuaciones se denominan **Ecuaciones de Yule-Walker**.
  
  Observa que si tenemos una estimación de las dos primeras autocorrelaciones, estas ecuaciones nos permiten obtener una estimación de los coeficientes del proceso AR(2) como una aplicación del método de los momentos.

**Sobre todo**,

* La FAC del proceso decae *exponencialmente* a partir del orden p
* La FACP verifica que los p primeros valores son no nulos y todos los demás valen cero.

### Ejemplos {.unnumbered}

* $y_t \sim AR(1): \;\;y_t = c + \phi_1 y_{t-1} + \varepsilon_t$ o $(1 - \phi_1 L)y_t = c + \varepsilon_t$

* $y_t \sim AR(2): \;\;y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \varepsilon_t$ o $(1 - \phi_1 L - \phi_2 L^2)y_t = c + \varepsilon_t$

### Simulación de procesos autorregresivos {.unnumbered}

La @fig-simulAR muestra dos simulaciones del proceso AR(1) $y_t = 0.8y_{t-1} + \varepsilon_t$, el panel superior con 20 datos y el inferior con 100 datos. En ambos casos $\varepsilon_t$ se distribuye como una normal con media cero y varianza la unidad. (Todas las simulaciones se han realizado con la función `arima.sim` de la librería `stats`.)

```{r}
#| echo: false
set.seed(111116)
y20 <- arima.sim(n = 20, list(ar = 0.8), innov = rnorm(20))
y100 <- arima.sim(n = 100, list(ar = 0.8), innov = rnorm(100))
y100a <- arima.sim(n = 100, list(ar = c(0.5, 0.4)), innov = rnorm(100))
y100b <- arima.sim(n = 100, list(ar = c(0.9, -0.5)), innov = rnorm(100))
```

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-simulAR
#| fig-cap: "Simulación de dos procesos AR(1) con diferente tamaño muestral"
#| fig-subcap: 
#|   - "n = 20"
#|   - "n = 100"
#| layout-nrow: 2
ggtsdisplay(y20)
ggtsdisplay(y100)
```

\

## Procesos en medias móviles MA(q)

### Definición {.unnumbered}

El modelo general **en medias móviles de orden q**, $y_t \sim MA(q)$ viene definido por
$$y_t=c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots + \theta_q \varepsilon_{t-q},$$
que usando el operador retardo queda
$$y_t = c + (1 + \theta_1 L + \theta_2 L^2 + \ldots + \theta_q L^q) \varepsilon_t$$

### Propiedades {.unnumbered}

El proceso es **invertible** si quedan fuera del círculo de radio la unidad todas las raíces del polinomio en medias móviles
$$\Theta_q(z) = 1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q.$$

* Podemos transformar el proceso MA(q) en un proceso AR($\infty$).
* Si conocemos las q primeras autocorrelaciones, podemos estimar los q parámetros del modelo. Por ejemplo, para un proceso MA(2) se verifica que:
$$\rho_1 = \frac{\theta_1 + \theta_1\theta_2}{1 + \theta_1^2 + \theta_2^2}$$
$$\rho_2 = \frac{\theta_2}{1 + \theta_1^2 + \theta_2^2}$$

Es **estacionario** siempre.
     
**Sobre todo**,

* La FAC verifica que los q primeros valores son no nulos y todos los demás valen cero.
* La FACP decae *exponencialmente* a partir del orden q.

### Ejemplos {.unnumbered}

* $y_t \sim MA(1): \;\;y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1}$ o $y_t = c + (1 + \theta_1 L)\varepsilon_t$

* $y_t \sim MA(2): \;\;y_t=c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2}$ o $y_t = c + (1 + \theta_1 L + \theta_2 L^2)\varepsilon_t$

### Simulación de procesos en medias móviles {.unnumbered}

La @fig-simulMA ofrece dos simulaciones del proceso MA(1) $y_t = 0.8\varepsilon_{t-1} + \varepsilon_t$, la primera con 20 datos y la segunda con 100 datos. En ambos casos $\varepsilon_t$ se distribuye como una normal con media cero y varianza la unidad.

```{r}
#| echo: false
set.seed(654321)
y20<-arima.sim(n=20,list(ma=0.8), innov=rnorm(20))
y100<-arima.sim(n=100,list(ma=0.8), innov=rnorm(100))
y100a <- arima.sim(n = 100, list(ma = c(0.5, 0.4)), innov = rnorm(100))
y100b <- arima.sim(n = 100, list(ma = c(0.9, -0.5)), innov = rnorm(100))
```

```{r}
#| echo: false
#| fig-height: 4
#| label: fig-simulMA
#| fig-cap: "Simulación de dos procesos MA(1) con diferente tamaño muestral"
#| fig-subcap: 
#|   - "n = 20"
#|   - "n = 100"
#| layout-nrow: 2
ggtsdisplay(y20)
ggtsdisplay(y100)
```   

\

## Procesos ARMA(p,q)

### Definición {.unnumbered}

El modelo general $y_t \sim ARMA(p,q)$ viene dado por
$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p}  + 
        \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \ldots +
        \theta_q \varepsilon_{t-q}+ \varepsilon_t,$$
que usando el operador retardo queda
$$(1 - \phi_1 L - \ldots - \phi_p L^p)y_t = c + (1 + \theta_1 L + \ldots + \theta_q L^q) \varepsilon_t.$$

El proceso más simple es el ARMA(1,1), $y_t = c  + \phi_1 y_{t-1} + \theta_1 \varepsilon_{t-1} + \varepsilon_{t}$.


### Propiedades {.unnumbered}

El proceso es **estacionario** si quedan fuera del círculo de radio la unidad todas las raíces del polinomio
$$\Phi_p(z) = 1 - \phi_1 z - \phi_2 z^2 - \ldots - \phi_p z^p.$$
El proceso es **invertible** si quedan fuera del círculo de radio la unidad todas las raíces del polinomio
$$\Theta_q(z) = 1 + \theta_1 z + \theta_2 z^2 + \ldots + \theta_q z^q.$$
**Sobre todo**,

* La FAC decae exponencialmente a partir del orden p.
* La FACP decae exponencialmente a partir del orden q.

### Ejemplos {.unnumbered}

* $y_t \sim ARMA(1, 1): \;\;y_t = c  + \phi_1 y_{t-1} + \theta_1 \varepsilon_{t-1} + \varepsilon_{t}$ o $(1 - \phi_1 L)y_t = c + (1 + \theta_1 L)\varepsilon_t$.

* $y_t \sim ARMA(0, 0): \;\;y_t = c + \varepsilon_{t}$. Si $c = 0$, a este proceso se le denomina **ruido blanco**.


### Simulación de procesos ARMA {.unnumbered}

La @fig-simulARMA1 muestra las FAC y FACP de una simulación de tamaño 200 para un proceso ARMA(1,1), la @fig-simulARMA2 para un proceso ARMA(2,1) y la @fig-simulARMA3 para un ARMA(1,2). En todos los casos $\varepsilon_t$ se distribuye como una normal con media cero y varianza la unidad.

```{r}
#| echo: false
set.seed(654321)
y200a <- arima.sim(n = 200,list(ar = 0.7, ma = 0.6), innov = rnorm(200))
y200b <- arima.sim(n = 200,list(ar=c(0.7, -0.5), ma = 0.6), innov = rnorm(200))
y200c <- arima.sim(n = 200,list(ar = 0.6, ma = c(0.5, 0.3)), innov = rnorm(200))
```

$y_t = 0.7y_{t-1} + 0.6\varepsilon_{t-1} + \varepsilon_t$

```{r}
#| echo: false
#| fig-height: 3
#| label: fig-simulARMA1
#| fig-cap: "Simulación de un proceso ARMA(1, 1)"
#| fig-subcap: 
#|   - "Función de autocorrelación"
#|   - "Función de autocorrelación parcial"
#| layout-nrow: 1
ggAcf(y200a,  main = "", lag = 10, xlab = "Retardo", ylab = "")
ggPacf(y200a, main = "", lag = 10, xlab = "Retardo", ylab = "")
```

$y_t = 0.7y_{t-1}- 0.5y_{t-2} + 0.6\varepsilon_{t-1} + \varepsilon_t$

```{r}
#| echo: false
#| fig-height: 3
#| label: fig-simulARMA2
#| fig-cap: "Simulación de un proceso ARMA(2, 1)"
#| fig-subcap: 
#|   - "Función de autocorrelación"
#|   - "Función de autocorrelación parcial"
#| layout-nrow: 1
ggAcf(y200b,  main = "", lag = 10, xlab = "Retardo", ylab = "")
ggPacf(y200b, main = "", lag = 10, xlab = "Retardo", ylab = "")
```

$y_t = 0.6y_{t-1} + 0.5\varepsilon_{t-1} + 0.3\varepsilon_{t-2} + \varepsilon_t$

```{r}
#| echo: false
#| fig-height: 3
#| label: fig-simulARMA3
#| fig-cap: "Simulación de un proceso ARMA(2, 1)"
#| fig-subcap: 
#|   - "Función de autocorrelación"
#|   - "Función de autocorrelación parcial"
#| layout-nrow: 1
ggAcf(y200c,  main = "", lag = 10, xlab = "Retardo", ylab = "")
ggPacf(y200c, main = "", lag = 10, xlab = "Retardo", ylab = "")
```  

\

## Proceso ARIMA(p,d,q)

Si la serie $y_t$ no es estacionaria pero tras diferenciarla $d$ veces se hace estacionaria, diremos que la serie es integrada de orden $d$: $y_t \sim I(d)$. Por tanto,

* una serie estacionaria se indicará como $y_t \sim I(0)$
* $y_t \sim I(d)$ es equivalente a $\nabla^d y_t = (1 - L)^d y_t \sim I(0)$

Una serie $y_t$ sigue un proceso **$ARIMA(p,d,q)$** si:

1.  hay que diferenciar la serie $d$ veces para hacerla estacionaria, $y_t \sim I(d)$; y
2.  la serie diferenciada sigue un proceso ARMA(p,q), $\nabla^d y_t \sim ARMA(p,q)$.
      
Entonces, podemos escribir: 
\begin{equation*}
\begin{array}{c@{\qquad}c@{\quad}ccc}
  y_t \sim  ARIMA(p,d,q): & (1 - \phi_1 L - \ldots - \phi_p L^p) & (1- L)^d y_t & = & c + (1 + \theta_1 L + \ldots + \theta_q L^q) \varepsilon_t \\ 
                          & \uparrow                            & \uparrow      &   & \uparrow \\
                          & AR(p)                               & I(d)          &   & MA(q) 
\end{array}
\end{equation*}


### Ejemplos {.unnumbered}

* $y_t \sim ARIMA(1, 1, 1)$:
$$
\begin{aligned}
(1 - \phi_1 L)(1- L) y_t & = c + (1 + \theta_1 L) \varepsilon_t \\
y_t & = c + y_{t-1} + \phi_1(y_{t-1} - y_{t-2}) + \theta_1 \varepsilon_{t-1} + \varepsilon_t
\end{aligned}
$$

* $\log(y_t) \sim ARIMA(1, 1, 1)$:
$$
\begin{aligned}
(1 - \phi_1 L)(1- L) \log(y_t) & = (1 - \phi_1 L)TVy_t = c + (1 + \theta_1 L) \varepsilon_t \\
TVy_t & = c + \phi_1 TVy_{t-1} + \theta_1 \varepsilon_{t-1} + \varepsilon_t
\end{aligned}
$$

* $y_t \sim ARIMA(0, 1, 0)$:
$$
\begin{aligned}
(1- L) y_t & = c + \varepsilon_t \\
y_t & = c + y_{t-1} + \varepsilon_t. 
\end{aligned}
$$
\noindent Si $c=0$, tenemos un **paseo aleatorio**; si $c \neq 0$, tenemos un **paseo aleatorio con deriva**.

### Simulación de procesos no estacionarios {.unnumbered}

Veamos la FAC de tres series simuladas no estacionarias (@fig-simulARIMA): un paseo aleatorio, un paseo aleatorio con deriva y un modelo lineal. Sus FAC son indistinguibles, pero los tres casos revelan claramente su carácter no estacionario.

```{r}
#| echo: false
y1 <- cumsum(rnorm(200))
y2 <- 1:200
```

```{r}
#| echo: false
#| fig-height: 2
#| label: fig-simulARIMA
#| fig-cap: "FAC para tres procesos no estacionarios simulados"
#| fig-subcap: 
#|   - "FAC de un paseo aleatorio"
#|   - "FAC de un paseo aleatorio con deriva"
#|   - "FAC de in proceso determinista lineal"
#| layout-nrow: 3
ggAcf(y1,      main = "", lag = 10, xlab = "", ylab = "")
ggAcf(y1 + y2, main = "", lag = 10, xlab = "", ylab = "")
ggAcf(y2,      main = "", lag = 10, xlab = "", ylab = "")
``` 

\
\

# Aproximación de Box-Jenkins

La @fig-AproximaciónBoxJenkins muestra el flujo de procesos asociado a la modelización por modelos ARIMA, con cuatro grandes áreas:

-   **Identificación**, que requiere primero transformar la serie para que sea estacionaria y ergódica, para después identificar los valores de p y q.

    La FAC y la función de autocorrelación parcial teóricas son diferentes en cada tipo de proceso. Por tanto, idealmente estas funciones podría servir de ayuda en la identificación del modelo. En la práctica, la funciones estimadas son tan diferentes de las teóricas que resultan de muy poca ayuda.

    Nosotros haremos uso de algunas funciones de *auto* identificación que nos ayudaran en este punto.

-   **Estimación** de los parámetros del modelo, incluidas las variables de intervención. El método usual de estimación de los parámetros es por máxima verosimilitud.

-   **Validación** de las hipótesis sobre el modelo. Analizaremos que no es necesaria más intervención y veremos la pertinencia de los parámetros del modelo (bien contrastando su significatividad o bien por alguna regla más sencilla).

    Si la validación no se pasa, puede ser necesario volver al proceso inicial y realizar una nueva identificación del modelo.

-   **Predicción** e interpretación del modelo válido. Si las predicciones se alejan de los valores reales más de lo esperado o presentan sesgo, puede ser necesario identificar y estimar un nuevo modelo.

::: {#fig-AproximaciónBoxJenkins}
![](./imagenes/BoxJenkins.png)

Esquema de la aproximación de Box-Jenkins a la modelización de procesos ARIMA
:::

### Identificación automática {.unnumbered}

El paquete `forecast` dispone de la función `auto.arima()` que localiza el mejor modelo basándose en el AIC corregido para pequeñas muestras (`AICc`). No hay que fiarse ciegamente de los resultados de esta función, pero ayuda en la identificación. Básicamente el algoritmo seguido es el siguiente:

1. Determina el orden de diferenciación regular $0 \leq d \leq 2$ usando la función `ndiffs`, que usa la prueba KPSS repetidas veces. 
2. Tras diferenciar la serie:
    * se estiman una serie de modelos básicos predeterminados. 
    * se usa el criterio AICc para seleccionar el mejor de estos modelos. 
    * a partir del modelo seleccionado, se hacen pequeñas variaciones modificando en una unidad *p* y *q* y añadiendo/quitando la constante y se vuelve a seleccionar el mejor de los nuevos modelos.
3. Se repite el paso 2 hasta que no se puede mejorar el AICc. 

Cuando usemos esta función, debemos tener cuenta que:

* La función `auto.arima` no permite contante si la suma de las diferenciaciones es 2 o superior. 
* Si se desea hacer una búsqueda exhaustiva entre todos los posibles modelos se debe usar el argumento `stepwise = FALSE`.
* Si se desea que el cálculo de AICc sea exacto (por defecto para ganar tiempo calcula una aproximación), se debe usar el argumento `approximation = FALSE`.
* Si se desea ver para todos los modelos analizados el valor de AICc, se debe incluir el argumento `trace = TRUE`.

La función `auto.arima` tiende a sobreparametrizar los modelos y es muy recomendable *ayudarla* indicando las diferenciaciones y la posible intervención. 

\
\

# Ejemplos

\

## Recogida de Residuos

Vamos a aplicar la metodología de Box-Jenkins a la serie Residuos, una *serie anual* de 1995 a 2022 (fuente [Instituto Nacional de Estadística](https://www.ine.es)) que muestra los residuos recogidos por o en nombre de las autoridades municipales y eliminados a través del sistema de gestión de residuos. La unidad es el kg per cápita. 

```{r}
#| label: fig-Residuos
#| fig-cap: "Recogida de residuos"
residuos <- read.csv2("./series/Residuos.csv", 
                      header = TRUE)

residuos <- ts(residuos[, 2], 
               start = 1995, 
               frequency  = 1)

autoplot(residuos,
         xlab = "", 
         ylab = "Kg per cápita", 
         main = "")
```

### Transformación de la serie {.unnumbered}

El primer paso es transformar la serie original para que sea estacionaria. La @fig-ResiduosFAC muestra la gráfica temporal y la FAC para la serie original y su primera diferencia, y la función `ndiffs` usa un contraste de raíces unitarias para determinar el número de diferencias necesarias para que la serie sea estacionaria. Tras su análisis, podemos concluir que es necesario diferenciar la serie una vez. Es decir, $d=1$ o $residuos_t \sim I(1)$.

```{r}
#| eval: false
autoplot(residuos, xlab = "", ylab = "", main = "")
autoplot(diff(residuos), xlab = "", ylab = "", main = "")
ggAcf(residuos, xlab = "", ylab = "", main = "")
ggAcf(diff(residuos), xlab = "", ylab = "", main = "")
```

```{r}
#| echo: false
#| label: fig-ResiduosFAC
#| fig-cap: "Gráfica y FAC para Residuos"
#| fig-subcap: 
#|   - "Serie original"
#|   - "Dif.  de la serie"
#|   - "FAC serie original"
#|   - "FAC dif. de la serie"
#| layout: [[50, 50], [50, 50]]
autoplot(residuos, xlab = "", ylab = "", main = "")
autoplot(diff(residuos), xlab = "", ylab = "", main = "")
ggAcf(residuos, xlab = "", ylab = "", main = "")
ggAcf(diff(residuos), xlab = "", ylab = "", main = "")
```

```{r}
ndiffs(residuos)
```

### Identificación {.unnumbered}

Tras diferenciar la serie, vamos a identificar los valores de $p$ y $q$ a partir de las FAC y FACP de la serie diferenciada (@fig-ResiduosDisplay). 

```{r} 
#| label: fig-ResiduosDisplay
#| fig-cap: "Residuos (primera diferencia"
ggtsdisplay(diff(residuos), main = "")
```

Observamos que tanto en la FAC como en la FACP solo la primera autocorrelación sobrepasa las líneas que marcan el intervalo de confianza al 95%. podemos estar antes un proceso AR(1, 1, 0), MA(1, 1, 0) o ARIMA(1,1,1). Si queremos hilar fino, podemos aventurar que la FAC presenta decrecimiento y apostar por un proceso AR(1, 1, 0).

También podemos ayudarnos de la función `auto.arima`, fijando el número de diferenciaciones con `d = 1`. El argumento `trace = TRUE` sirve para que en la salida se muestren todos los modelos que se han probado.

```{r}
auto.arima(residuos, 
           d = 1,
           trace = TRUE)
```

La identificación automática da como mejor modelo $p=1$ y $q=0$ sin constante. Además, observa como el segundo mejor modelo (según AICc) es ARIMA(0, 1, 1) y el tercero un ARIMA(1, 1, 1).

Vamos a asumir que $residuos_t \sim ARIMA(1,1,0)$ sin deriva (sin constante):
$$
\begin{aligned}
(1 - \phi_1 L)(1 - L)residuos_t & = \varepsilon_t \\
residuos_t &= residuos_{t-1} + \phi_1(residuos_{t-1} - residuos_{t-2}) + \varepsilon_t
\end{aligned}
$$

### Estimación {.unnumbered}

Aunque existe la función `arima` de `stats`, vamos a usar la función `Arima` de la librería `forecast` para estimar el modelo identificado por ser más versátil. El argumento `order` indica los valores de (p, d, q) como un vector y el argumento lógico `include.constant` indica si debe incluirse la constante $c$ en el modelo.[^1].

[^1]: Mira en la ayuda de la función `Arima` la diferencia entre los argumentos `include.mean`, `include.drift` e `include.constant`

```{r}
arima110 <- Arima(residuos, 
                  order = c(1, 1, 0), 
                  include.constant = FALSE)
arima110
```

Una forma rápida, aunque imprecisa, de determinar si un coeficiente es relevante (significativo) es compararlo con su error estándar (standard error, s.e). Si el coeficiente es mayor que dos veces su error estándar, hay evidencia de que es significativo. En la salida de `R`, en la tabla `Coefficients` tienes en la primera fila el nombre de los coeficientes; su valor estimado aparece en la segunda fila de la tabla; y los errores estándar en la tercera fila (precedidos por s.e.). Con esta regla, parece que el coeficiente $\phi_1$ (`ar1` en la salida) es significativo.

### Intervención {.unnumbered}

Se analiza si para algún año se observa un error atípico (por ejemplo 3 veces superior al error estándar). La @fig-ResiduosError muestra que en los años 1999 y 2004, el residuo sobrepasa los dos errores estándar pero queda lejos de los tres errores estándar así que asumiremos que no hay valores atípicos.

```{r}
#| label: fig-ResiduosError
#| fig-cap: "Error + Intervención"
error <- residuals(arima110)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1995, 2023, 2)) 
```

### Validación {.unnumbered}

**Coeficientes signifciativos**

A fin de poner un poco de *objetividad* en la decisión de si un coeficiente es significativo (distinto de cero), podemos usar una prueba estadística. Existen varias alternativas (prueba z, prueba t, prueba de Wald...), así que vamos a optar por la más sencilla y cómoda, la prueba z. Esta prueba asume normalidad asintótica en la distribución de los coeficientes o, al menos, una muestra suficientemente grande.

Para implementar la prueba z usaremos la función `coeftest` (se precisa la librería `lmtest`) que contrasta individualmente si cada coeficiente de un modelo es significativo. Esta función requiere por defecto un solo argumento, el que contiene la estimación del modelo Arima.

```{r}
coeftest(arima110)
```

Como el valor de p es mayor que el nivel de significatividad $\alpha = 0.05$, se concluye que $\phi_1$ es significativo. 


**Media cero**

No se puede contrastar si el residuo tiene **media cero**, pero el error medio es $ME=$ `r round(accuracy(arima110)[1],2)`, relativamente bajo en comparación con el valor medio de la serie.

Además, tenemos las diferentes medidas de bondad del ajuste. En media nos equivocamos en `r round(accuracy(arima110)[2],0)` kg per cápita (RMSE) y el error porcentual medio (MAPE) es `r round(accuracy(arima110)[5],1)`%. El modelo no presenta sesgo y los intervalos de confianza están correctamente calculados.

```{r}
#| eval: false
accuracy(arima110)
```

```{r}
#| echo: false
round(accuracy(arima110),2)
```
  
\

**Incorrelación**

Lo veremos con el test de Box-Ljung

* La hipótesis nula es $H_0: \rho_1 = ... = \rho_k = 0$
* El valor de p = `r round(Box.test(residuals(arima110), lag = 2,type = "Ljung-Box")$p.value, 3)` es mayor que el nivel de significatividad 0.05. No se rechaza la hipótesis de incorrelación, hasta el orden $k = 2$.

```{r}
error <- residuals(arima110)
Box.test(error, lag = 2,type = "Ljung-Box")
```
    
La elección de dos retardos para la prueba, fijado con el parámetros `lag = 2`, es bastante arbitraria. Sería mejor realizar la prueba para un rango de valores de $k$ (véase @tbl-residuosInco).
    
```{r}
#| echo: false
#| label: tbl-residuosInco
#| tbl-cap: "Prueba de incorrelacion para diferentes valores del retardo"
pp <- rep(0,4)
for (i in 1:4) pp[i] <- round(Box.test(error, lag = i, type = "Ljung-Box")$p.value, 3)
tmp <- data.frame(k = 1:4, "valor de p" = pp)
knitr::kable(tmp)
```

\

**Homocedasticidad (varianza constante)**

Lo veremos con el test de Box-Ljung para el residuo al cuadrado. La hipótesis nula seria que las primeras $k$ autocorrelaciones estimadas sobre el cuadrado del residuo son cero.

El valor de p = `r round(Box.test(error^2, lag = 2,type = "Ljung-Box")$p.value, 3)` es mayor que el nivel de significatividad 0.05. No se rechaza la  hipótesis de homocedasticidad, hasta el orden 2.

```{r}
Box.test(error^2, lag = 2, type = "Ljung-Box")
```

De nuevo, la elección de dos retardos es totalmente arbitraria y sería mejor realizar la prueba para un rango de valores de $k$ (véase @tbl-residuosHomo).
    
```{r}
#| echo: false
#| label: tbl-residuosHomo
#| tbl-cap: "Prueba de homocedasticidad para diferentes valores del retardo"
pp <- rep(0,4)
for (i in 1:4) pp[i] <- round(Box.test(error^2, lag = i, type = "Ljung-Box")$p.value, 3)
tmp <- data.frame(k = 1:4, "valor de p" = pp)
knitr::kable(tmp)
```

\

**Normalidad**

Recuerda que todos las pruebas de normalidad son muy sensibles al tamaño de la muestra. Siempre es recomendable empezar por un análisis gráfico (histograma, gráfico PP, gráfico QQ).

Sin embargo, cuando es necesario un criterio más objetivo o se precisa de un proceso automático, entonces si la muestra es reducida (30 a 50 observaciones según autores) se aplica la prueba de Shapiro-Wilk; en otro caso se aplica la prueba de Jarque-Bera (`tseries`) o Kolmogorov-Smirnov. En nuestro ejemplo, con 26 datos, lo correcto es aplicar la prueba de Shapiro-Wilk.

```{r}
shapiro.test(error)
```

\

### Predicción {.unnumbered}

Una vez validado el modelo podemos pasar a realizar predicciones, en este caso a 5 años vista.

```{r}
parima110 <- forecast(arima110, 
                      h = 5, 
                      level = 95)
parima110
```

```{r}
#| label: fig-ResiduosPre
#| fig-cap: "Residuos (1999-2022) y predicción (2023-2027)"
autoplot(parima110, 
         xlab = "", 
         ylab = "Kg per cápita",
         main = "") +
  scale_x_continuous(breaks= seq(1999, 2027, 2)) 
```

La @fig-ResiduosPre muestra la serie, la previsión y el intervalo de confianza al 95%. La predicción es constante e igual al último dato. En las series diferenciadas el intervalo de confianza de las predicciones crece muy rápidamente porque los errores se van acumulando sin ningún tipo de amortiguamiento.

Anecdóticamente, para este ejemplo las predicciones son constantes porque el volumen de residuos en los dos últimos años de la serie ha sido exactamente el mismo. Si te fijas en la ecuación de predicción, este hecho hace que la predicción de un año sea el valor de la serie para el año previo.

  
\

## Aforo de vehículos

Vamos a aplicar de nuevo la metodología de Box-Jenkins a la serie **aforo de vehículos** por Oropesa, carretera N-340, km. 996,48 (fuente Ministerio de Fomento). La serie es anual de 1960 a 2023 (64 datos) y ofrece el número medio diario de vehículos que pasan por esta carretera.
    
```{r}
#| label: fig-Aforo
#| fig-cap: "Aforo de vehículos en N-340, Oropesa"
aforo <- read.csv2("./series/Aforo_oropesa.csv", 
                   header = TRUE)
aforo <- ts(aforo, 
            start = 1960, 
            freq = 1)

autoplot(aforo, 
         xlab = "", 
         ylab = "Vehículos diarios",
         main = "")
```

Los puntos de inflexión de la tendencia de la serie aforo en la carretera N-340 están muy relacionados con la autopista AP-7 y las crisis ocurridas en España: la caída del aforo en 1979 se debe a la inauguración en 1978 del tramo de la AP-7 Torreblanca - Castellón; la caída en 2009 a la crisis financiera que llevó a la Gran Recesión; y la caída en 2020 a la crisis económica originada por la pandemia de la Covid-19.

En este ejemplo incluiremos, por primera vez, intervención y veremos como **la presencia de valores atípicos puede distorsionar el proceso de identificación**. Por ello, es conveniente realizar en paralelo ambas actividades, identificar el proceso y detectar valores atípicos.
      
### Transformación de la serie {.unnumbered}

La @fig-AforoFAC muestra que la serie Aforo no es estacionaria. por lo que el primer paso es transformar la serie original para que lo sea. La serie no es estacionaria, pero sí lo es su primera diferencia. Ten siempre presente que diferenciar más veces de las necesarias puede dificultar la identificación y la interpretación. Por otro lado, aunque la función `ndiffs` aconseja dos diferenciaciones, nada apunta a que esta sea una buena decisión. Así, optamos por fijar $d = 1$.

```{r}
#| eval: false
autoplot(aforo, xlab = "", ylab = "", main = "")
autoplot(diff(aforo), xlab = "", ylab = "", main = "")
ggAcf(aforo, xlab = "", ylab = "", main = "")
ggAcf(diff(aforo), xlab = "", ylab = "", main = "")
```

```{r}
#| echo: false
#| label: fig-AforoFAC
#| fig-cap: "Gráfica y FAC para Aforo"
#| fig-subcap: 
#|   - "Serie original"
#|   - "Dif.  de la serie"
#|   - "FAC serie original"
#|   - "FAC dif. de la serie"
#| layout: [[50, 50], [50, 50]]
autoplot(aforo, xlab = "", ylab = "", main = "")
autoplot(diff(aforo), xlab = "", ylab = "", main = "")
ggAcf(aforo, xlab = "", ylab = "", main = "")
ggAcf(diff(aforo), xlab = "", ylab = "", main = "")
```

```{r}
ndiffs(aforo)
```


### Identificación y estimación {.unnumbered}

Veamos ahora a identificar los valores de $p$ y $q$ a partir de la FAC y la FACP, que se muestran en la @fig-AforoDisplay. Ni para la FAC ni para la FACP se observan coeficientes fuera del intervalo de confianza (líneas azules de las gráficas). Podría tratarse de un proceso ARIMA(0, 1, 0).

```{r}
#| label: fig-AforoDisplay
#| fig-cap: "Aforo (una diferencia)"
ggtsdisplay(diff(aforo))
```

¿Qué recomienda `auto.arima`? 

```{r} 
auto.arima(aforo, 
           d = 1)
```


Sugiere un proceso ARIMA(0,1,0). Vamos a ver la gráfica de los residuos del modelo para identificar los valores extremos (intervención).

```{r}
#| label: fig-AforoError
#| fig-cap: "Error + Intervención"
arima010 <- Arima(aforo, 
                  order = c(0, 1, 0),
                  include.constant = FALSE)

error <- residuals(arima010)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1960, 2024, 4)) 

fechas <- format(seq(as.Date("1960-01-01"), as.Date("2023-01-01"), "year"), "%Y")
fechas[abs(error) > 2.5 * sderror]
```

Se identifican tres posibles valores extremos, tres intervenciones, en los años 1979, 2011 y 2020 (el error supera las 2.5 desviaciones típicas). Cada una de las intervenciones es del tipo *pulso* porque solo afecta un periodo de la serie.

Ahora, creamos una variable ficticia asociada a cada intervención, que denominaremos d1979, d2011 y d2020. La forma de definir la variable ficticia asociada a un pulso consiste en crear una variable de ceros, excepto para el periodo atípico en que la variable valdrá 1.

```{r}
d1979 <- 1*(time(error) == 1979)
d2011 <- 1*(time(error) == 2011)
d2020 <- 1*(time(error) == 2020)
```

Por último, incluimos las tres variables ficticias en la autoidentificación.

```{r}
auto.arima(aforo,
           d = 1,
           xreg = cbind(d1979,  d2011, d2020))
```

Observa como la inclusión de intervención modifica la autoidentificación, que ahora es un proceso ARIMA(2, 1, 0).

```{r}
arima210 <- Arima(aforo, 
                  order = c(2, 1, 0), 
                  xreg = cbind(d1979,  d2011, d2020))
arima210
```

La @fig-AforoError2 muestra que para ningún año se observa un error atípico. Es decir, no es necesaria más intervención.

```{r}
#| label: fig-AforoError2
#| fig-cap: "Error + Intervención"
error <- residuals(arima210)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1960, 2024, 4)) 
```


### Validación {.unnumbered}

**Variables significativas**

La identificación de errores atípicos ---para la posterior inclusión de sus variables de intervención asociadas--- ha sido un tanto arbitraria: ¿es atípico el error que supera las 2 desviaciones típicas, las dos y media, las tres desviaciones típicas?

De nuevo, podemos contrastar si sus coeficientes son significativos y dejar solo aquellas variables de intervención cuyo coeficientes lo sean. Aunque si la serie es suficientemente larga, también podríamos saltarnos este paso y dejar las variables de intervención que mejoren las predicciones extramuestrales del modelo o las que recojan efectos conocidos.

Veamos qué coeficientes estimados son significativos.

```{r}
coeftest(arima210)
```

Las tres variables de intervención son significativas y el coeficiente $\phi_2$ (`ar2`) también. No es significativo el coeficiente $\phi_1$ (`ar1`) al 5% pero si al 10%. En cualquier caso, los modelos Arima son modelos jerárquicos donde la presencia de un coeficiente significativo de cierto orden exige que los coeficiente de orden inferior estén presentes, sean o no significativos. En nuestro caso, como el coeficiente $\phi_2$ es significativo, se debe dejar en el modelo el coeficiente  $\phi_1$.

**Medidas de error**

El error medio es `r round(accuracy(arima210)[2],0)` vehículos por día (RMSE) y el error porcentual medio (MAPE) es `r round(accuracy(arima210)[5],1)`%. No hay sesgo de predicción y la fórmula empleada para el cálculo del intervalo de confianza de las predicciones es válida.

```{r}
#| eval: false
accuracy(arima210)
```

```{r}
#| echo: false
round(accuracy(arima210),2)
```

\

**Incorrelación, Homocedasticidad y Normalidad de los residuos**

Veamos ahora si el residuo es ruido blanco:

```{r}
Box.test(error, lag = 2, type = "Ljung-Box")
Box.test(error^2, lag = 2, type = "Ljung-Box") 
jarque.bera.test(error)
```

Las hipótesis de incorrelación y homocedasticidad se aceptan. También se aceptarían para otros valores de $k$ razonables. Igualmente, se acepta la hipótesis de normalidad. 

\

Confirmamos que $aforo_t \sim ARIMA(2,1,0)$ con intervención.

### Interpretación del modelo {.unnumbered}

El **modelo teórico** es $aforo_t \sim ARIMA(2,1,0) + AI$: 

$$(1 - \phi_1 L - \phi_2 L^2)(1 - L)aforo_t =  \varepsilon_t + \gamma_1 \cdot d1979 + \gamma_2 \cdot d2011 + \gamma_3 \cdot d2020.$$

Si desarrollamos, queda la **ecuación de predicción**: 

$$aforo_t = aforo_{t-1} + \phi_1(aforo_{t-1}-aforo_{t-2}) + \phi_2(aforo_{t-2}-aforo_{t-3}) +$$ 
$$\gamma_1 \cdot d1979 +  \gamma_2 \cdot d2011 + \gamma_3 \cdot d2020 + \varepsilon_t.$$

Finalmente, el **modelo estimado** es: 
$$\widehat{aforo}_t = aforo_{t-1} + 0.20(aforo_{t-1}-aforo_{t-2}) + 0.41(aforo_{t-2}-aforo_{t-3})$$ 
$$-1664 \cdot d1979 - 1188 \cdot d2011 - 2368 \cdot d2020$$ 
Cada año el aforo es el mismo que el aforo del año pasado más un 20% del último incremento observado y un 41% del incremento anterior.

Respecto de la intervención, en 1979 hubo cerca de 1700 de vehículos por día menos de lo esperado debido a la apertura de la autopista AP-7; en 2011 hubo unos 1200 vehículos menos debido a la Gran Recesión; y en 2020 las restricciones de movilidad debidas a la pandemia redujeron el aforo en 2400 vehículos al día.
      

### Predicción {.unnumbered}

Como hemos incluido tres variables ficticias en el ajuste, de cara a predecir el aforo hemos de indicar cuales serán los valores futuros para estas variables. En este caso serán cero puesto que son intervenciones que no responden a un efecto calendario. Las causas detrás de estas dos intervenciones no se espera que se repitan en el futuro.

En `R` esto se hace incluyendo en el comando `forecast` el argumento `xreg = cbind(rep(0, 4), rep(0, 4), rep(0, 4))` que añade cuatro ceros por cada variable de intervención porque la predicción va a ser a cuatro años vista.

```{r}
parima210 <- forecast(arima210, 
                      h = 4, 
                      level = 95,
                      xreg = cbind(d1979=rep(0, 4), d2011=rep(0, 4),
                                   d2020=rep(0, 4)))
parima210
```

```{r}
#| label: fig-AforoPre
#| fig-cap: "Aforo (1960-2023) y predicción (2024-2027)"
autoplot(parima210, 
         xlab = "",
         ylab = "Vehículos diarios",
         main = "") +
  scale_x_continuous(breaks= seq(1960, 2028, 4)) 
```

\

### Validación con origen de predicción móvil {.unnumbered}

Vamos a calcular el error extramuestral según el horizonte temporal de previsión. Asumiremos que se precisan 30 años para estimar el modelo, fijaremos el horizonte temporal en 4 años y calcularemos el error MedAPE, para evitar el efecto de los años atípicos.

```{r}
k <- 30                  
h <- 4                  
T <- length(aforo)     
s <- T - k - h    

mapeArima <- matrix(NA, s + 1, h)

for (i in 0:s) {
  train.set <- subset(aforo, start = i + 1, end = i + k)
  test.set <-  subset(aforo, start = i + k + 1, end = i + k + h) 
  
  fit <- Arima(train.set, 
               include.constant = FALSE,
               order = c(2, 1, 0))
  
  fcast <- forecast(fit, h = h)

  mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set
  
}

mapeArima <- apply(mapeArima, MARGIN = 2, FUN = median)
mapeArima
```

El error de previsión extramuestral crece notablemente con el horizonte temporal. El error de las previsiones a un año vista es del 2%, muy inferior al error intramuestral del 6% por ser un error mediano, pero para tres años vista alcanza el 10%.


## Consumo de alimentos en el hogar per cápita

Analizaremos el **consumo alimentario en hogar per cápita** en España. Esta serie está construida a partir de la serie de consumo alimentario en hogar (disponible en el Ministerio de Agricultura, Alimentación y Medio Ambiente), y la serie de población (disponible en el Instituto Nacional de Estadística). Es una serie anual de 1990 a 2023 (34 datos) y la unidad es el Kg per cápita. La @fig-Alimentos muestra que es una serie estacionaria.

```{r}
#| label: fig-Alimentos
#| fig-cap: "Consumo alimentario en hogar"
alimentospc <- read.csv2("./series/Alimentacionpc.csv",
                         header = TRUE)

alimentospc <- ts(alimentospc,
                  start = 1990, 
                  freq = 1)
    
autoplot(alimentospc, 
         xlab = "", 
         ylab = "Kg per cápita",
         main = "",
         ylim = c(0, 700))
```

El pico en el año 2020 se debe al aumento del consumo de alimentos en el hogar causado por el periodo de confinamiento por la Covid-19 (marzo a junio) y el aumento del trabajo desde casa. La aparente caída en 2022 y 2023 se debe a que como efecto rebote, los españoles ahora comemos y cenamos más fuera del hogar.[^2]

[^2]: Según el INE, el gasto monetarios per cápita de los españoles en restauración y hoteles cayó ligeramente en 2018 y 2019, y retrocedió un 40% en 2020 a causa de la pandemia. Desde entonces, el gasto per cápita en esta partida ha crecido año tras año: un 30.7% en el 2021 y 2022 y un 13.6% en el 2023.

### Transformación de la serie {.unnumbered}

La @fig-AlimentosFAC indica que la serie original ya es estacionaria y la función `ndiffs` lo corrobora. Por tanto asumimos que $d=0$ o $alimentospc_t \sim I(0)$.

```{r}
#| eval: false
autoplot(alimentospc, xlab = "", ylab = "", main = "")
autoplot(diff(alimentospc), xlab = "", ylab = "", main = "")
ggAcf(alimentospc, xlab = "", ylab = "", main = "")
ggAcf(diff(alimentospc), xlab = "", ylab = "", main = "")
```

```{r}
#| echo: false
#| label: fig-AlimentosFAC
#| fig-cap: "Gráfica y FAC para Alimentos"
#| fig-subcap: 
#|   - "Serie original"
#|   - "Dif.  de la serie"
#|   - "FAC serie original"
#|   - "FAC dif. de la serie"
#| layout: [[50, 50], [50, 50]]
autoplot(alimentospc, xlab = "", ylab = "", main = "")
autoplot(diff(alimentospc), xlab = "", ylab = "", main = "")
ggAcf(alimentospc, xlab = "", ylab = "", main = "")
ggAcf(diff(alimentospc), xlab = "", ylab = "", main = "")
```

```{r}
ndiffs(alimentospc)
```

### Identificación y Estimación {.unnumbered}

Para identificar los valores de $p$ y $q$ veremos que nos sugiere `auto.arima` :

```{r}
auto.arima(alimentospc,
           d = 0)
```

La identificación automática sugiere un proceso AR(1) con constante. Tanto el coeficiente de la constante ("mean" en la salida) como el coeficiente del proceso autorregresivo son significativos ---su valor supera dos veces su error estándar.

Vamos a ver la gráfica de los residuos de este proceso para identificar rápidamente si hay valores extremos (@fig-AlimentosError).

```{r}
#| label: fig-AlimentosError
#| fig-cap: "Error + Intervención"
arima100 <- Arima(alimentospc, 
                  order = c(1, 0, 0),
                  include.constant = TRUE)

error <- residuals(arima100)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1990, 2024, 2)) 

fechas <- format(seq(as.Date("1990-01-01"), as.Date("2023-01-01"), by = "year"), "%Y")
fechas[abs(error) > 2.5 * sderror]
```

Hay dos valores extremos, uno en 2020 asociado al aumento de las comidas en el hogar por el confinamiento durante la pandemia y otro en 2022 asociado a un aumento de las comidas fuera del hogar tras la pandemia. Tras crear las variables ficticias asociadas a los años 2020 y 2022, la autoidentifición sigue mostrando un proceso ARIMA(1, 0, 0) con constante. 

```{r}
d2020 <- 1* (time(alimentospc) == 2020)
d2022 <- 1* (time(alimentospc) == 2022)

auto.arima(alimentospc, 
           d = 0, 
           xreg = cbind(d2020, d2022))
```

Vamos a estimar el modelo identificado.

```{r}
arima100 <- Arima(alimentospc, 
                  order = c(1, 0, 0),
                  include.constant = TRUE,
                  xreg = cbind(d2020, d2022))
arima100
```

Vamos a ver la gráfica de los residuos de este proceso para determinar si hay más valores extremos (@fig-AlimentosError2).

```{r}
#| label: fig-AlimentosError2
#| fig-cap: "Error + Intervención"
error <- residuals(arima100)
sderror <- sd(error)

autoplot(error, series="Error",
         colour = "black",
         xlab = "",
         ylab = "Error",
         main = "") +
  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, 
             colour = c("red", "green", "green", "red"), 
             lty = 2) + 
  geom_point() +
  scale_x_continuous(breaks= seq(1990, 2024, 4)) 

fechas[abs(error) > 2.5 * sderror]
```

Se identifican otro posible valor extremo para el año 2023 (el error supera las 2.5 desviaciones típicas). Parece que, tras la pandemia, los españoles tenemos más ganas de disfrutar de nuestro ocio y salimos más a comer y cenar fuera, reduciendo el consumo de alimentos del hogar. Vamos a crear una variable para 2023. Además, si aplicamos `auto.arima` añadiendo esta nueva intervención, la identificación se mantiene.

```{r}
d2023 <- 1* (time(alimentospc) == 2023)

arima100 <- Arima(alimentospc, 
                  order = c(1, 0, 0),
                  include.constant = TRUE,
                  xreg = cbind(d2020, d2022, d2023))
arima100
```

Si se repite el análisis de intervención, aparece otro valor extremo en 1999, con un consumo inferior al esperado. Sin embargo, para este año no se identifica ninguna razón para la caída del consumo por lo que damos el modelo previo como definitivo. **Toda intervención debe tener detrás una causa identificable, si no puede ser fruto de simple azar.** 

### Validación {.unnumbered}

**Coeficientes significativos**

Tanto $\phi_1$ como el intercepto $\mu$ y las variables de intervención son significativas.

```{r}
coeftest(arima100)
```

\

**Medidas de error**

El error medio es `r round(accuracy(arima100)[2],1)` kg per cápita (RMSE) y el error porcentual medio (MAPE) es `r round(accuracy(arima100)[5],1)`%. No hay sesgo y los intervalos de confianza de las predicciones son correctos.

```{r}
#| eval: false
accuracy(arima100)
```

```{r}
#| echo: false
round(accuracy(arima100),2)
```

\

**Incorrelación, Homocedasticidad y Normalidad del residuo**

Veamos ahora si el residuo es ruido blanco:

```{r}
error <- residuals(arima100)
Box.test(error, lag = 2, type = "Ljung-Box")
Box.test(error^2, lag = 2, type = "Ljung-Box") 
jarque.bera.test(error)
```


Las hipótesis de incorrelación y homocedasticidad se aceptan. También se aceptarían para otros valores de $k$ razonables. La hipótesis de normalidad también se acepta al 5%.

### Interpretación del modelo {.unnumbered}

El **modelo teórico** identificado es 
$$(1 - \phi_1 L) alimentospc_t = c +  \gamma_1 d2020 + \gamma_2 d2022 + \gamma_3 d2023 + \varepsilon_t,$$
que desarrollando queda (**ecuación de predicción**)
$$alimentospc_t = c + \phi_1 alimentospc_{t-1} + \gamma_1 d2020 + \gamma_2 d2022 + \gamma_3 d2023 + \varepsilon_t.$$

Finalmente, el **modelo estimado** es 
$$\widehat{alimentospc}_t = 204.1 + 0.68 \cdot alimentospc_{t-1} +$$ 
$$55.3\cdot d2020 - 62.9\cdot d2022 - 74.2\cdot d2023.$$

::: callout-important
## La constante del modelo teórico y la media del modelo estimado

El término contante $\mu$ que estima `R` no es el valor "c" que hemos visto en la teoría. Para convertir la contante estimada por `R` en "c" hemos de multiplicarla por el polinomio autorregresivo. En este caso, $$c = (1 - \phi_1)\cdot \mu = (1 - 0.67790) \cdot 633.72878= 204.124$$
:::

Cada año el consumo de alimentos per cápita en el hogar es 204 kilos más un 68% del consumo del año pasado.

En 2020, debido al efecto combinado del periodo de confinamiento entre marzo y junio y el incremento del trabajo en casa, se produjo un fuerte aumento del consumo de alimentos en el hogar, estimado en 55 kg per cápita. Por el contrario, en 2022 y 2023 se redujo en 63 y 74 kg, respectivamente, por el aumento del ocio.

Si el incremento en las salidas a comer y cenar fuera se mantiene, la caída en el consumo de alimentos en el hogar se hará permanente y debemos pensar en un cambio de nivel más que en la presencia de dos intervenciones independientes.

### Predicciones de la serie {.unnumbered}

Como hemos incluido tres variables ficticias en el ajuste, de cara a predecir el consumo de alimentos hemos de indicar cuales serán los valores futuros para estas variables. En el caso de asumir que el consumo de alimentos en el hogar va a volver a sus valores prepandemia, el valor futuro de las variables intervención será cero. Estamos asumiendo que las causas detrás de estas intervenciones no se repitirán en el futuro.

```{r}
parima100 <- forecast(arima100, 
                      h = 5, 
                      level = 95,
                      xreg = cbind(rep(0, 5), rep(0, 5), rep(0, 5)))
parima100
```

Ahora bien, si pensamos que la caída del consumo de alimentos en el hogar por un aumento del ocio es un efecto permanente, podemos actualizar nuestras previsiones poniendo el valor de 1, en lugar de 0, como valores futuros de d2022 o de d2023, según creamos que el efecto va a ser mayor o menor.

```{r}
parima100b <- forecast(arima100, 
                       h = 5, 
                       level = 95,
                       xreg = cbind(rep(0, 5), rep(0, 5), rep(1, 5)))
parima100b
```

Observa que estas últimas predicciones son menores que las obtenidas previamente. Podemos ver gráficamente el efecto sobre las predicciones de cada supuesto (véase la @fig-AlimentosPre).

```{r}
#| label: fig-AlimentosPre
#| fig-cap: "Consumo de alimentos y predicción"
autoplot(alimentospc, 
         series = "Alimentos",
         xlab = "",
         ylab = "Kg per cápita",
         main = "",
         PI = FALSE,
         ylim = c(300, 700)) +
  autolayer(parima100,  PI = FALSE, series = "Aumento ocio desaparece") +
  autolayer(parima100b, PI = FALSE, series = "Aumento ocio se mantiene") +
  scale_x_continuous(breaks= seq(1990, 2028, 4)) +
  scale_colour_discrete(limits=c("Alimentos", "Aumento ocio desaparece", 
                                 "Aumento ocio se mantiene")) +
  labs(colour="Predicciones") + 
  theme(legend.position=c(0.2,0.2))
```

\

## Comparación con alisado exponencial

Veamos una comparativa, para los tres ejemplos vistos, entre los resultados obtenidos con ARIMA y con Alisado exponencial.

-   Residuos:
    -   MAPE ARIMA: $2.65\%$ - ARIMA(1,1,0) sin deriva

    -   MAPE ETS: $2.95\%$ - ETS(M,N,N), $\alpha=1$

    -   Ambos métodos presentan similar calidad de ajuste.

\vspace{.5cm}

-   Aforo:
    -   MAPE ARIMA: $5.99\%$ - ARIMA(2,1,0) sin deriva, con intervención

    -   MAPE ETS: $7.42\%$ - ETS(M,A,N), $\alpha=0.94$, $\beta=0.05$

    -   ARIMA tiene menor error al incluir tres variables de intervención

\vspace{.5cm}

-   Alimentos per cápita:
    -   MAPE ARIMA: $1.32\%$ - ARIMA(1,0,0) con constante e intervención

    -   MAPE ETS: $2.24\%$ - ETS(A,N,N), $\alpha = 0.95$

    -   ARIMA tiene menor error menor y permite capturar la caída del consumo de alimentos en el hogar tras la pandemia

\
\


# Resumen de los comandos utilizados


|Función        |Paquete |  Descripción                                             |
|:------------------|:-------------------|:-------------------------------------------------------|
|`arima.sim`    |stats  |genera una simulación de un proceso ARIMA               |
|`Arima`        |forecast  |estima un proceso ARIMA |
|`auto.arima`   |forecast  |identificación automática de un modelo ARIMA          |
|`coeftest`     |lmtest    |contrasta la significatividad individual de los parámetros de un modelo|
|`forecast`     |forecast  |realiza una predicción de un modelo|
|`Box.test`     |stats  |prueba de independencia de una serie temporal           |
|`shapiro.test` |stats  |prueba de normalidad de Shapiro-Wilks                   |
|`jarque.bera.test` |tseries    |prueba de normalidad de Jarque-Bera          |

\
\
\
\






