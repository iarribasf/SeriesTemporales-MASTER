---
title: "Autorregresión con redes neuronales"
author: "Iván Arribas (Depto. Análisis Económico. Universitat de València)"
subtitle: "Máster de Bioestadística (Modelización Estadística)"
toc: false
number-sections: true
bibliography: references.bib
crossref:
  fig-title: Figura
  tbl-title: Tabla
  fig-prefix: Figura
  tbl-prefix: Tabla
---

```{r}
#| echo: false
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "",
                      fig.align = "center", 
                      fig.show = "hold",
                      fig.height = 4,
                      fig.width = 8,
                      out.width = "80%")
```

```{r}
#| echo: false
library(forecast)
library(ggplot2); theme_set(theme_bw())
library(tidyverse)
```

\
\

# Antecedentes

En las dos grandes familias de modelos que permiten ajustar y predecir series temporales --Alisado exponencial y modelos Arima-- se ajusta un modelo a una serie temporal y el resultado del ajuste nos permite no solo predecir, sino aprender y entender el comportamiento de la serie. Por ejemplo, el resultado del ajuste por alisado nos permite saber si la pendiente de la serie cambia en el tiempo (parámetro $\beta$ del ajuste) o el tipo de esquema de la serie según que la estacionalidad sea aditiva o multiplicativa. Con los modelos Arima podemos estimar el impacto en la serie de un efecto calendario (Semana Santa, días laborables...).

Otra familia de modelos muy versátiles que permiten predecir con todo tipo de datos --transversales, series temporales, imágenes, espacio-temporales...-- son las **redes neuronales**. Estos modelos son el embrión del *Deep Learning* y el motor de muchas *AI* y los estudiaréis en detalle el próximo año en la asignatura *Técnicas Avanzadas de Predicción en Negocios*.

Vamos a ver muy, pero que muy por encima en que consisten las redes neuronales y como se pueden aplicar para predecir series temporales. Esto es una pequeña píldora.

# Arquitectura de una red neuronal de una capa

Una red neuronal puede ser entendida como una red de neuronas dispuestas en capas. Siempre hay una capa de entrada de los datos y una capa de salida de la respuesta. Entre estas dos capas se puede disponer de tantas capas intermedias (ocultas) como se considere necesario.

Cada capa está formada por un número determinado y potencialmente diferente de neuronas o nodos. Los nodos de una capa están conectadas a los nodos de la siguiente. Por simplicidad asumiremos que todos los nodos de una capa se conectan con los nodos de la capa siguiente.

Aquí vamos a considerar solo redes neuronales con una capa intermedia y donde la capa de salida tiene solo una neurona. La @fig-RN es un ejemplo de este tipo de redes neuronales.

\

::: {#fig-RN}
![](./imagenes/Redes_neuronales.png)

Red neuronal con una sola capa intermedia
:::

En esta red cada nodo de una capa recibe entradas de los nodos de la capa previa. Dicho de otra forma, las salidas de los nodos de una capa son las entradas de los nodos de la siguiente capa. Es lo que se denomina una *multilayer feed-forward network*.

Las entradas que recibe cada nodo se combinan usando una función lineal ponderada. Por ejemplo, un nodo $j$ de la capa intermedia recibe las dos entradas $x_1$ y $x_2$ de los nodos de la capa de entrada y los combina linealmente

$$z_j = b_j + \sum_{i=1}^2 w_{i,j}x_i$$ Para los nodos de la capa intermedia el valor $z_j$ se transforma usando una función no lineal, por ejemplo la sigmoidea:

$$s_j = \frac{1}{1 + e^{-z_j}}$$ y este valor $s_j$ es la salida del nodo $j$ que va al nodo de la capa de salida.

Los valores de los pesos $b_1$, $b_2$, $w_{1,1}$, $w_{1,2}$...$w_{2,5}$ se deben *ajustar* a partir de los datos. Estos valores suelen estar restringidos para evitar que sean demasiado grandes. El parámetro que restringe las ponderaciones se conoce como *parámetro de decaimiento*, y suele ser igual a $0.1$.

Los pesos toman valores aleatorios al principio y luego se actualizan con los datos observados en un proceso de *aprendizaje*. Por lo tanto, hay un elemento de aleatoriedad en las predicciones producidas por una red neuronal. Por este motivo, la red suele entrenarse varias veces utilizando diferentes puntos de partida aleatorios, y los resultados se promedian.

# Autoregresión de redes neuronales

En el contexto de series temporales, los valores de entrada pueden ser valores retardados de la serie y el valor de salida deseado el valor contemporáneo. De la misma forma que en un modelo AR usamos los datos pasados para predecir el futuro.

::: {#fig-RN2}
![](./imagenes/Redes_neuronales2.png)

Red neuronal para predecir una serie temporal. El dato del periodo $t$ se predice a partir de los dos datos previos.
:::

Vamos a extender estas ideas e ir añadiendo algo de notación.

Como hemos indicado vamos a considerar solo redes simples con una capa intermedia y una capa de salida de un solo nodo, que denominaremos $NNAR$. La notación $NNAR(p, k)$ indica que hay $p$ valores desfasados en la capa de entrada y $k$ nodos en la capa intermedia. Por ejemplo, la red de la @fig-RN2 es modelo $NNAR(2,5)$, donde $y_{t-1}$ e $y_{t-2}$ son usados para predecir $y_t$. Así, un modelo $NNAR(p, 0)$ sería equivalente a un modelo $ARIMA(p,0,0)$.

Si la serie tiene estacionalidad es conveniente que entre los datos de entrada estén las observaciones pasadas de la misma estación que se desea predecir. Por ejemplo, para la serie diaria de consumo eléctrico un modelo $NNAR(2, 1, k)$ usaría como datos de entrada $y_{t-1}$, $y_{t-2}$ e $y_{t-7}$ para predecir $y_t$. En general, $NNAR(p,P,k)_m$ usa como datos de entrada $y_{t-1}$,$y_{t-2}$,...,$y_{t-p}$,$y_{t-m}$, $y_{t-2m}$,...,$y_{t-Pm}$ y una capa intermedia de $k$ neuronas. Por lo tanto, $NNAR(p,P,0)_m$ es equivalente a $ARIMA(p,0,0)(P,0,0)_m$.

# Aplicación

La función `nnetar` de la librería `forecast` permite estimar modelos $NNAR(p,P,k)_m$. En su forma más sencilla el usuario no tiene que especificar los valor de los parámetros $p$, $P$ y $k$ ya que la función los identifica según ciertos criterios.

La siguiente gráfica muestra el consumo eléctrico en España en GWh para 17 semanas desde febrero hasta mayo de 2023. Hay una fuerte componente estacional diaria de orden $7$, donde el consumo es alto de lunes a viernes, algo mas reducido el sábado y aún menor el domingo.

```{r}
#| label: fig-Electricidad
#| fig-cap: "Consumo eléctrico (febrero a mayo de 2023)"
electricidad <- read.csv("./series/Consumo electrico.csv", 
                         header = TRUE)

electricidad <- ts(electricidad[, 1], 
                   start = c(1, 7),
                   frequency = 7)

electricidad <- window(electricidad,
                       start = c(7, 1),
                       end = c(23, 7))

autoplot(electricidad) + 
  ggtitle("") +
  ylab("GWh") + 
  xlab("Semana")
```

La @fig-ElectricidadPre muestra la serie y su predicción para los siguientes 14 días. El modelo ajustado es $NNAR(1,1,2)_7$. Es decir, la capa de entrada tiene 2 nodos porque para predecir el consumo del día $t$, $y_t$, se usa el consumo del día previo $y_{t-1}$ y el consumo de hace una semana $y_{t-7}$. La capa intermedia tiene dos nodos. 

```{r}
fit <- nnetar(electricidad)

accuracy(fit)
```

El error de ajuste es de 3.5% o 31 GWh y el ajuste no presenta sesgo.

Por otro lado, el ACF1 muestra una elevada autocorrelación de orden 1. Sin embargo, esto no afecta a la calidad de la predicción por intervalos porque con redes neuronales no se utiliza una fórmula cerrada que precise de la hipótesis de incorrelación. Se utiliza un proceso complejo y costoso temporalmente.

```{r}
#| label: fig-ElectricidadPre
#| fig-cap: "Consumo eléctrico (febrero a mayo de 2023) y predicción"
tiempo <- Sys.time()

pfit <- forecast(fit, 
                 h = 14,
                 level = 95,
                 PI = TRUE)

Sys.time() - tiempo

autoplot(pfit) +
  ylab("GWh") + 
  xlab("Semana")
```


```{r}
pfit
```

La función `nnetar` admite la inclusión de variables de intervención de la forma usual a través del argumento `xreg`.

\
\
\
\
