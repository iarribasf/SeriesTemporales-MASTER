[
  {
    "objectID": "04-01-Bootstrapping.html#idea-general",
    "href": "04-01-Bootstrapping.html#idea-general",
    "title": "Bootstrapping para obtener intervalos de predicción",
    "section": "2.1 Idea general",
    "text": "2.1 Idea general\nVeamos primero la idea general y luego los detalles:\n\nPartimos de una serie temporal \\(\\{y_t\\}_{t=1}^T\\) y un horizonte de predicción \\(h\\).\nA partir de la serie original vamos a generar una nueva serie que es similar a la original. Luego veremos como.\nAjustamos nuestro modelo a la nueva serie y obtenemos una predicción \\(h\\) periodos hacia adelante, que llamaremos \\(\\hat y_{T+h|T}^1\\).\nRepetimos el paso 2 y 3 un numero \\(n\\) de veces (típicamente \\(n=5000\\)), de forma que al final del proceso tenemos \\(n\\) predicciones \\(h\\) periodos hacia adelante: \\(\\hat y_{T+h|T}^1,\\; \\hat y_{T+h|T}^2,\\; \\ldots,\\; \\hat y_{T+h|T}^n\\).\nPor último, obtenemos el intervalo de predicción calculando los percentiles correspondientes a partir de estas \\(n\\) predicciones.\n\nEste proceso hay que repetirlo para cada horizonte de predicción en que estemos interesados."
  },
  {
    "objectID": "04-01-Bootstrapping.html#detalles",
    "href": "04-01-Bootstrapping.html#detalles",
    "title": "Bootstrapping para obtener intervalos de predicción",
    "section": "2.2 Detalles",
    "text": "2.2 Detalles\n\nPaso 2: nueva serie\nEl paso clave del proceso es el paso 2, donde se obtiene una nueva serie similar a la original. En lo que viene a continuación no voy a ser riguroso para no perdernos en cuestiones matemáticas, pero sí suficientemente preciso para entender bien el proceso.\n\nDada la serie original, la descomponemos en sus tres componentes: tendencia, estacionalidad y error.\nA continuación, obtenemos una versión barajada de la componente del error. (Aquí es donde no estoy siendo preciso porque el proceso de barajado se tiene que hacer por bloques y es con reemplazamiento.)\nAhora se combinan –sumando o multiplicando, según el esquema– la tendencia, la estacionalidad y el error barajado para obtener una nueva serie que se parecerá a la original.\n\nVeamos un ejemplo muy sencillo, para una serie simulada de orden estacional 5.\n\nLa dos primeras columnas indican la estación de cada dato y el valor de la serie, para un total de 15 observaciones.\nLas columnas Tendencia, Estacionalidad y Error han sido obtenidas aplicando el método de descomposición por regresiones locales ponderadas. Observa que cada dato de la serie es la suma de estas tres componentes.\nLa columna ErrorBootstrapping se ha obtenido como un muestreo con reemplazamiento de los datos de la columna Error. Observa que como es una muestra con reemplazamiento hay algunos errores repetidos.\nPor último, la nueva serie (columna NuevaSerie) se ha obtenido sumando las columnas Tendencia, Estacionalidad y ErrorBootstrapping. Esta serie tiene la misma tendencia y estacionalidad que la serie original y solo se diferencia en el error, así que la nueva serie debería parecerse a la serie original.\n\n\n\n   Estacion Serie Tendencia Estacionalidad  Error ErrorBootstrapping NuevaSerie\n1         1 27.00     19.96           9.60  -2.56              -2.96      26.59\n2         2 16.72     25.12         -13.33   4.93               9.43      21.22\n3         3 15.08     30.14         -12.09  -2.96              -2.56      15.49\n4         4 18.79     34.77          -9.11  -6.87               3.10      28.76\n5         5 75.53     38.58          24.94  12.01              12.01      75.53\n6         1 63.31     39.24           9.60  14.47               3.10      51.94\n7         2 17.28     38.48         -13.33  -7.87               9.43      34.58\n8         3 18.00     34.84         -12.09  -4.75              -6.87      15.88\n9         4 24.84     30.85          -9.11   3.10              -0.40      21.34\n10        5 54.67     30.13          24.94  -0.40              -6.87      48.21\n11        1 30.85     31.11           9.60  -9.86              12.01      52.72\n12        2 22.02     30.52         -13.33   4.84              -9.86       7.32\n13        3 26.51     29.17         -12.09   9.43               3.10      20.18\n14        4 24.14     27.94          -9.11   5.31              -7.87      10.96\n15        5 41.31     26.54          24.94 -10.17              12.01      63.49\n\n\n\n\nPaso 5: Predicción por intervalos\nVamos a aclarar también el paso 5.\nPartimos de \\(n\\) predicciones a \\(h\\) periodos vista (\\(\\hat y_{T+h|T}^1,\\; \\hat y_{T+h|T}^2,\\; \\ldots,\\; \\hat y_{T+h|T}^n\\)) y queremos obtener a partir de ellas el intervalo de confianza.\nSupongamos que el nivel de confianza deseado es del 95%. Entonces, debemos calcular para las predicciones el percentil 2.5% y 97.5%. Recuerda que el percentil 2.5% es el valor numérico que deja un 2.5% de las predicciones por debajo de él; y que el percentil 97.5% es el valor numérico que deja un 97.5% de las predicciones por debajo de él. La función de R quantile() permite obtener estos valores.\nSi denominamos \\(l_h\\) al percentil 2.5% y \\(u_h\\) al percentil 97.5%, el intervalo de confianza de la predicción a \\(h\\) periodos vista es \\([l_h,\\; u_h]\\).\n\n\n¿Y la prediccion puntual?\nPara la predicción puntual tenemos dos opciones: obtener la predicción a partir de la serie original (como hemos visto en clase); u obtenerla como media de las \\(n\\) predicciones obtenidas\n\\[\\frac{1}{n}\\sum_{j=1}^n \\hat y_{T+h|T}^j\\]\nEste segundo método es el usual y se denomina bagging de bootstrap aggregating."
  },
  {
    "objectID": "03-07-Ejemplo2.html",
    "href": "03-07-Ejemplo2.html",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "",
    "text": "1 Introducción\nConsideremos de nuevo la serie temporal correspondiente al número de defunciones causadas por enfermedades cerebrovasculares, Esta serie está disponible en el Instituto Nacional de Estadística desde enero de 1980 hasta diciembre de 2020, un total de 41 años o 492 meses.\nEn la descriptiva vimos que la descomposición revelaba la presencia de varios valores atípicos concentrados al inicio de la serie. Por este motivo, para el análisis por técnicas de Alisado Exponencial vamos a recortar la serie y considerarla solo desde enero de 1988, 33 años o 396 meses.\nLa serie presenta tendencia decreciente y estacionalidad de orden 12 en un claro esquema multiplicativo. Ya vimos que el determinante principal del efecto estacional es la temperatura.\n\nDefEnfCer <- read.csv2(\"./series/Enfermedades cerebrovasculares.csv\", \n                       header = TRUE)\n\nDefEnfCer <- ts(DefEnfCer[,2], \n                start = 1980, \n                frequency = 12)\n\nDefEnfCer <- window(DefEnfCer, \n                    start = 1988)\n\nautoplot(DefEnfCer,\n         xlab = \"\",\n         ylab = \"Defunciones\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1980, 2020, 2)) \n\n\n\n\nFigura 1: Defunciones causadas por enfermedades cerebrovasculares\n\n\n\n\n\n\n\n\n\n2 Ajuste por alisado exponencial e interpretación\nVamos a aplicar la metodología de Alisado Exponencial a la serie de defunciones. Si se estima el modelo sin imponer ninguna restricción ets identifica como modelo óptimo ETS(M,A,M).\n\nDefEnfCerEts <- ets(DefEnfCer)\nsummary(DefEnfCerEts) \n\nETS(M,A,M) \n\nCall:\n ets(y = DefEnfCer) \n\n  Smoothing parameters:\n    alpha = 0.0762 \n    beta  = 1e-04 \n    gamma = 2e-04 \n\n  Initial states:\n    l = 3833.3678 \n    b = -3.0363 \n    s = 1.1355 0.9861 0.9498 0.8359 0.9117 0.9468\n           0.8954 0.9582 0.9751 1.0874 1.0854 1.2326\n\n  sigma:  0.0517\n\n     AIC     AICc      BIC \n6337.017 6338.636 6404.701 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -10.48538 156.2314 113.1895 -0.5400421 3.789047 0.6753591\n                  ACF1\nTraining set 0.2317092\n\n\nEl modelo estimado tiene pendiente aditiva, estacionalidad multiplicativa y residuo multiplicativo:\n\\[y_{t+1} = (l_t + b_t) \\cdot s_{t+1-m} \\cdot (1 + \\varepsilon_{t+1}).\\]\nEl valor de \\(\\alpha\\) indica que el nivel de la serie ha ido variando muy lentamente en el tiempo. Los valores de \\(\\beta\\) y \\(\\gamma\\) son prácticamente cero e indican que ambas, pendiente y estacionalidad, se mantienen constantes en el tiempo (véase Figura 2).\n\nautoplot(DefEnfCerEts,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\nFigura 2: Descomposición por Alisado Exponencial para defunciones por enfermedades cerebrovasculares\n\n\n\n\nLa calidad del ajuste es bastante buena, con un MAPE de 3.8% y un RMSE de 156 casos (o 113 si usamos el MAE). Además, según el MASE, el modelo de alisado supone una mejora del 33% respecto del método ingenuo con estacionalidad, el más sencillo que podríamos aplicar. Además, la ACF1 indica que las estimaciones por intervalo de las previsiones serán incorrectas.\nLos últimos valores estimados del nivel y la estacionalidad, que corresponden a diciembre de 2020, nos permiten mostrar gráficamente la componente estacional (Figura 3).\n\ntail(DefEnfCerEts$states, 1)\n\n\n\n               l     b   s1   s2   s3   s4   s5   s6  s7   s8   s9  s10  s11\nDec 2020 2119.81 -3.64 1.14 0.99 0.95 0.84 0.91 0.95 0.9 0.96 0.98 1.09 1.09\n          s12\nDec 2020 1.23\n\n\n\ncomponenteEstacional <- tail(DefEnfCerEts$states, 1)[14:3]\n\nggplot() +\n  geom_line(aes(x = 1:12, y = componenteEstacional)) + \n  geom_hline(yintercept = 1, colour = \"blue\", lty = 2) +\n  ggtitle(\"\") +\n  xlab(\"\") +\n  ylab(\"Efecto estacional\") +\n  scale_x_continuous(breaks= 1:12, \n                     labels = c(\"Ene\", \"Feb\", \"Mar\", \"Abr\", \"May\", \"Jun\", \n                                \"Jul\", \"Ago\", \"Sep\", \"Oct\", \"Nov\", \"Dic\")) \n\n\n\n\nFigura 3: Componente estacional\n\n\n\n\nEl nivel de las defunciones por enfermedades cerebrovasculares en diciembre de 2020 (última observación) es de 2120 casos y la pendiente -3.6 casos, prácticamente nula. La mayor incidencia de las defunciones por enfermedades cerebrovasculares tiene lugar en invierno, en los meses de diciembre a marzo. En concreto, destaca el mes enero con un incremento del 23% (s12) en las defunciones por enfermedades cerebrovasculares respecto de la media anual. La incidencia en verano es menor que la media anual, observándose en septiembre un 16% menos de casos (s4). El efecto estacional estimado por el método de alisado es muy similar al estimado durante la descriptiva de la serie.\n\n\n\n\n\n3 Predicción\nSi pedimos los valores de predicción y su intervalo de confianza al 95% para los próximos tres años, tenemos (numéricamente sólo se muestra el primer año):\n\nDefEnfCerEtsPre <- forecast(DefEnfCerEts, \n                            h = 36, \n                            level = 95)\nDefEnfCerEtsPre\n\nRecuerda que las fórmulas usadas para el cálculo del intervalo de confianza de las predicciones no son adecuadas.\n\n\n         Point Forecast    Lo 95    Hi 95\nJan 2021       2608.311 2343.797 2872.825\nFeb 2021       2292.956 2059.742 2526.171\nMar 2021       2293.339 2059.402 2527.277\nApr 2021       2052.779 1842.765 2262.792\nMay 2021       2013.743 1807.118 2220.369\nJun 2021       1878.561 1685.239 2071.884\nJul 2021       1982.926 1778.262 2187.591\nAug 2021       1906.109 1708.792 2103.426\nSep 2021       1744.680 1563.540 1925.820\nOct 2021       1978.778 1772.724 2184.831\nNov 2021       2050.840 1836.649 2265.030\nDec 2021       2357.421 2110.480 2604.362\n\n\n\nautoplot(DefEnfCerEtsPre,\n         xlab = \"\",\n         ylab = \"Casos\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\nFigura 4: Muertes por enf. cerebrovasculares (1998-2020) y predicción (2021-2023)\n\n\n\n\nLas predicciones mantiene la tendencia decreciente observada en la serie (véase Figura 4).\n\n\n\n\n\n4 Análisis del error\nLa Figura 5 muestra el residuo del modelo. Se aprecian cuatro meses, febrero de 1999, junio y agosto de 2003 y febrero de 2012 en los que el residuo supera las tres desviaciones típicas porque el número de muertes por enfermedades cerebrovasculares fue muy superior al estimado por el modelo. Además, otros residuos cercanos a las tres desviaciones típicas susceptibles de ser considerados intervención tienen lugar en mayo de 2001, enero de 2005 y febrero de 2015.\n\nerror <- residuals(DefEnfCerEts)\nsderror <- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"black\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(1988, 2020, 2)) \n\n\n\n\nFigura 5: Error + Intervención\n\n\n\n\n\n\n\n\n\n5 Validación: origen de predicción móvil\nAsumimos que se precisan diez años para hacer una buena estimación, \\(k=120\\), y que el horizonte temporal es un año, \\(h = 12\\) meses. La siguiente rutina permite obtener el MAPE para previsiones con un horizonte temporal desde 1 mes hasta 12 meses.\n\nk <- 120                 \nh <- 12                  \nTT <- length(DefEnfCer)  \ns <- TT - k - h          \n\nmapeAlisado <- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set <- subset(DefEnfCer, start = i + 1, end = i + k)\n  test.set <-  subset(DefEnfCer, start = i + k + 1, end = i + k + h)\n  \n  fit <- ets(train.set, model = \"MAM\", damped = FALSE)\n  fcast<-forecast(fit, h = h)\n  mapeAlisado[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n}\n\nerrorAlisado <- colMeans(mapeAlisado)\nerrorAlisado\n\n [1] 3.987776 4.261731 4.409643 4.393159 4.453403 4.516924 4.540244 4.563792\n [9] 4.514867 4.411136 4.501394 4.343623\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorAlisado)) +\n  ggtitle(\"Error de predicción según horizonte temporal\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"MAPE\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\nFigura 6: Error de predicción según horizonte temporal\n\n\n\n\nLa Figura 6 muestra que aunque el error depende del horizonte temporal de previsión, se mueve en una banda muy estrecha: entre el 4% para previsiones a un mes vista y el 4.6% para previsiones a 8 meses vista.\n\n\n\n\n\n6 Modelos alternativos\n¿Podemos reducir el error extramuestral de previsión si cambiamos las opciones por defecto de ets o la serie a analizar? La Figura 7 muestra el error de previsión extramuestral según el horizonte de previsión para los siguientes modelos (todos sin amortiguamiento):\n\n\n\n\n\n\n\n\n\nId\nTransformación\nModelo\nMétodo estimación\n\n\n\n\n1\nNinguna\nMAM\nMáxima verosimilitud\n\n\n2\nNinguna\nMAM\nmínimo error en previsiones a 2 periodos vista\n\n\n3\nLogaritmo\nAAA\nMáxima verosimilitud\n\n\n4\nLogaritmo\nAAA\nmínimo error en previsiones a 2 periodos vista\n\n\n5\nDefunciones por día\nMAM\nMáxima verosimilitud\n\n\n6\nDefunciones por día\nMAM\nmínimo error en previsiones a 2 periodos vista\n\n\n\nEn concreto los comandos utilizados han sido:\n\nModelo 1: ets(x, model = \"MAM\", damped = FALSE)\nModelo 2: ets(x, model = \"MAM\", damped = FALSE, opt.crit = \"amse\", nmse = 2)\nModelo 3: ets(x, model = \"AAA\", lambda = 0, damped = FALSE)\nModelo 4: ets(x, model = \"AAA\", lambda = 0, damped = FALSE,  opt.crit = \"amse\", nmse = 2)\nModelo 5: ets(x/monthdays(x), model = \"MAM\", damped = FALSE)\nModelo 6: ets(x/monthdays(x), model = \"MAM\", damped = FALSE, opt.crit = \"amse\", nmse = 2)\n\n\n\n\n\n\n\n\nFigura 7: Errores de previsión extramuestral. Varios modelos\n\n\n\n\nDe la Figura 7 deducimos que aunque todos los métodos resultan similares en calidad de las predicciones en el corto plazo, en el medio y largo plazo las diferencias pueden ser significativas: la mayor diferencia entre los modelos se da para la previsión a ocho y nueve meses vista y es de 0.6 puntos porcentuales.\nSi queremos entrar en matices:\n\nGlobalmente el modelo que ofrece mejores previsiones es el modelo 6, realizado sobre las defunciones medias por día y donde el criterio para estimar los parámetros es la minimización del error de las previsiones a dos periodos vista. Este modelo es de los mejores en previsiones a medio plazo y largo plazo.\nA corto plazo destaca el modelo 4, seguido de los modelos 2 y 5.\nA medio plazo destacan los modelos 2, 5 y 6.\n\nEs decir, tanto la estrategia de predecir la serie de defunciones medias por día, (en lugar de la serie original) como la de usar como criterio para estimar los parámetros del modelo la minimización del error de previsión a dos periodos vista mejoran la calidad de las previsiones extramuestrales. La combinación de estas dos estrategias es óptima para la previsiones a largo plazo."
  },
  {
    "objectID": "06-Practica.html",
    "href": "06-Practica.html",
    "title": "Prácticas de evaluación",
    "section": "",
    "text": "Durante todo el módulo realizarás una serie de ejercicios que aplicarán las técnicas vistas en cada tema. Lee con atención las siguientes instrucciones generales.\nTu primera tarea es descargarte tu serie temporal. Descárgate aquí el fichero .pdf con la descripción de las series y localiza en él el nombre de tu serie. El este fichero encontrarás también una descripción detallada de la serie que vas a analizar: nombre, definición, unidades, fuente, fechado…\nPor otro lado, descárgate aquí el fichero comprimido con todas las series, descomprímelo y localiza el fichero txt con tu serie.\n\n\n\n\n\nCaracterísticas generales de las entregas\n\n\n\nLa fecha máxima de entrega de cada trabajo se indicará al finalizar cada tema. Procura no retrasarte. Si el retraso supera la semana, tu nota en la práctica será de cero.\nEn Aulavirtual, en la tarea “Practica evaluación n” debes subir un fichero .pdf con los resultados de la práctica n-ésima. Sólo se admite un único fichero en formato .pdf.\nPiensa que además de los contenidos también su ordenación, estructura, sintaxis, comentarios, etc. son parte de la evaluación. Aquí tienes qué aspectos voy a valorar y la puntuación de cada uno.\nIncluye siempre todo el código en R utilizado en el trabajo. Si utilizas Markdown (o similar), haz que el código sea visible; en otro caso, añádelo como un apéndice al final del documento.\n\n\n\n\n\n\nPráctica 1: Descripción de la serie temporal\n\n\n\nHaz una descripción detallada de tu serie (tendencia, estacionalidad, intervención, tipo de esquema…).\nAyúdate para ello de todos los elementos numéricos y gráficos que hemos visto: gráfica de tu serie original, gráfica de su agregación anual, gráfica localización/dispersión, descomposición…\n\nSi el análisis realizado así lo aconseja, indica si vas a recortar tu serie para posteriores análisis.\n\n\n\n\n\nPráctica 2: Alisado exponencial\n\n\n\nAjusta el modelo de alisado más adecuado a tus datos.\n\nDescribe el modelo obtenido.\nObtén los indicadores de la calidad del modelo y coméntalos.\nRealiza un predicción a tres años vista y muestra los resultados de la predicción gráficamente.\nObtén la calidad de las previsiones con origen de previsión móvil.\nObtén el residuo e identifica la presencia de valores extremos.\nCompara el modelo ajustado por Alisado Exponencial con el método ingenuo con estacionalidad\n\n\n\n\n\n\n\nPráctica 3: Procesos estocásticos\n\n\nEsta práctica tiene como objetivos: i) analizar la estacionariedad en media y varianza de tu serie; y ii) transformarla para obtener una serie estacionaria.\n\nConsidera tu serie con fechado anual\n\nObtén su gráfica de la serie y de su primera diferencia.\nRealiza el contraste de raíces unitarias.\nTransforma la serie anual usando sólo las funciones log y diff para obtener otra que sea estacionaria y ergódica.\n\nConsidera ahora tu serie original (sin anualizar)\n\nObtén la gráfica de la serie y de su primera diferencia (regular y/o estacional)\nAdemás, calcula la FAC de la serie y de su primera diferencia (regular y/o estacional).\nTransforma la serie original usando sólo las funciones log y diff para obtener otra que sea estacionaria y ergódica\n\n\n\n\n\n\n\nPráctica 4: Procesos ARIMA sin estacionalidad\nA grandes rasgos el ejercicio consistirá en analizar tu serie temporal sin estacionalidad (agregación anual), siguiendo la metodología de Box y Jenkins. Por tanto, para esta práctica debes considerar solo tu serie con fechado anual.\nA continuación se indican los contenidos mínimos que se deben presentar. Eres libre de elegir el orden de la presentación de los contenidos, así como la información finalmente presentada.\n\nIdentificación del modelo ARIMA\n\nParte de la trasformación de la serie identificada en la práctica previa.\nIdentificación de la parte estructural, análisis de intervención y valores atípicos.\n\nEstimación del modelo definitivo\n\nEstimación de los coeficientes del modelo.\nDesarrollo de la ecuación estimada.\nInterpretación de los parámetros y de la ecuación estimada.\n\nValidación completa del modelo\n\nSignificatividad de los coeficientes (contraste de Wald).\nHipótesis de homocedasticidad e incorrelación.\nValores extremos.\nCálculo de la calidad del ajuste y de las predicciones.\n\nPrevisión de la serie (al menos tres años)\nValoración crítica del modelo ARIMA respecto de un modelo de alisado\n\nComparación del modelo estimado en esta práctica el mejor modelo de alisado para la serie anual.\nComparación de la calidad de los modelos.\nComparación de las previsiones.\n\n\n\n\n\n\n\nPráctica 5: Procesos ARIMA con estacionalidad\nLos objetivos de esta práctica son idénticos a los de la práctica previa, pero considerando tu serie original con estacionalidad.\n\nIdentificación del modelo ARIMA\n\nParte de la trasformación de la serie identificada en la práctica 3.\nIdentificación de la parte estructural, análisis de intervención y valores atípicos.\n\nEstimación del modelo definitivo\n\nEstimación de los coeficientes del modelo.\nDesarrollo de la ecuación estimada.\nInterpretación de la ecuación estimada.\n\nValidación completa del modelo\n\nSignificatividad de los coeficientes (contraste de Wald).\nHipótesis de homocedasticidad e incorrelación.\nValores extremos.\nCálculo de la calidad del ajuste y de las predicciones.\n\nPrevisión de la serie (al menos tres años, 36 meses)\nValoración crítica del modelo ARIMA respecto de un modelo de alisado:\n\nComparación del modelo estimado en esta práctica con el modelo de la práctica 2\nComparación de la calidad de los modelos.\nComparación de las previsiones."
  },
  {
    "objectID": "03-10-Ejemplo5.html",
    "href": "03-10-Ejemplo5.html",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "",
    "text": "1 Introducción\nConsideremos de nuevo la serie temporal correspondiente al número de defunciones causadas por enfermedades cerebrovasculares, Esta serie está disponible en el Instituto Nacional de Estadística desde enero de 1980 hasta diciembre de 2020, un total de 41 años o 492 meses\nEn la descriptiva vimos que la descomposición revelaba la presencia de varios valores atípicos concentrados al inicio de la serie. Por este motivo, para su análisis por modelos ARIMA vamos a recortar la serie, que empezará el enero de 1988.\nTambién hemos visto que para alcanzar la estacionariedad y verificar la hipótesis de ergodicidad es necesario diferenciar la serie tanto en la parte regular como estacional y decidimos usar la transformación logarítmica para linealizar la serie y ganar en interpretabilidad.\n\nDefEnfCer <- read.csv2(\"./series/Enfermedades cerebrovasculares.csv\", \n                       header = TRUE)\n\nDefEnfCer <- ts(DefEnfCer[,2], \n                start = 1980, \n                freq = 12)\n\nDefEnfCer <- window(DefEnfCer, \n                    start = 1988)\n\n\n\n\n\n\n2 Identificación\nVamos a identificar los valores de \\(p\\), \\(q\\), \\(P\\) y \\(Q\\). Para ello, analizaremos la FAC y la FACP, y solicitaremos con auto.arima y seas una identificación automática. Con auto.arima incluiremos variables ficticias para los cuatro valores atípicos ya identificados cuando aplicamos Alisado Exponencial: febrero de 1999, junio y agosto de 2003, y febrero de 2012.\n\nggtsdisplay(diff(diff(log(DefEnfCer), lag = 12)), lag = 48)\n\n\n\n\nFigura 1: Defunciones anuales por enfermedades cerebrovasculares, FAC y FACP de la serie transformada\n\n\n\n\nEn la parte regular, la FAC muestra que la primera autocorrelación está por encima del IC95 y en la FACP se observa decrecimiento. En la parte estacional, la FAC muestra una autocorrelación significativa en el orden 12 y la FACP muestra decrecimiento. Así, podemos identificar el proceso como \\(log(DefEnfCer_t) \\sim ARIMA_{12}(0,1,1)(0,1,1)\\).\nVeamos ahora auto.arima\n\nd0299 <- 1*(cycle(DefEnfCer) == 2 & trunc(time(DefEnfCer)) == 1999)\nd0603 <- 1*(cycle(DefEnfCer) == 6 & trunc(time(DefEnfCer)) == 2003)\nd0803 <- 1*(cycle(DefEnfCer) == 8 & trunc(time(DefEnfCer)) == 2003)\nd0212 <- 1*(cycle(DefEnfCer) == 2 & trunc(time(DefEnfCer)) == 2012)\n\nauto.arima(DefEnfCer, \n           d = 1, \n           D = 1,\n           lambda = 0,\n           xreg = cbind(d0299, d0603, d0803, d0212))\n\nSeries: DefEnfCer \nRegression with ARIMA(3,1,2)(2,1,2)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ar1     ar2     ar3      ma1      ma2     sar1     sar2     sma1\n      -0.6227  0.1144  0.0338  -0.0996  -0.7886  -0.8659  -0.2257  -0.1695\ns.e.   0.1119  0.0748  0.0603   0.0992   0.0909   0.1714   0.0648   0.1681\n         sma2   d0299   d0603   d0803   d0212\n      -0.5279  0.1964  0.1560  0.1524  0.2252\ns.e.   0.1449  0.0409  0.0403  0.0407  0.0407\n\nsigma^2 = 0.002055:  log likelihood = 637.19\nAIC=-1246.38   AICc=-1245.24   BIC=-1191.11\n\n\nLa función auto.arima identifica un complejo proceso \\(ARIMA_{12}(3,1,2)(2,1,2)\\). La identificación alcanzada por seas es un proceso \\(ARIMA_{12}(1,1,1)(1,1,1)\\), con dos intervenciones en febrero de 1999 y 2012.\n\nsummary(seas(DefEnfCer))\n\n\nCall:\nseas(x = DefEnfCer)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(>|z|)    \nAO1999.Feb         0.18988    0.04252   4.466 7.97e-06 ***\nAO2012.Feb         0.21523    0.04261   5.051 4.39e-07 ***\nAR-Nonseasonal-01  0.23806    0.05382   4.423 9.72e-06 ***\nAR-Seasonal-12    -0.19430    0.05499  -3.533  0.00041 ***\nMA-Nonseasonal-01  0.94119    0.01958  48.071  < 2e-16 ***\nMA-Seasonal-12     0.83840    0.03088  27.151  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (1 1 1)(1 1 1)  Obs.: 396  Transform: log\nAICc:  4854, BIC:  4881  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 17.24   Shapiro (normality): 0.987 **\n\n\nVamos a partir de la identificación obtenida por seas (más parsimoniosa que la de auto.arima), incluyendo las variables ficticias que afectan un único mes: \\(log(DefEnfCer) \\sim ARIMA_{12}(1,1,1)(1,1,1) + AI\\).\n\n\n\n\n\n3 Estimación\nLa siguiente salida muestra el modelo estimado y la Figura 2 permite analizar la presencia de más valores extremos.\n\nDefEnfCerArima1 <- Arima(DefEnfCer, \n                         order = c(1, 1, 1),  \n                         seasonal = c(1, 1, 1),\n                         lambda = 0,\n                         cbind(d0299, d0603, d0803, d0212))\nDefEnfCerArima1\n\nSeries: DefEnfCer \nRegression with ARIMA(1,1,1)(1,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n         ar1      ma1     sar1     sma1   d0299   d0603   d0803   d0212\n      0.2216  -0.9459  -0.1874  -0.8429  0.1908  0.1551  0.1439  0.2155\ns.e.  0.0560   0.0195   0.0605   0.0384  0.0422  0.0417  0.0420  0.0419\n\nsigma^2 = 0.002085:  log likelihood = 631.98\nAIC=-1245.96   AICc=-1245.48   BIC=-1210.43\n\n\n\nerror <- residuals(DefEnfCerArima1)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1988, 2020, 2)) \n\n\n\n\nFigura 2: Error + Intervención\n\n\n\n\nSe observan dos valores claramente atípicos en mayo de 2001 y enero de 2015. Además, hay otros candidatos a valor extremo, entre los que destacan marzo de 1989, julio de 1993, enero de 2005 y febrero de 2015. Procederemos a incluirlos en el modelo.\n\nd0501 <- 1*(cycle(DefEnfCer) == 5 & trunc(time(DefEnfCer)) == 2001)\nd0115 <- 1*(cycle(DefEnfCer) == 1 & trunc(time(DefEnfCer)) == 2015)\nd0389 <- 1*(cycle(DefEnfCer) == 3 & trunc(time(DefEnfCer)) == 1989)\nd0793 <- 1*(cycle(DefEnfCer) == 7 & trunc(time(DefEnfCer)) == 1993)\nd0105 <- 1*(cycle(DefEnfCer) == 1 & trunc(time(DefEnfCer)) == 2005)\nd0215 <- 1*(cycle(DefEnfCer) == 2 & trunc(time(DefEnfCer)) == 2015)\n\nDefEnfCerArima2 <- Arima(DefEnfCer, \n                         order = c(1, 1, 1),  \n                         seasonal = c(1, 1, 1),\n                         lambda = 0,\n                         xreg = cbind(d0389, d0793, d0299, d0501, d0603, \n                                      d0803, d0105, d0212, d0115, d0215))\nDefEnfCerArima2\n\nSeries: DefEnfCer \nRegression with ARIMA(1,1,1)(1,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n         ar1      ma1     sar1     sma1    d0389    d0793   d0299   d0501\n      0.1950  -0.9429  -0.1561  -0.8098  -0.0757  -0.0814  0.1929  0.1124\ns.e.  0.0581   0.0203   0.0650   0.0432   0.0415   0.0403  0.0404  0.0395\n       d0603   d0803   d0105   d0212   d0115   d0215\n      0.1525  0.1506  0.1273  0.2298  0.1136  0.1611\ns.e.  0.0399  0.0402  0.0403  0.0402  0.0411  0.0409\n\nsigma^2 = 0.001919:  log likelihood = 652.5\nAIC=-1275   AICc=-1273.69   BIC=-1215.78\n\n\nAparentemente las variables de intervención incluidas son significativas, exceptuando la de marzo de 1989. Este hecho lo contrastaremos más adelante.\nEl análisis gráfico del residuo indica que aún hay candidatos a valores atípicos (véase Figura 3). Aunque ninguno alcanza las tres desviaciones típicas, al menos dos de ellos están muy cerca. Sin embargo, vamos a dar por concluido este proceso.\n\nerror <- residuals(DefEnfCerArima2)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1988, 2020, 2)) \n\n\n\n\nFigura 3: Error + Intervención\n\n\n\n\nSe observa que tres de los valores atípicos corresponden al mes de febrero y que sus coeficientes estimados toman valores parecidos. También has dos meses de enero atípicos con similar efecto. Vamos a asumir que la causa que hay detrás del valor anómalo en los meses de febrero es la misma, posiblemente un invierno más frío de lo usual. Lo mismo asumiremos para los valores atípicos en enero. Esto nos permite agrupar variables de intervención y simplificar el modelo.\n\nd01aa <- d0105 + d0115\nd02aa <- d0299 + d0212 + d0215\n\nDefEnfCerArima3 <- Arima(DefEnfCer, \n                         order = c(1, 1, 1),  \n                         seasonal = c(1, 1, 1),\n                         lambda = 0,\n                         xreg = cbind(d01aa, d02aa, \n                                      d0389, d0793, d0501, d0603, d0803))\nDefEnfCerArima3\n\nSeries: DefEnfCer \nRegression with ARIMA(1,1,1)(1,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n         ar1      ma1     sar1     sma1   d01aa   d02aa    d0389    d0793\n      0.2005  -0.9445  -0.1533  -0.8093  0.1246  0.1948  -0.0758  -0.0817\ns.e.  0.0570   0.0199   0.0647   0.0433  0.0290  0.0244   0.0416   0.0403\n       d0501   d0603   d0803\n      0.1126  0.1522  0.1504\ns.e.  0.0395  0.0399  0.0402\n\nsigma^2 = 0.001911:  log likelihood = 651.69\nAIC=-1279.39   AICc=-1278.55   BIC=-1232.01\n\n\nPor último, veamos si todos los coeficientes del modelo son significativos.\n\nancho <- max(nchar(names(coef(DefEnfCerArima3)))) + 2\nfor(i in 1:length(coef(DefEnfCerArima3))) {\n  wt <- wald.test(b = coef(DefEnfCerArima3), \n                  Sigma = vcov(DefEnfCerArima3), \n                  Terms = i)\n  cat(\"\\nCoeficiente: \", \n      format(names(coef(DefEnfCerArima3))[i], width = ancho), \n      \"valor de p: \",\n      formatC(wt$result$chi2[3], digits = 4, format = \"f\"))\n}\n\n\nCoeficiente:  ar1     valor de p:  0.0004\nCoeficiente:  ma1     valor de p:  0.0000\nCoeficiente:  sar1    valor de p:  0.0179\nCoeficiente:  sma1    valor de p:  0.0000\nCoeficiente:  d01aa   valor de p:  0.0000\nCoeficiente:  d02aa   valor de p:  0.0000\nCoeficiente:  d0389   valor de p:  0.0681\nCoeficiente:  d0793   valor de p:  0.0428\nCoeficiente:  d0501   valor de p:  0.0044\nCoeficiente:  d0603   valor de p:  0.0001\nCoeficiente:  d0803   valor de p:  0.0002\n\n\nAunque el coeficiente del efecto de marzo de 1989 no es significativo al 5%, si los es al 10% así que optamos por dejarlo.\n\n\n\n\n\n4 Validación\nAnalizando los criterios de bondad de ajuste se tiene que el error medio (ME), igual a -5.77, es prácticamente cero por lo que no parece que haya sesgo en las predicciones; en media nos equivocamos en 131 defunciones (RMSE); y el error porcentual medio es 3.3%, muy bajo.\n\naccuracy(DefEnfCerArima3)\n\n\n\n                ME   RMSE   MAE   MPE MAPE MASE ACF1\nTraining set -5.77 131.04 96.91 -0.25 3.29 0.58    0\n\n\n\n\n\nIncorrelación, Homocedasticidad y Normalidad\nVeamos ahora si el residuo es ruido blanco.\n\nerror <- residuals(DefEnfCerArima3)\nBox.test(error, lag = 2,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 0.63976, df = 2, p-value = 0.7262\n\nBox.test(error, lag = 24,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 15.856, df = 24, p-value = 0.8932\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 0.42582, df = 2, p-value = 0.8082\n\nBox.test(error^2, lag = 24, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 25.784, df = 24, p-value = 0.3642\n\njarque.bera.test(error) \n\n\n    Jarque Bera Test\n\ndata:  error\nX-squared = 0.67287, df = 2, p-value = 0.7143\n\n\nEl error muestra ser incorrelado, homocedástico y seguir una distribución normal.\n\n\n\n\n\n\n5 Ecuación del modelo identificado\nAhora que ya hemos dado por válido el modelo, veamos cuál es su ecuación.\nEl modelo teórico es\n\\[(1 - \\phi_1 L)(1 - \\phi_{12}L^{12})(1-L)(1-L^{12})log(DefEnfCer_t) =(1+\\theta_1 L)(1 + \\theta_{12}L^{12})\\varepsilon_t +AI.\\]\nDebido a la complejidad del modelo, no vamos a desarrollarlo ni interpretarlo en su parte estructural. Respecto de la intervención:\n\nEn los dos meses de enero atípicos, la defunciones fueron 12.5% mayores que las observadas en otros meses de enero.\nDe la misma forma, en los tres meses de febrero atípicos, la defunciones fueron un 19.5% mayores que las observadas en otros meses de febrero.\nEn mayo de 2001 hubo un aumento en las defunciones del 11.3% respecto de lo esperado; en junio de 2003 del 15.2% y en agosto de 2003 del 15%.\nEn marzo de 1989, las defunciones fueron un 7.6% menores de lo esperado y en julio de 1993 un 8.2% menor de los esperado.\n\n\n\n\n\n\n6 Predicción de las defunciones por enfermedad cerebrovascular\nUna vez dado por válido el modelo, podemos pasar a realizar predicciones para los próximos años. Como la variables de intervención no son efectos calendario sus valores previstos serán cero.\n\npDefEnfCerArima3 <- forecast(DefEnfCerArima3, \n                             h = 60,\n                             xreg = cbind(rep(0, 60), rep(0, 60), rep(0 ,60), \n                                          rep(0 ,60), rep(0, 60), rep(0, 60), \n                                          rep(0, 60)), \n                             level = 95)\n\naggregate(pDefEnfCerArima3$mean, FUN = sum)\n\nTime Series:\nStart = 2021 \nEnd = 2025 \nFrequency = 1 \n[1] 25125.74 24771.38 24372.46 23987.53 23607.54\n\n\n\nautoplot(pDefEnfCerArima3, \n         xlab = \"\",\n         ylab = \"Defunciones\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1988, 2026, 4)) \n\n\n\n\nFigura 4: Defunciones (1988-2020) y predicción (2021-2025)\n\n\n\n\nA partir de 2022 se espera que el total de defunciones por enfermedad cerebrovascular caiga por debajo de los 25000 casos.\n\n\n\n\n\n7 Comparación con Alisado Exponencial\nEl método de Alisado exponencial, aplicado sobre el logaritmo de las defunciones identifica un proceso (A,A,A) con \\(\\alpha=0.089\\) y \\(\\beta = \\gamma = 0\\). La raíz del error cuadrático medio (RMSE) es de 156 casos y el error porcentual (MAPE) del 3.7%. Estos valores son algo superiores a los obtenidos con el modelo Arima (131 y 3.3%, respectivamente).\n\nsummary(ets(DefEnfCer, lambda = 0))\n\nETS(A,A,A) \n\nCall:\n ets(y = DefEnfCer, lambda = 0) \n\n  Box-Cox transformation: lambda= 0 \n\n  Smoothing parameters:\n    alpha = 0.0893 \n    beta  = 1e-04 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 8.2282 \n    b = -0.0014 \n    s = 0.1321 -0.0034 -0.0439 -0.1616 -0.0914 -0.0524\n           -0.1076 -0.0407 -0.0269 0.0931 0.0836 0.219\n\n  sigma:  0.0505\n\n     AIC     AICc      BIC \n22.01631 23.63536 89.70035 \n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE      ACF1\nTraining set 2.244036 155.9707 110.6407 -0.1694108 3.679888 0.6601516 0.2235112\n\n\nVamos a determinar si la aplicación de modelos Arima mejora la calidad de las predicciones lo suficiente como para justificar su uso –frente a los métodos de alisado, mucho más sencillos. Para ello, aplicaremos la metodología de origen de predicción móvil para estimar la capacidad predictiva del modelo Arima y compararla con el modelo de Alisado.\n\nk <- 120                   \nh <- 12                    \nT <- length(DefEnfCer)     \ns <- T - k - h               \n\nmapeArima <- matrix(NA, s + 1, h)\nmapeAlisado <- matrix(NA, s + 1, h)\n\nX <- data.frame(cbind(d01aa, d02aa, d0389, d0793, d0501, d0603, d0803))\n\nfor (i in 0:s) {\n  train.set <- subset(DefEnfCer, start = i + 1, end = i + k)\n  test.set <-  subset(DefEnfCer, start = i + k + 1, end = i + k + h) \n  \n  X.train <- data.frame(X[(i + 1):(i + k),])\n  hay <- colSums(X.train)\n  X.train <- X.train[, hay>0]\n  \n  X.test <- data.frame(X[(i + k + 1):(i + k + h),])\n  X.test <- X.test[, hay>0]\n  \n  if (length(X.train) > 0) {\n    fit <- try(Arima(train.set, \n                     order = c(1, 1, 1),\n                     seasonal = c(1, 1, 1), \n                     lambda = 0,\n                     xreg = as.matrix(X.train)), silent = TRUE)\n  } else {\n    fit <- try(Arima(train.set, \n                     order = c(1, 1, 1),\n                     seasonal =  c(1, 1, 1),\n                     lambda = 0), silent = TRUE)\n  }\n  \n  if (!is.element(\"try-error\", class(fit))) {\n    if (length(X.train) > 0) \n      fcast <- forecast(fit, h = h, xreg = as.matrix(X.test)) \n    else\n      fcast <- forecast(fit, h = h)\n    \n    mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  }\n  \n  fit <- ets(train.set, lambda = 0, model = \"AAA\", damped = FALSE)\n  fcast<-forecast(fit, h = h)\n  mapeAlisado[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n}\n  \nerrorArima <- colMeans(mapeArima, na.rm = TRUE)\nerrorArima\n\n [1] 3.500559 3.717206 3.737817 3.814593 3.866561 3.847458 3.693418 3.792541\n [9] 3.836325 3.770663 3.796191 3.891282\n\nerrorAlisado <- colMeans(mapeAlisado)\nerrorAlisado\n\n [1] 3.831531 4.290697 4.411337 4.497644 4.580066 4.730074 4.778655 4.851094\n [9] 4.659138 4.570610 4.559090 4.560527\n\n\n\ndatos <- data.frame(\n  factor = c(rep(\"Arima\", 12), rep(\"Alisado\", 12)),\n  x = c(1:12,1:12),\n  y = c(errorArima, errorAlisado)\n)\n\nggplot(datos, aes(x = x, y = y,  colour= factor)) + \n  geom_line() +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"%\") +\n  scale_x_continuous(breaks= 1:12) +\n  labs(colour = \"Métodos\") + \n  theme(legend.position=c(0.1,0.8)) \n\n\n\n\nFigura 5: Error de predicción (MAPE) según horizonte temporal\n\n\n\n\nLa Figura 5 revela que Arima siempre es superior a Alisado en calidad de predicciones, con un error inferior en promedio de 0.75 puntos porcentuales. Además, en el modelo Arima el error se mantiene prácticamente constante."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-tendencia",
    "href": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-tendencia",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.1 Análisis de la tendencia",
    "text": "2.1 Análisis de la tendencia\nSi anualizamos la serie podemos, por un lado, identificar mejor en que años se producen los cambios en la tendencia y, por otro lado, poner cifras al volumen de pasajeros en transporte urbano.\n\nPasajerosAnual <- aggregate(Pasajeros, FUN = sum)\n\nautoplot(PasajerosAnual, colour = \"darkblue\",\n         xlab = \"\",\n         ylab = \"Millones de pasajeros\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 2: Pasajeros en transporte urbano (datos anuales)\n\n\n\n\nLa Figura 2 muestra el volumen anual de pasajeros en transporte urbano. El crecimiento continuado, posiblemente iniciado antes de 1996 y que permitió superar los 3000 millones de pasajeros en 2007, se ve interrumpido con el inicio de la pasada crisis económica. La caída en el número de pasajeros se interrumpe en 2014, año que marca la salida de la Gran recesión y el inicio de la recuperación en el serie. En 2019 se superaron los 3100 millones de pasajeros.\nEl incremento en el uso del transporte urbano observado antes y después de la crisis puede tener distintas causas: un uso más intensivo del transporte urbano en detrimento de otros medios de transporte, una reorganización de los servicios de transporte urbano que haya mejorado la conectividad dentro de los municipios, o un aumento en el número de líneas de autobús, tranvía o metro en determinadas ciudades.\nLa causa del repunte aislado observado el año 2011, en plena crisis, fue una ligera recuperación de la economía que tuvo lugar a finales de 2010 y principios de 2011."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-estacionalidad",
    "href": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-estacionalidad",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.2 Análisis de la estacionalidad",
    "text": "2.2 Análisis de la estacionalidad\nLa principal causa de la estacionalidad observada en la serie es la estructura vacacional de la sociedad, especialmente caracterizada por las vacaciones de verano (julio a septiembre) y las vacaciones de Semana Santa (en marzo y/o abril, según el año). Además, debido a que el transporte urbano se usa principalmente para ir a trabajar, también influye el número de días laborables del mes. Por ejemplo, en 2017 el mes de junio tuvo 22 días laborables, mientras que en 2019 tuvo 20 días laborables. Esta diferencia de dos días tendrá un efecto sobre el volumen de pasajeros.\nEl número de días laborables de un mes viene marcado por los fines de semana del mes y por las festividades nacionales. Es cierto que el sábado se trabaja en diversos sectores (comercio, ocio, distribución) pero la caída en el número de trabajadores respecto de los días entre semana (lunes a viernes) es muy notable. También es cierto que, además de las festividades nacionales, hay muchas festividades autonómicas o municipales que podrían afectar al volumen de pasajeros en transporte urbano. Por ejemplo, las festividades regionales en comunidades como Madrid o Cataluña pueden tener un efecto significativo sobre la serie Pasajeros. Sin embargo, las festividades no nacionales no se van a tener en cuenta.\nPor tanto, para realizar un análisis detallado de la estacionalidad, es necesario crear una serie con el número de días laborables de cada mes. Además, esta serie se usará más adelante para modelizar y predecir la serie Pasajeros.\n\nDías laborables de cada mes\nLa librería timeData proporciona una serie de funciones que permiten definir un calendario de festividades, identificar los fines de semana y, a partir de aquí, crear la serie de días laborables (véase código más abajo).\n\nCon timeCalendar se definen las festividades nacionales que vamos a considerar: Año nuevo (1 de enero), Reyes (6 de enero), Viernes Santo (fecha variable), Día del Trabajo (1 de mayo), Día de la Asunción (15 de agosto), Día de la Hispanidad (12 de octubre), Día de Todos los Santos (1 de noviembre), la Constitución (6 de diciembre) la Inmaculada Concepción (8 de diciembre) y Navidad (25 de diciembre).\nPor claridad, cada festivo se ha definido de forma independiente para después crear una variable con todas las festividades (FestivosNacionales).\nEl rango para todos los cálculos va desde 1996 hasta 2024, que incluye el rango de la serie Pasajeros más cinco años de predicción.\nLa función utilizada Easter de la librería timeDate difiere de la función easter de forecast.\nA continuación, con timeSequence se crea una serie diaria desde el 1 de enero de 1996 hasta el 31 de diciembre de 2024.\nLas dos siguientes líneas eliminan de la serie diaria los festivos y los fines de semana, (función isBizday), para después dar a esta nueva serie el formato año-mes eliminando el día. De esta forma, la serie de días laborales tendrá el mismo identificador para todos los días del mismo mes.\nDespués, se crea una tabla que, por la naturaleza de la serie de días laborales, tendrá para cada año-mes el numero de días laborables. Por último fechamos la tabla, que es nuestra serie de días laborables y mostramos algunos datos.\nLas dos últimas líneas de código dividen la serie en el periodo muestral y el de predicción.\n\n\n\n\nAnoNuevo <- timeCalendar(d = 1, m = 1, y = 1996:2024)\nReyes <- timeCalendar(d = 6, m = 1, y = 1996:2024)\nViernesSanto <- Easter(1996:2024, shift = -2)\nDiaTrabajo <- timeCalendar(d = 1, m = 5, y = 1996:2024)\nAsuncion <- timeCalendar(d = 15, m = 8, y = 1996:2024)\nHispanidad <- timeCalendar(d = 12, m = 10, y = 1996:2024)\nTodoSantos <- timeCalendar(d = 1, m = 11, y = 1996:2024)\nConstitucion <- timeCalendar(d = 6, m = 12, y = 1996:2024)\nInmaculada <- timeCalendar(d = 8, m = 12, y = 1996:2024)\nNavidad <- timeCalendar(d = 25, m = 12, y = 1996:2024)\n\nFestivosNacionales <- c(AnoNuevo, Reyes, ViernesSanto,\n                        DiaTrabajo, Asuncion,  Hispanidad, TodoSantos, \n                        Constitucion, Inmaculada, Navidad)\n\nfechaDiaria <- timeSequence(from = \"1996-01-01\", to = \"2024-12-31\")\nbiz <- fechaDiaria[isBizday(fechaDiaria, holidays = FestivosNacionales)]\nbizdays <- format(biz, format = \"%Y-%m\")\n\nDiasLaborables <- table(bizdays)\nDiasLaborables <- ts(DiasLaborables, start = 1996, frequency = 12)\n\nsubset(DiasLaborables, start = 289) #Mostramos solo los 5 últimos años\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2020  21  20  22  21  20  22  23  21  22  21  21  21\n2021  19  20  23  21  21  22  22  22  22  20  21  21\n2022  20  20  23  20  22  22  21  22  22  20  21  20\n2023  21  20  23  19  22  22  21  22  21  21  21  18\n2024  22  21  20  22  22  20  23  21  21  23  20  20\n\npDiasLaborables <- subset(DiasLaborables, start = length(DiasLaborables) - 59)\nDiasLaborables <- subset(DiasLaborables, end = length(DiasLaborables) - 60)\n\nEs conveniente indicar que la identificación de las festividades nacionales dista de ser perfecta por varios motivos:\n\nalgunos festivos nacionales si caen en domingo, se pasan a lunes (por ejemplo Reyes y la Inmaculada de 2019), aspecto que no se ha tenido en cuenta.\nalgunos festivos nacionales pueden ser sustituidos por otros días por las Comunidades Autónomas, por ejemplo Reyes o Jueves Santo.\n\n\n\n\n\nAnálisis gráfico de la estacionalidad\nVeamos ahora una descriptiva detallada de la estacionalidad de la serie Pasajeros, haciendo especial hincapié en el efecto de las vacaciones (verano y Semana Santa) y el número de días laborables. Para ello, mostraremos gráficamente las subseries definidas por el mes tanto para Pasajeros como para Pasajeros por día laborable, esta segunda resultado de dividir Pasajeros por DiasLaborables.\n\nPasajerosDL <- Pasajeros/DiasLaborables\n\nggsubseriesplot(Pasajeros) +\n  ylab(\"Millones de pasajeros\") +\n  xlab(\"\") +\n  ggtitle(\"\")\n\nggsubseriesplot(PasajerosDL) +\n  ylab(\"Millones de pasajeros\") +\n  xlab(\"\") +\n  ggtitle(\"\")\n\n\n\n\n\n\n\n(a) Pasajeros\n\n\n\n\n\n\n\n\n\n(b) Pasajeros por día laborable\n\n\n\n\nFigura 3: Estacionalidad de la serie Pasajeros\n\n\n\nLa Figura 3 muestra para cada mes la serie de pasajeros (total o por día laborable) y el valor medio (línea azul horizontal). En ambos paneles se identifica perfectamente el efecto de los periodos vacacionales sobre el transporte urbano de pasajeros. En las vacaciones de verano se observa una fuerte caída en el número de pasajeros, especialmente en agosto y, en menor medida, en julio y septiembre. Por otro lado, las subseries de marzo y abril muestran mucha más irregularidad que las de otros meses debido a que el volumen de pasajeros depende de cómo ha caído la Semana Santa. Si un año esta cae en marzo, ese mes presentará un volumen de pasajeros inferior al de los meses de marzo sin Semana Santa, mientras que en abril se dará el efecto contrario. Diciembre, para ser un mes de 31 días, presenta también un reducido número de pasajeros debido a las vacaciones navideñas.\nLa Figura 3 a) muestra el efecto estacional total debido al número de días del mes y de días laborales. Por ejemplo, en febrero, el mes con menos días y por tanto con menos días laborales, en media se transportan menos pasajeros, comparado con enero o marzo. Octubre, un mes con 31 días, muestra un volumen medio de pasajeros mayor que noviembre de 30 días.\nEn la Figura 3 b) se ha eliminado el efecto de los días laborables al trabajar con la serie de pasajeros transportados por día laborable. Si la comparamos con la Figura 3 a), destaca que las diferencias entre las medias (lineas azules) se han reducido: prácticamente no hay diferencias entre los meses de enero a junio, o entre los meses de octubre a diciembre.\nCabría pensar que al excluir de la serie de días laborables la Semana Santa, en la Figura 3 b) las subseries de marzo y abril deberían ser tan suaves como las observadas para otro meses, pero no es así. Claramente la simple exclusión de los festivos nacionales de Semana Santa no es suficiente para recoger bien su efecto sobre el transporte urbano. La razón hay que buscarla en las vacaciones escolares de este periodo, que en algunas comunidades autónomas tiene lugar durante la propia semana de Semana Santa, mientras que en otras comunidades tiene lugar en la semana posterior. De esta forma, el efecto sobre el transporte urbano de Semana Santa no es homogéneo en el territorio nacional y resulta difícil incluirlo en el análisis de la serie Pasajeros.\n\n\n\n\nAnálisis numérico de la estacionalidad\nPodemos obtener la componente estacional de forma sencilla para poder valorarla numéricamente y ver que efecto tiene el número de días laborables. Previamente, debemos determinar el esquema, aditivo o multiplicativo, de la serie.\nLa gráfica media-desviación típica (Figura 4) refuerza la impresión que se obtenía de la gráfica de la serie (Figura 1), que el esquema es aditivo.\n\nMediaAnual <- aggregate(Pasajeros, FUN = mean)\nDesviacionAnual <- aggregate(Pasajeros, FUN = sd)\n\nggplot() +\n  geom_point(aes(x = MediaAnual, y = DesviacionAnual), size = 2) +\n  xlab(\"Media de pasajeros por año\") + \n  ylab(\"Desviación típica de pasajeros por año\") + \n  ggtitle(\"\")\n\n\n\n\nFigura 4: Identificación del tipo de esquema\n\n\n\n\n\nPasajerosMedia <- tapply(Pasajeros - mean(Pasajeros), \n                         cycle(Pasajeros), \n                         mean)\n\nPasajerosDLMedia <- tapply((PasajerosDL - mean(PasajerosDL)), \n                           cycle(PasajerosDL), \n                           mean)\n\ndatos <- cbind(PasajerosMedia, PasajerosDLMedia)\ncolnames(datos) <- c(\"Pasajeros\", \"Pasajeros por día laborable\")\nrownames(datos) <- meses\n\nkable(datos, \n      digits = 2)\n\n\n\nTabla 1: Efecto estacional\n\n\n\nPasajeros\nPasajeros por día laborable\n\n\n\n\nEne\n3.19\n0.32\n\n\nFeb\n-0.36\n0.46\n\n\nMar\n17.71\n0.47\n\n\nAbr\n3.36\n0.35\n\n\nMay\n19.11\n0.70\n\n\nJun\n6.30\n0.17\n\n\nJul\n-15.09\n-1.22\n\n\nAgo\n-71.64\n-3.51\n\n\nSep\n-7.75\n-0.50\n\n\nOct\n27.60\n1.05\n\n\nNov\n15.36\n0.97\n\n\nDic\n2.20\n0.74\n\n\n\n\n\n\nLa Tabla 1 pone en cifras el efecto estacional sobre los Pasajeros (primera columna): en agosto la caída en el número de pasajeros, respecto de la media anual, se cifra en 72 millones de pasajeros. En julio, septiembre y en menor medida febrero también el uso del transporte urbano es inferior a la media anual, en el caso de los dos primeros meses por las vacaciones de verano y en febrero debido a ser el mes con menos días del año. Por otro lado, destaca el elevado número de pasajeros en los meses de marzo, mayo y octubre, por tener 31 días, y noviembre, por razones desconocidas.\nTras la corrección por el número de días laborales, el efecto estacional es más suave (véase la segunda columna en la Tabla 1). Ahora, los meses de febrero y marzo tienen un efecto similar, al igual que octubre y noviembre. También se observa que las diferencias entre marzo y abril se han reducido."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#descomposición-de-la-serie",
    "href": "03-11-Ejemplo-Pasajeros.html#descomposición-de-la-serie",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.3 Descomposición de la serie",
    "text": "2.3 Descomposición de la serie\nYa hemos realizado una descripción detallada de las principales componentes de la serie, tendencia y estacionalidad. Ahora vamos a proceder a descomponerla a fin de analizar, aunque sea de forma gráfica, el error y tener ya una primera impresión sobre la relevancia de la componente de intervención en Pasajeros.\nDado que la serie presenta un esquema aditivo, usaremos el método de descomposición por regresiones locales ponderadas, asumiendo una componente estacional constante y considerando la presencia de posibles valores extremos.\n\nPasajerosStl <- stl(Pasajeros[,1], \n                    s.window = \"periodic\", \n                    robust = TRUE)\n\nerror <- remainder(PasajerosStl)\nsderror <- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(3, 2, -2, -3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"),\n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 5: Error + Intervención. Descomposición de Pasajeros\n\n\n\n\nLa Figura 5 muestra el error de la descomposición y los intervalos de confianza al 95% (líneas verdes) y el 99.7% (líneas rojas). Se aprecian claramente múltiples valores extremos (superan las tres desviaciones típicas) en forma de compensación (dos errores extremos consecutivos de signo opuesto) que corresponden a los meses de marzo y abril de 1997, 2002, 2008 y 2013, y otro valor extremo en abril de 2005. Además, en marzo y abril de 2016 hay dos valores atípicos. Nótese que todos los valores identificados corresponden a los meses de marzo y abril, y en todos los casos el error negativo tiene lugar en marzo y el positivo en abril. Si miramos un calendario veremos que tienen lugar en los años en que Semana Santa cayó en marzo.\nSi repetimos este análisis para la serie de Pasajeros por día laborable, los resultados son bien diferentes (véase Figura 6). Ahora solo se detectan dos valores extremos en diciembre de 2000 y 2006. También destaca el error de diciembre de 2017. Los errores en diciembre se dan cuando Navidad cae en lunes, de forma que la caída en el transporte urbano debida a la nochebuena coincide con la de cualquier domingo. Así, estos meses de diciembre presentan más transporte urbano que los meses de diciembre donde la nochebuena cae entre semana.\n\nPasajerosStl <- stl(PasajerosDL[,1], \n                    s.window = \"periodic\", \n                    robust = TRUE)\n\nerror <- remainder(PasajerosStl)\nsderror <- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(3, 2, -2, -3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"),\n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 6: Error + Intervención. Descomposición de Pasajeros por día laborable\n\n\n\n\nPara la identificación de los valores extremos se ha hecho uso de las funciones easter del paquete forecast y dayOfWeek del paquete timeDate."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#conclusión",
    "href": "03-11-Ejemplo-Pasajeros.html#conclusión",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.4 Conclusión",
    "text": "2.4 Conclusión\nLa serie de pasajeros en transporte urbano muestra una tendencia creciente solo interrumpida entre 2008 y 2013 debido a la Gran Recesión.\nLos principales determinantes de la estacionalidad de la serie Pasajeros son los grandes periodos vacacionales en España (Semana Santa y verano) y el número de días laborales del mes. Así, en la serie Pasajeros corregida por días laborables la componente estacional se ha suavizado y prácticamente queda determinada por las vacaciones.\nLa intervención tiene lugar en los meses de marzo y abril debido al carácter móvil de la Semana Santa, y en diciembre cuando el día de Navidad cae en lunes de forma que la caída de pasajeros de nochebuena se solapa con la de cualquier domingo."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-serie-pasajeros",
    "href": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-serie-pasajeros",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.1 Análisis de la serie Pasajeros",
    "text": "4.1 Análisis de la serie Pasajeros\n\n\n\nEstimación e interpretación\nEl modelo óptimo, estimado con la función ets sin imponer ninguna restricción, es ETS(M,Ad,A): pendiente aditiva con amortiguamiento, estacionalidad aditiva y residuo multiplicativo. \\[y_{t+1} = (l_t + \\phi b_t + s_{t+1-m}) \\cdot (1 + \\varepsilon_{t+1}).\\]\n\nPasajerosEts <- ets(Pasajeros)\nsummary(PasajerosEts) \n\nETS(M,Ad,A) \n\nCall:\n ets(y = Pasajeros) \n\n  Smoothing parameters:\n    alpha = 0.1374 \n    beta  = 0.017 \n    gamma = 1e-04 \n    phi   = 0.9438 \n\n  Initial states:\n    l = 203.1964 \n    b = -0.4508 \n    s = 2.0753 14.0423 27.0229 -8.7953 -72.369 -15.0625\n           6.6935 19.5129 4.2863 18.0168 0.373 4.2039\n\n  sigma:  0.0333\n\n     AIC     AICc      BIC \n2823.866 2826.408 2889.799 \n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.5929553 7.492818 5.654414 0.1765693 2.447442 0.6844865\n                   ACF1\nTraining set -0.1434247\n\n\nEl valor de \\(\\phi=\\) 0.94 indica que la inclusión de amortiguamiento en el modelo mejora sensiblemente su ajuste a los datos. Por otro lado, \\(\\gamma\\) es técnicamente cero, indicando que el efecto estacional se mantiene constante en el tiempo. Sin embargo, el valor de \\(\\beta\\), reducido pero no nulo, indica que la pendiente cambia en el tiempo de forma muy lenta.\nLa calidad del ajuste es bastante buena, con un error porcentual del 2.4% o un error de 7.5 millones de pasajeros (RMSE). La aplicación del método de alisado supone una reducción de un punto en el error porcentual respecto del método ingenuo, o una reducción de 3.5 millones de pasajeros. Es decir, el modelo de alisado exponencial supone una mejora en la calidad del ajuste del 32% respecto del método ingenuo con estacionalidad visto en el epígrafe previo (MASE). El indicador ACF1 revela que las fórmulas usadas para el intervalo de confianza no son válidas.\nEl efecto estacional, que recordemos se mantiene constante en el tiempo, es prácticamente idéntico al estimado en la descriptiva y viene determinado por los periodos vacacionales y el número de días del mes (y por consiguiente el número de días laborables). Véase la Figura 8.\nEn verano (julio a septiembre) el uso del transporte urbano es inferior a la media anual, destacando agosto con un descenso de 72 millones de pasajeros. Por el contrario, octubre destaca por ser el mes con mayor incremento en el volumen de pasajeros (27 millones) respecto de la media anual.\n\nPasajerosEtsEst <- PasajerosEts$states[nrow(PasajerosEts$states), 14:3]\nnames(PasajerosEtsEst) <- meses\n\nround(PasajerosEtsEst, 2)\n\n   Ene    Feb    Mar    Abr    May    Jun    Jul    Ago    Sep    Oct    Nov \n  4.21   0.37  18.02   4.29  19.51   6.69 -15.06 -72.37  -8.79  27.02  14.04 \n   Dic \n  2.07 \n\nggplot() +\n  geom_line(aes(x = 1:12, y = PasajerosEtsEst), colour = \"darkblue\") + \n  geom_hline(yintercept = 0, colour = \"black\", lty = 2) +\n  ggtitle(\"\") +\n  xlab(\"\") +\n  ylab(\"Efecto estacional\") +\n  scale_x_continuous(breaks= 1:12, \n                     labels = meses)\n\n\n\n\nFigura 8: Componente estacional estimada con Alisado exponencial\n\n\n\n\n\n\n\n\nPredicción\nPodemos ahora pedir los valores de predicción para los próximos cinco años. No mostramos los resultados numéricos, pero si gráficos (Figura 9). Las predicciones muestran una tendencia creciente amortiguada y, por tanto, no tan acusada como la observada en los años precedentes.\n\nPasajerosEtsPre <- forecast(PasajerosEts, \n                            h = 60)\n\nautoplot(PasajerosEtsPre,\n         xlab = \"\",\n         ylab = \"Millones de pasajeros\",\n         main = \"\",\n         PI = FALSE) \n\n\n\n\nFigura 9: Pasajeros (1996-2019) y predicción (2020-2024). Método de Alisado Exponencial\n\n\n\n\nEn el año 2020 se esperan 3153 millones de pasajeros, un 1.6% más que en 2019.2\n\n\n\n\nAnálisis del error\nEl residuo del modelo (Figura 10) muestra varios valores que pueden ser considerados como atípicos y que se dan siempre en los meses de marzo y abril para los años donde la Semana Santa recayó en marzo.\n\nerror <- residuals(PasajerosEts)\nsderror <- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 10: Error + Intervención. Método de Alisado Exponencial"
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#otras-alternativas-de-análisis",
    "href": "03-11-Ejemplo-Pasajeros.html#otras-alternativas-de-análisis",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.2 Otras alternativas de análisis",
    "text": "4.2 Otras alternativas de análisis\nEn la descriptiva se ha visto que la serie de pasajeros por día laborable tiene un comportamiento más suave (la componente estacional era más plana) y presentaba un menor número de valores atípicos que la serie original Pasajeros. Cabe esperar, por tanto, que esta serie presente un mejor ajuste con los métodos de Alisado Exponencial y ofrezca mejores predicciones.\nPor otro lado, siempre vale la pena analizar la transformación logarítmica de la serie y ver si ofrece mejores resultados que la serie original. La transformación logarítmica es especialmente eficaz para series no lineales, así que para Pasajeros posiblemente no suponga ningún mejora.\nLas transformaciones indicadas en los dos párrafos precedentes son solo dos de las posibles. También se pude analizar la serie de pasajeros por día del mes o la transformación óptima de Box-Cox. La idea es no quedarse con lo inmediato –la serie tal cual nos la han ofrecido–, sino probar otras alternativas. Por ejemplo, la serie de Pasajeros es el agregado del número de pasajeros que viajan en transporte urbano según el tipo de transporte (autobús, metro, tranvía…). Se podría proceder a analizar cada serie por separado (pasajeros en autobús, pasajeros en metro, etc.), para luego agregar los resultados y ver si este enfoque da mejores resultados que el análisis directo de la serie agregada Pasajeros.\nEn este epígrafe se analizarán tres de las transformaciones indicadas: la transformación logarítmica, los pasajeros por día laborable y los pasajeros por día del mes. El objetivo es ver si es posible mejorar la calidad de las predicciones obtenidas para Pasajeros. Se usará como criterio de bondad el error de las predicciones extramuestrales según el horizonte temporal, obtenido con el procedimiento origen de predicción móvil. Asumiremos que son necesarios 12 años para obtener una buena estimación del modelo y el horizonte temporal se fijará en 12 meses (\\(k = 144, h = 12\\)). Previamente, hay que crear la serie Pasajeros por día del mes, e identificar el mejor modelo para las series transformadas.\n\nPasajerosDM <- Pasajeros/monthdays(Pasajeros)\nets(Pasajeros, lambda = 0)$method\n\n[1] \"ETS(A,Ad,A)\"\n\nets(PasajerosDL)$method\n\n[1] \"ETS(M,Ad,A)\"\n\nets(PasajerosDM)$method\n\n[1] \"ETS(A,Ad,A)\"\n\n\n\n\n\nk <- 144                 \nh <- 12                  \nTT <- length(Pasajeros)  \ns <- TT - k - h          \n\nmapeAlisadoPas <- matrix(NA, s + 1, h)\nmapeAlisadolPas <- matrix(NA, s + 1, h)\nmapeAlisadoPasDL <- matrix(NA, s + 1, h)\nmapeAlisadoPasDM <- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set <- subset(Pasajeros, start = i + 1, end = i + k)\n  test.set <-  subset(Pasajeros, start = i + k + 1, end = i + k + h)\n  \n  trainDL.set <- subset(PasajerosDL, start = i + 1, end = i + k)\n  testDL.set <-  subset(PasajerosDL, start = i + k + 1, end = i + k + h)\n  \n  trainDM.set <- subset(PasajerosDM, start = i + 1, end = i + k)\n  testDM.set <-  subset(PasajerosDM, start = i + k + 1, end = i + k + h)\n  \n  fit <- ets(train.set, model = \"MAA\", damped = TRUE)\n  fcast <- forecast(fit, h = h)\n  mapeAlisadoPas[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n  fit <- ets(train.set, model = \"AAA\", damped = TRUE, lambda = 0)\n  fcast <- forecast(fit, h = h)\n  mapeAlisadolPas[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  \n  fit <- ets(trainDL.set, model = \"MAA\", damped = TRUE)\n  fcast <- forecast(fit, h = h)\n  mapeAlisadoPasDL[i + 1,] <- 100*abs(testDL.set - fcast$mean)/testDL.set\n  \n  fit <- ets(trainDM.set, model = \"AAA\", damped = TRUE)\n  fcast <- forecast(fit, h = h)\n  mapeAlisadoPasDM[i + 1,] <- 100*abs(testDM.set - fcast$mean)/testDM.set\n}\n\nerrorAlisadoPas <- colMeans(mapeAlisadoPas)\nerrorAlisadolPas <- colMeans(mapeAlisadolPas)\nerrorAlisadoPasDL <- colMeans(mapeAlisadoPasDL)\nerrorAlisadoPasDM <- colMeans(mapeAlisadoPasDM)\n\ndatos <- data.frame(\n  factor = c(rep(\"Pasajeros\", 12), \n             rep(\"Pasajeros por día laborable\", 12), \n             rep(\"Pasajeros por día del mes\", 12), \n             rep(\"Pasajeros (log)\", 12)),\n  x = c(1:12, 1:12, 1:12, 1:12),\n  y = c(errorAlisadoPas, errorAlisadoPasDL, errorAlisadoPasDM, errorAlisadolPas)\n)\n\n\nggplot(datos, aes(x = x, y = y,  colour= factor)) + \n  geom_line() +\n  ggtitle(\"\") +\n  xlab(\"Horizonte temporal de predicción\") +\n  ylab(\"%\") +\n  scale_x_continuous(breaks= 1:12) +\n  scale_y_continuous(breaks= seq(2.6, 4, .2)) +\n  labs(colour = \"Métodos\") + \n  theme(legend.position=c(0.15,0.7))\n\n\n\n\nFigura 11: Error de predicción (MAPE) según horizonte temporal y enfoque. Método de Alisado Exponencial\n\n\n\n\nAntes de pasar al análisis de los resultados, indicar que en las predicciones sobre el logaritmo no se ha pedido corrección por sesgo, y que al trabajar con errores porcentuales no es necesario pasar la predicción de pasajeros por día (laborable o del mes) a predicción de pasajeros.\nLa Figura 11 muestra los errores de predicción según el horizonte temporal para las tres aproximaciones. En todos los casos el error aumenta con el horizonte temporal de predicción, de forma que las predicciones a doce meses vista tienen un error un punto porcentual superior a las predicciones a un mes vista.\nPor otro lado, para horizontes temporales de hasta 8 meses vista las predicciones realizadas sobre el logaritmo de la serie original Pasajeros son las más ajustadas. Para predicciones de 9 a 12 meses, las mejores predicciones se obtienen con Pasajeros por día del mes, seguidas de las predicciones realizadas sobre la serie sin transformar. En contra de lo esperado, la predicción a partir de la serie de pasajeros por día laborable es la que mayor error porcentual presenta."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#conclusión-1",
    "href": "03-11-Ejemplo-Pasajeros.html#conclusión-1",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.3 Conclusión",
    "text": "4.3 Conclusión\nLos modelos de Alisado Exponencial resultan excelentes para predecir la serie Pasajeros. El error de ajuste, del 2.4%, es un punto inferior al error obtenido con el método ingenuo con estacionalidad. Además, en las predicciones extramuestrales a 12 meses vista el error porcentual sigue manteniéndose bajo, no superando el 4%.\nLa transformación logarítmica de la serie pasajeros y la serie Pasajeros por día del mes han dado mejores predicciones por Alisado Exponencial que el análisis directo de la serie."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#transformación-de-la-serie-pasajeros",
    "href": "03-11-Ejemplo-Pasajeros.html#transformación-de-la-serie-pasajeros",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.1 Transformación de la serie Pasajeros",
    "text": "5.1 Transformación de la serie Pasajeros\nPor un lado, el análisis por Alisado Exponencial ha puesto de relieve el carácter lineal de Pasajeros y la efectividad que podría tener usar la transformación logarítmica para mejorar la calidad de las predicciones.\nPor otro lado, tras un análisis preliminar por modelos ARIMA se puede ver que la transformación logarítmica mejora la identificación del modelo y facilitaba su interpretación.\nAsí, optamos por aplicar la transformación logarítmica a Pasajeros.\nLas FAC del logaritmo de la serie y algunas de sus diferenciaciones (Figura 12) indican que es necesaria la doble diferenciación regular y estacional para alcanzar las hipótesis de estacionariedad y ergodicidad: \\(\\log(Pasajeros) \\sim I(1)I_{12}(1)\\). Los resultados ofrecidos por las funciones ndiffs y nsdiffs apoyan esta conclusión.\n\nggAcf(log(Pasajeros), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(Pasajeros)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(Pasajeros), lag = 12), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(diff(log(Pasajeros), lag=12)), lag = 48, xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n(a) Log serie\n\n\n\n\n\n\n\n(b) Dif. regular log serie\n\n\n\n\n\n\n\n\n\n(c) Dif. estacional log serie\n\n\n\n\n\n\n\n(d) Dif. regular y estacional log serie\n\n\n\n\nFigura 12: FAC para Pasajeros\n\n\n\n\nndiffs(log(Pasajeros))\n\n[1] 1\n\nnsdiffs(log(Pasajeros))\n\n[1] 1\n\n\nLa Figura 13 muestra la serie original \\(y_t\\) y la serie transformada \\(\\nabla \\nabla_{12} \\log(y_t)\\). En la serie transformada destacan las compensaciones asociadas a la intervención de Semana Santa.\n\nseries <- cbind(\"Original\" = Pasajeros,\n                \"Dif reg. y est. de log\" = diff(diff(log(Pasajeros), lag = 12)))\n\nautoplot(series, facets = TRUE,\n         xlab = \"\",\n         ylab = \"\",\n         main = \"\")\n\n\n\n\nFigura 13: Serie original de Pasajeros y su transformación"
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#identificación-de-la-serie-pasajeros",
    "href": "03-11-Ejemplo-Pasajeros.html#identificación-de-la-serie-pasajeros",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.2 Identificación de la serie Pasajeros",
    "text": "5.2 Identificación de la serie Pasajeros\nVamos a identificar los valores de \\(p\\), \\(q\\), \\(P\\) y \\(Q\\) del proceso ARIMA. Para ello, solicitaremos con auto.arima y seas una identificación automática, en el primer caso incluyendo todos los efectos calendario ya identificados.\n\n\n\nIdentificación automática con auto.arima\nPara ayudar a la función auto.arima en el proceso de identificación vamos a definir previamente todas las variables de intervención que en el desarrollo del análisis de la serie hemos ido identificando: días del mes, días laborables del mes, meses de diciembre con el día de Navidad en lunes y Semana Santa:\n\nLa variables días laborables del mes ya ha sido definida previamente como DiasLaborables.\nLa variable Días del mes se puede definir directamente con la función monthdays de la librería forecast. En lugar de días del mes, consideraremos la variable días no laborables del mes, resultante de restar a los días del mes los días laborables.\nLos meses de diciembre en que el día de Navidad cae en lunes requiere un poco más de trabajo. La idea general es generar un rango de fechas diarias que cubra todo el periodo de análisis (variable fechas), identificar los lunes de Navidad (variable dicotómica lunesNavidad), eliminar el identificador del día del rango de fechas con la función format y, por último, con tapply sumar para cada mes-año los lunes de Navidad, que lógicamente solo tendrán lugar algunos meses de diciembre y una sola vez. En los objetos definidos con la función as.POSIXlt los meses van de 0 a 11 (enero a diciembre) y los días de la semana de 0 a 6 (domingo a sábado).\nLa creación de las variables de intervención que estiman el efecto de la Semana Santa es aún más complejo. El efecto del viernes de Semana Santa ya queda recogido en la variable DiasNoLaborables. Lo que vamos a hacer ahora es crear una variable que permita estimar el efecto de las vacaciones escolares (y de muchos padres y madres) de lunes a Jueves Santo en aquellas comunidades donde así es; y otra variable para estimar el efecto de las vacaciones escolares que tienen lugar la semana posterior al Domingo de Resurrección, de lunes a viernes tras Semana Santa. Estas nuevas variables (DiasPreSanta y DiasPascua) valdrán cero para los meses distintos de marzo y abril, pero para marzo y abril valdrán la proporción de días vacacionales que recaen en el correspondiente mes.\n\nDías no laborables\nGeneramos la variables para DiasNoLaborables y se muestra su valor para los últimos 5 años.\n\nDiasNoLaborables <- monthdays(DiasLaborables) - DiasLaborables\npDiasNoLaborables <- monthdays(pDiasLaborables) - pDiasLaborables\ntail(DiasNoLaborables, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015  11   8   9   9  11   8   8  10   8  10   9  10\n2016  12   8   9   9   9   8  10   9   8  11   9  11\n2017  10   8   8  11   9   8  10   9   9  10   9  13\n2018   9   8  10   9   9   9   9   9  10   9   9  12\n2019   9   8  10   9   9  10   8  10   9   8  10  11\n\n\nNavidad cae en lunes\nGeneramos la variable que identifica los mes de diciembre en los que la Navidad cayó en lunes.\n\nfechas <- as.POSIXlt(seq(from = as.Date(\"1996-1-1\"), \n                         to = as.Date(\"2024-12-31\"), \n                         by = 1))\nLunesNavidad <- 1*(fechas$wday == 1 & fechas$mon == 11 & fechas$mday == 25)\nfechas <- format(fechas, format = \"%Y-%m\")\nLunesNavidad <- tapply(LunesNavidad, fechas, sum)\nLunesNavidad <- ts(LunesNavidad, start = 1996, frequency = 12)\npLunesNavidad <- subset(LunesNavidad, start = length(LunesNavidad) - 59)\nLunesNavidad <- subset(LunesNavidad, end = length(LunesNavidad) - 60)\n\nLunesNavidad[LunesNavidad == 1]\n\n2000-12 2006-12 2017-12 \n      1       1       1 \n\n\nSemana Santa\nSe generan las variables DiasPreSanta y DiasPascua y se muestra su valor para los últimos 5 años.\n\nLunSanto <- Easter(1996:2024, shift = -6)\nMarSanto <- Easter(1996:2024, shift = -5)\nMieSanto <- Easter(1996:2024, shift = -4)\nJueSanto <- Easter(1996:2024, shift = -3)\n\nPreSanta <- c(LunSanto, MarSanto, MieSanto, JueSanto)\nbiz <- fechaDiaria[isBizday(fechaDiaria, holidays = PreSanta, wday = 0:6)]\nbizdays <- format(biz, format = \"%Y-%m\")\n\nDiasPreSanta <- table(bizdays)\nDiasPreSanta <- ts(DiasPreSanta, start = 1996, frequency = 12)\nDiasPreSanta <- (monthdays(DiasPreSanta) - DiasPreSanta)/4\n\npDiasPreSanta <- subset(DiasPreSanta, start = length(DiasPreSanta) - 59)\nDiasPreSanta <- subset(DiasPreSanta, end = length(DiasPreSanta) - 60)\n\nLunPascua <- Easter(1996:2024, shift = 1)\nMarPascua <- Easter(1996:2024, shift = 2)\nMiePascua <- Easter(1996:2024, shift = 3)\nJuePascua <- Easter(1996:2024, shift = 4)\nViePascua <- Easter(1996:2024, shift = 5)\n\nPascua <- c(LunPascua, MarPascua, MiePascua, JuePascua, ViePascua)\nbiz <- fechaDiaria[isBizday(fechaDiaria, holidays = Pascua, wday = 0:6)]\nbizdays <- format(biz, format = \"%Y-%m\")\n\nDiasPascua <- table(bizdays)\nDiasPascua <- ts(DiasPascua, start = 1996, frequency = 12)\nDiasPascua <- (monthdays(DiasPascua) - DiasPascua)/5\n\npDiasPascua <- subset(DiasPascua, start = length(DiasPascua) - 59)\nDiasPascua <- subset(DiasPascua, end = length(DiasPascua) - 60)\n\ntail(DiasPreSanta, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2016 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2017 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2018 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2019 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n\ntail(DiasPascua, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2016 0.0 0.0 0.8 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2017 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2018 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2019 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n\n\nAhora tenemos todos los elementos para pedir la identificación automática con auto.arima.\n\nauto.arima(Pasajeros, \n           lambda = 0,\n           d = 1, \n           D = 1,\n           xreg = cbind(DiasLaborables, DiasNoLaborables, \n                        LunesNavidad, DiasPreSanta, DiasPascua))\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5775  -0.4194          0.0352            0.0159         0.028\ns.e.   0.0496   0.0531          0.0057            0.0057         0.008\n      DiasPreSanta  DiasPascua\n           -0.0577     -0.0248\ns.e.        0.0042      0.0070\n\nsigma^2 = 0.0003214:  log likelihood = 718.25\nAIC=-1420.51   AICc=-1419.97   BIC=-1391.58\n\n\nLa identificación automática muestra el proceso de las aerolíneas. Además, parece que todas las variables de intervención son significativas.\n\n\n\n\nIdentificación automática con seas\nLa función seas de seasonal incluye automáticamente durante la identificación las variables de intervención necesarias.\n\nsummary(seas(log(Pasajeros)))\n\n\nCall:\nseas(x = log(Pasajeros))\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \nLeap Year          0.026067   0.005643   4.620 3.84e-06 ***\nWeekday            0.005285   0.000265  19.946  < 2e-16 ***\nEaster[1]         -0.086964   0.003186 -27.296  < 2e-16 ***\nAO2005.Jul        -0.054099   0.013221  -4.092 4.28e-05 ***\nAR-Nonseasonal-01 -0.980370   0.271938  -3.605 0.000312 ***\nAR-Nonseasonal-02 -0.597933   0.121438  -4.924 8.49e-07 ***\nAR-Nonseasonal-03 -0.305587   0.086942  -3.515 0.000440 ***\nMA-Nonseasonal-01 -0.561895   0.281101  -1.999 0.045619 *  \nMA-Seasonal-12     0.382738   0.055910   6.846 7.61e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (3 1 1)(0 1 1)  Obs.: 288  Transform: none\nAICc: -1375, BIC: -1339  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 54.28 *** Shapiro (normality): 0.9895 *\n\n\nEn este caso el proceso identificado en la parte regular es más complejo que el obtenido con auto.arima, ARIMA(3,1,1). Además, se han incluido variables de intervención asociadas a los años bisiestos, la Semana Santa y días laborables. Conjuntamente estas variables de intervención recogen los mismos efectos considerados por nosotros.\nConcluimos que el modelo de partida para Pasajeros será \\(\\log(Pasajeros) \\sim ARIMA_{12}(0, 1, 1)(0, 1, 1) + AI\\)."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#estimación-del-modelo-e-identificación-de-otras-componentes-de-intervención",
    "href": "03-11-Ejemplo-Pasajeros.html#estimación-del-modelo-e-identificación-de-otras-componentes-de-intervención",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.3 Estimación del modelo e identificación de otras componentes de intervención",
    "text": "5.3 Estimación del modelo e identificación de otras componentes de intervención\nVamos a estimar el modelo identificado y a analizar la presencia de otros valores atípicos en el residuo.\n\nPasajerosAri <- Arima(Pasajeros, \n                      lambda = 0,\n                      order = c(0, 1, 1),  \n                      seasonal = c(0, 1, 1),\n                      xreg = cbind(DiasLaborables, DiasNoLaborables, \n                                   LunesNavidad, DiasPreSanta, DiasPascua))\nPasajerosAri\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5775  -0.4194          0.0352            0.0159         0.028\ns.e.   0.0496   0.0531          0.0057            0.0057         0.008\n      DiasPreSanta  DiasPascua\n           -0.0577     -0.0248\ns.e.        0.0042      0.0070\n\nsigma^2 = 0.0003214:  log likelihood = 718.25\nAIC=-1420.51   AICc=-1419.97   BIC=-1391.58\n\n\n\nerror <- residuals(PasajerosAri)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 0, 2, 3)*sderror, \n             colour = c(\"red\", \"blue\", \"black\", \"blue\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 14: Error + Intervención. Modelo ARIMA\n\n\n\n\nEn la Figura 14 identificamos tres meses en los que el error supera o casi alcanza las tres desviaciones típicas y son candidatos a valores atípicos: abril de 2002, agosto de 2005 y marzo de 2010. No es fácil conocer las causas para estos valores atípicos.\nTras incluir las correspondientes variables artificiales en el modelo y estimarlo, identificamos otro valor extremo en agosto de 2006 y procedemos a incluirlo en el modelo y repetir el análisis. En esta ocasión ya no identificamos más valores atípicos (véase Figura 15).\n\nd0402 <- 1*(trunc(time(Pasajeros)) == 2002 & cycle(Pasajeros) == 4)\nd0805 <- 1*(trunc(time(Pasajeros)) == 2005 & cycle(Pasajeros) == 8)\nd0806 <- 1*(trunc(time(Pasajeros)) == 2006 & cycle(Pasajeros) == 8)\nd0310 <- 1*(trunc(time(Pasajeros)) == 2010 & cycle(Pasajeros) == 3)\n\nPasajerosAri <- Arima(Pasajeros,\n                      lambda = 0,\n                      order = c(0, 1, 1),  \n                      seasonal =  c(0, 1, 1),\n                      xreg = cbind(DiasLaborables, DiasNoLaborables, \n                                   LunesNavidad, DiasPreSanta, DiasPascua,\n                                   d0402, d0805, d0806, d0310))\nPasajerosAri\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5276  -0.3752          0.0343            0.0158        0.0259\ns.e.   0.0539   0.0591          0.0051            0.0052        0.0072\n      DiasPreSanta  DiasPascua   d0402   d0805   d0806   d0310\n           -0.0579     -0.0253  0.0333  0.0644  0.0284  0.0377\ns.e.        0.0042      0.0067  0.0130  0.0126  0.0126  0.0126\n\nsigma^2 = 0.0002842:  log likelihood = 737.61\nAIC=-1451.23   AICc=-1450.03   BIC=-1407.82\n\n\n\nerror <- residuals(PasajerosAri)\nsderror <- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 0, 2, 3)*sderror, \n             colour = c(\"red\", \"blue\", \"black\", \"blue\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\nFigura 15: Error + Intervención. Modelo ARIMA\n\n\n\n\nAntes de finalizar el proceso de identificación vamos a confirmar la significatividad de todos los parámetros estimados haciendo uso del estadístico de Wald (véase Tabla 2).\n\ndatos <- NULL\nfor(i in 1:length(coef(PasajerosAri))) {\n  datos <- rbind(datos,\n                 data.frame(\n                   \"Coeficiente\" = names(coef(PasajerosAri))[i],\n                   \"Valor de p\" = wald.test(b = coef(PasajerosAri), \n                                            Sigma = vcov(PasajerosAri), \n                                            Terms = i)$result$chi2[3])\n                 )\n}\n\n\nkable(datos, digits = 4, row.names = FALSE)\n\n\n\nTabla 2: Contrastes de significatividad\n\n\nCoeficiente\nValor.de.p\n\n\n\n\nma1\n0.0000\n\n\nsma1\n0.0000\n\n\nDiasLaborables\n0.0000\n\n\nDiasNoLaborables\n0.0023\n\n\nLunesNavidad\n0.0003\n\n\nDiasPreSanta\n0.0000\n\n\nDiasPascua\n0.0001\n\n\nd0402\n0.0106\n\n\nd0805\n0.0000\n\n\nd0806\n0.0246\n\n\nd0310\n0.0028"
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#validación-del-modelo",
    "href": "03-11-Ejemplo-Pasajeros.html#validación-del-modelo",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.4 Validación del modelo",
    "text": "5.4 Validación del modelo\nEn el proceso de validación verificaremos si se cumplen las hipótesis básicas sobre el vector de residuos y analizaremos la calidad de ajuste y predicción del modelo estimado.\n\n\n\nIncorrelación, Homocedasticidad y Normalidad\nVeamos si el residuo es ruido blanco.\n\nBox.test(error, lag = 2,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 0.020802, df = 2, p-value = 0.9897\n\nBox.test(error, lag = 24,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 25.067, df = 24, p-value = 0.4021\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 3.5249, df = 2, p-value = 0.1716\n\nBox.test(error^2, lag = 24, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 25.854, df = 24, p-value = 0.3606\n\njarque.bera.test(error) \n\n\n    Jarque Bera Test\n\ndata:  error\nX-squared = 0.40002, df = 2, p-value = 0.8187\n\n\nEl error muestra ser incorrelado, homocedástico y seguir una distribución normal.\n\n\n\n\nCalidad del ajuste\nAnalizando los criterios de bondad de ajuste (sobre el error de predicción intramuestral a un periodo vista) se tiene un error medio (ME) de -0.05 millones de pasajeros, prácticamente cero por lo que no parece que haya sesgo en las predicciones; en media nos equivocamos 3.7 millones de pasajeros (RMSE) y el error porcentual medio (MAPE) es 1.3%, muy bajo. Para ambos indicadores de bondad de ajuste el error obtenido es la mitad que el visto con Alisado Exponencial.\n\n\n                ME RMSE  MAE  MPE MAPE MASE ACF1\nTraining set -0.05 3.72 2.92 0.01 1.26 0.35 0.02\n\n\n\n\n\n\nCalidad de las predicciones\nSe completará el proceso de validación estimado el error de predicción extramuestral según el horizonte temporal. Se considerarán 12 años para el periodo de estimación y un año para el de predicción.\n\nk <- 144                  \nh <- 12                   \nT <- length(Pasajeros)    \ns<-T - k - h            \n\nmapeArima <- matrix(NA, s + 1, h)\n\nX <- cbind(DiasLaborables, DiasNoLaborables, \n           LunesNavidad, DiasPreSanta, DiasPascua,\n           d0402, d0805, d0806, d0310)\n\nfor (i in 0:s) {\n  train.set <- subset(Pasajeros, start = i + 1, end = i + k)\n  test.set <-  subset(Pasajeros, start = i + k + 1, end = i + k + h) \n  \n  X.train <- X[(i + 1):(i + k),]\n  hay <- colSums(X.train)\n  X.train <- X.train[, hay>0]\n  \n  X.test <- X[(i + k + 1):(i + k + h),]\n  X.test <- X.test[, hay>0]\n  \n  fit <- try(Arima(train.set, \n                   lambda = 0,\n                   order = c(0, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   xreg=X.train), silent = TRUE)\n  \n  if (!is.element(\"try-error\", class(fit))) {\n    fcast <- forecast(fit, h = h, xreg = X.test)\n    mapeArima[i + 1,] <- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n\nerrorArima <- colMeans(mapeArima, na.rm = TRUE)\nround(errorArima, 2)\n\n [1] 1.39 1.53 1.61 1.72 1.87 1.96 2.08 2.22 2.35 2.48 2.62 2.79\n\n\nEl error es creciente en el horizonte temporal de predicción. Para predicciones extramuestrales a un periodo vista vale 1.4%, algo superior al error de estimación, pero realmente bajo. Incluso para predicciones a doce meses vista, el error sigue siendo reducido, 2.8%. Recordemos que para Alisado Exponencial era de 3.6%."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#interpretación-del-modelo-estimado",
    "href": "03-11-Ejemplo-Pasajeros.html#interpretación-del-modelo-estimado",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.5 Interpretación del modelo estimado",
    "text": "5.5 Interpretación del modelo estimado\nEl modelo estimado y validado corresponde al modelo de las aerolíneas con intervención: \\(ARIMA_{12}(0,1,1)(0,1,1) + AI\\). La ecuación teórica completa del modelo es:\n\\[(1-L)(1-L^{12})\\log(Pasajeros) = (1+\\theta_1 L)(1 + \\theta_{12} L^{12})\\varepsilon_t+\\]\n\\[\\gamma_1 DiasLaborables +\\gamma_2 DiasNoLaborables +\\gamma_3 LunesNavidad+\\]\n\\[\\gamma_4 DiasPreSanta + \\gamma_5 DiasPascua +\\]\n\\[\\gamma_6 d0402 +\\gamma_7 d0805 +\\gamma_8 d0806 +\\gamma_9 d0310.\\]\nSi se desarrolla el modelo y se deja en función de la tasa de variación anual del número de pasajero, queda (la parte de intervención no cambia):\n\\[TVAPasajeros_t = TVAPasajeros_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12}+ \\theta_1 \\theta_{12} \\varepsilon_{t-13}+\\varepsilon_t + AI.\\]\nFinalmente, el modelo estimado es:\n\\[\\widehat{TVAPasajeros}_t = TVAPasajeros_{t-1} -0.53 \\varepsilon_{t-1} -0.37 \\varepsilon_{t-12}+ 0.20 \\varepsilon_{t-13} +\\]\n\\[0.034\\cdot DiasLaborables +0.016\\cdot DiasNoLaborables+ 0.026\\cdot LunesNavidad\\]\n\\[- 0.058\\cdot DiasPreSanta - 0.025\\cdot DiasPascua +\\]\n\\[0.033\\cdot d0402 +0.064\\cdot d0805 +0.028\\cdot d0806 +0.038\\cdot d0310.\\] Interpretación:\n\nLa tasa de variación anual del número de pasajeros en transporte urbano para un mes dado es la misma que la observada en el mes previo.\nSi hace uno, doce o trece meses se observó un número atípico de pasajeros, se debe tener en cuenta para corregir la predicción.\nCada día laborable adicional en un mes supone un incremento en el número de pasajeros del 3.4% y cada día no laborable un incremento adicional del 1.6%.\nSi la Navidad cae en lunes y por tanto Nochebuena en domingo, ese mes de diciembre el número de pasajeros será un 2.6% superior al de un mes de diciembre donde la Navidad no cae en lunes.\nSi los días laborables (lunes a jueves) de la Semana Santa caen íntegramente en marzo, ese mes el número de pasajeros cae un 5.8% respecto de un marzo sin Semana Santa. Lo mismo pasaría con abril.\nDe la misma forma, si los días laborables (lunes a viernes) de la semana posterior a Domingo de Resurrección (semana de Pascua) caen en marzo, ese mes el número de pasajeros cae un 2.5% respecto de un marzo sin Pascua. Lo mismo para abril.\nAdemas, para cuatro meses se observó una tasa de variación anual atípicamente superior a la esperada."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#predicción-del-número-de-pasajeros-en-transporte-urbano",
    "href": "03-11-Ejemplo-Pasajeros.html#predicción-del-número-de-pasajeros-en-transporte-urbano",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.6 Predicción del número de pasajeros en transporte urbano",
    "text": "5.6 Predicción del número de pasajeros en transporte urbano\nUna vez dado por válido el modelo, podemos pasar a realizar predicciones para los próximos años. Para las variables de intervención sujetas a fecha de calendario ya hemos ido creando sus valores previstos, para las demás los fijaremos a cero.\n\npPasajerosAri <- forecast(PasajerosAri, \n                          h = 60,\n                          xreg = cbind(pDiasLaborables, pDiasNoLaborables, \n                                       pLunesNavidad, pDiasPreSanta, pDiasPascua,\n                                       rep(0, 60), rep(0, 60), \n                                       rep(0 ,60), rep(0, 60)), \n                          level = 95)\nautoplot(pPasajerosAri, \n         xlab = \"\",\n         ylab = \"\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1996, 2024, 4))\n\n\n\n\nFigura 16: Pasajeros (1996-2019) y predicción (2020-2024). Modelo ARIMA\n\n\n\n\nAsí, en 2020 se esperan 3180 millones de pasajeros y para 2021 un total de 3247 millones de pasajeros."
  },
  {
    "objectID": "04-06-Covid_Nacimientos.html",
    "href": "04-06-Covid_Nacimientos.html",
    "title": "Efecto de la Covid-19 sobre la serie Nacimientos",
    "section": "",
    "text": "Introducción\nExiste un amplia y creciente literatura sobre el efecto de la Covid-19 en España en todos los ámbitos posibles: demográfico, laboral, económico, educación, salud… Dentro de la dimensión demográfica, vamos a centrarnos en el impacto de la Covid en la natalidad, concretamente, en el numero de nacimientos mensuales.\nDurante 2020 en España se tomaron diferentes medidas de confinamiento que tuvieron un impacto directo sobre las familias, afectaron la tasa de fecundidad y, por tanto, el número de nacimientos. Repasemos brevemente estas medidas:\n\nEn 14 de marzo de 2020 se decreta el estado de alarma y todas las personas deben permanecer confinadas en sus hogares.\nEl 2 de mayo empieza el proceso de desconfinamiento, permitiéndose a la gente salir a pasear cerca del domicilio y hacer deporte. Este proceso se inició en momentos diferentes en cada comunidad, dependiendo de una serie de indicadores de gravedad.\nEn las semanas sucesivas cada territorio fue relajando las medidas de movilidad entre provincias y de aforo en los locales públicos.\nEl 21 de junio finaliza el estado de alarma y se acaban todas las restricciones de movilidad.\n\nResumiendo, podemos considerar que hubo un periodo duro de confinamiento en marzo y abril, seguido de otro periodo de confinamiento más leve de mayo a junio. Como a efectos del número de nacimientos es necesario esperar nueve meses desde el inicio del confinamiento para ver su impacto, su efecto se empezaría a notar a partir de noviembre de 2020 hasta febrero de 2021.\nPor supuesto, hay mucha literatura mucha literatura que analiza el efecto de la Covid sobre la natalidad. Por ejemplo González (2021) estima que en diciembre de 2020 y enero de 2021 hubo un 21% menos de nacimientos de lo esperado; que entre noviembre de 2020 y febrero de 2021 se redujo el número de nacimientos en 13000 sobre lo esperado; y que en marzo de 2021 se volvió a los niveles prepandemia. Por otro lado, Blanes, Domingo, and Esteve (2021) estiman que la caída en diciembre de 2020 y enero de 2021 fue del 20%; que entre noviembre de 2020 y febrero de 2021 la reducción en el número de nacimientos fue de 8000 a 10000 bebés; y que en marzo y abril de 2021 hubo un ligero incremento en el número de nacimientos respecto del año previo. Las discrepancias entre los trabajos pueden deberse a la información de que disponían en el momento de su elaboración, los supuestos asumidos, la metodología empleada, etc.\nPero ¿qué podemos decir nosotros? Con los conocimientos que disponemos de análisis y predicción de series temporales, ¿en cuánto cuantificamos la caída en los nacimientos a finales del año 2020 y principios del 2021? ¿Ha habido un efecto rebote, con un mayor número de concepciones tras el confinamiento? ¿Cuándo volvió el número de nacimientos a los niveles prepandemia?\n\n\nQue dicen los datos\nComo primera aproximación, vamos a ver el número de nacimientos en los años prepandemia, durante el año 2020 y en los años postpandemia. El INE nos da información consolidada sobre el número de nacimientos hasta diciembre de 2020. Para el año 2021 los datos aun son provisionales, y hasta agosto de 2022 muestra estimaciones realizadas con su propia metodología.1\nLa Figura 1 muestra el número mensual de nacimientos desde 2018 hasta agosto de 2022. Lo primero que se observa es la tendencia decreciente en el número de nacimientos, que viene observándose desde la Gran Recesión. Con independencia del mes, los nacimientos en 2020 son inferiores a los de 2019, y estos inferiores a los de 2018.\nEn segundo lugar, claramente desde noviembre de 2020 hasta febrero de 2021, el número de nacimientos es muy inferior al esperado. En tercer lugar, desde marzo 2021 el número de nacimientos alcanza una cifra similar a la observada en el mismo mes de 2020. Es decir, si que puede existir un efecto rebote.\n\nnacimientos <- read.csv2(\"./series/nacimientos extendida.csv\",\n                         header = TRUE)\n\nnacimientos <- ts(nacimientos[, 2],\n                  start = 1975,\n                  frequency = 12)\n\nnacimientos <- window(nacimientos, \n                      start = 2000)\n\n\nggseasonplot(window(nacimientos, start = 2018)) +\n  ylab(\"Nacimientos\") +\n  xlab(\"\") +\n  ggtitle(\"\") + \n  geom_point()\n\n\n\n\nFigura 1: Nacimientos mensuales (enero 2018 - agosto 2022)\n\n\n\n\nEn cuarto y último lugar, las estimaciones del INE para 2022 parecen apuntar a que el número de nacimientos ya se ha normalizado y está en línea con lo que cabría esperar si no hubiera habido Covid-19.\nPero todas estas afirmaciones son meramente descriptivas y es necesario un análisis más riguroso para poder confirmarlas o descartarlas.\n\n\n¿Qué hubiera pasado si…?\nEn los dos epígrafes previos hemos usado constantemente la coletilla respecto de lo esperado: los nacimientos en diciembre de 2020 son inferiores a lo esperado y en marzo de 2021 superiores a lo esperado. Pero ¿qué era lo esperado?\nLa forma óptima de ver el efecto del confinamiento sobre los nacimientos sería comparar lo que realmente ha ocurrido con lo que hubiera pasado en un hipotético mundo paralelo sin Covid. Pero claro, esto es imposible, solo podemos observar una realidad y en ella la Covid ha ocurrido.\nUn forma de construir el mundo hipotético sin Covid es considerar la serie de nacimientos hasta mediados de 2020, ajustarla a un modelo y predecir los siguientes meses (desde finales de 2020 hasta julio de 2022). Podríamos interpretar estas predicciones como los nacimientos que hubieran tenido lugar en un mundo sin Covid, donde todo hubiera discurrido según la inercia de los años previos. Después, podemos comparar los nacimientos reales con las predicciones y responder a nuestra principal pregunta: ¿cómo ha afectado el confinamiento el número de nacidos?\n\nAjuste a un modelo y predicción\nVamos a considerar la serie de nacimientos desde enero de 2000 hasta octubre de 2020, ajustarla a un modelo ARIMA y predecir hasta agosto de 2022.\nEl siguiente código estima el mismo modelo visto en el tema de ARIMA con estacionalidad para la serie nacimientos. La única diferencia es que en esta ocasión la serie alcanza hasta septiembre de 2020. El ajuste, con un error porcentual del 1.5% es muy bueno.\n\nnacimientos2 <- window(nacimientos, \n                       end = c(2020, 10))\n\nDiasMes <- monthdays(nacimientos2)\nSemanaSanta <- easter(nacimientos2)\n\nd1206 <- 1*(cycle(nacimientos2) == 12 & trunc(time(nacimientos2)) == 2006)\nd1210 <- 1*(cycle(nacimientos2) == 12 & trunc(time(nacimientos2)) == 2010)\nd0111 <- 1*(cycle(nacimientos2) == 1  & trunc(time(nacimientos2)) == 2011)\nd0416 <- 1*(cycle(nacimientos2) == 4  & trunc(time(nacimientos2)) == 2016)\nd0616 <- 1*(cycle(nacimientos2) == 6  & trunc(time(nacimientos2)) == 2016)\n\nmodelo <- Arima(nacimientos2, \n                order = c(0, 1, 1),\n                seasonal =  c(0, 1, 1),\n                lambda = 0,\n                xreg = cbind(DiasMes, SemanaSanta, \n                             d1206, d1210, d0111, d0416, d0616))\nsummary(modelo)\n\nSeries: nacimientos2 \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasMes  SemanaSanta    d1206   d1210    d0111    d0416\n      -0.4915  -0.7604   0.0293      -0.0216  -0.0463  0.0616  -0.0559  -0.0511\ns.e.   0.0601   0.0451   0.0072       0.0049   0.0157  0.0163   0.0162   0.0166\n       d0616\n      0.0281\ns.e.  0.0158\n\nsigma^2 = 0.0003878:  log likelihood = 595.36\nAIC=-1170.73   AICc=-1169.75   BIC=-1136.05\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE   MAPE      MASE       ACF1\nTraining set -75.88295 699.8042 565.3364 -0.2245479 1.5423 0.4137952 0.02443195\n\n\nAhora vamos a predecir la serie desde noviembre de 2020 hasta agosto de 2022 (22 meses).\n\ntmp <- ts(rep(0, 22), start = c(2020, 11), freq = 12)\npdm <- monthdays(tmp)\npss <- easter(tmp)\nprediccion <- forecast(modelo, \n                       h = 21,\n                       xreg = cbind(pdm, pss, rep(0,22), rep(0,22), \n                                    rep(0,22), rep(0,22), rep(0,22)))\n\nLa Figura 2 muestra la serie original de nacimientos desde 2018 y la predicción. Por la metodología seguida, desde noviembre de 2020 disponemos para cada mes de dos datos: los nacimientos en el mundo real con Covid (observaciones) y los nacimientos en un mundo sin Covid (previsiones).\n\nautoplot(nacimientos,\n         xlab = \"\",\n         ylab = \"Bebés\",\n         main = \"\", \n         series = \"Con Covid\") + \n  xlim(2018, 2023) +\n  ylim(20000, 35000) + \n  autolayer(prediccion, series = \"Sin Covid\", PI = FALSE) + \n  labs(colour = \"Nacimientos\") + \n  theme(legend.position=c(0.9,0.85)) \n\n\n\n\nFigura 2: Nacimientos mensuales (enero 2018 - agosto 2022) y previsiones (noviembre 2020 - agosto 2022)\n\n\n\n\nUna lectura rápida de la Figura 2 muestra que efectivamente, el número observado de nacimientos entre noviembre de 2020 y febrero de 2021 (en rojo) fue muy inferior a los valores esperados (en azul). Sin embargo, el resto del año 2021 el número de nacimientos fue superior al esperado, apuntando a un ligero efecto rebote. En 2022 el efecto de la pandemia ha desaparecido totalmente. Veamos estas observaciones en detalle.\n\n\nEfecto del confinamiento sobre los nacimientos\nEn primer lugar veamos cuál ha sido la caída en el número de nacimientos entre noviembre de 2020 y febrero de 2021.\n\nCon_covid <- as.numeric(window(nacimientos, \n                               start = c(2020, 11), \n                               end = c(2021, 2)))\nSin_covid <- as.numeric(window(prediccion$mean, \n                               start = c(2020, 11), \n                               end = c(2021, 2)))\n# Caida porcentual\n(Con_covid - Sin_covid)/Con_covid\n\n[1] -0.06893514 -0.20764127 -0.19638402 -0.03668175\n\n#Caida en el total de nacidos\nsum(Con_covid - Sin_covid)\n\n[1] -12311.26\n\n\nLa mayor caída porcentual en el número de nacimientos tuvo lugar en los meses de diciembre de 2020 y enero de 2021 (20.7% y 19.6%, respectivamente) y nuestras estimaciones coinciden con las aportadas en González (2021) y Blanes, Domingo, and Esteve (2021). Respecto del número de nacimientos, nosotros estimamos una reducción de 12300 nacidos, un valor algo inferior a la estimación en González (2021) y muy superior a la estimación en Blanes, Domingo, and Esteve (2021).\n\n\nEfecto rebote\nUna posibilidad es que el confinamiento no hizo que las parejas decidieran no tener hijos de forma permanente, sino que simplemente retrasó la decisión de tenerlos. Si esto es así, cabria esperar a mediados o finales de 2021 un número de nacimientos superior al esperado: por una lado tendríamos los nacimientos de las parejas que tenían pensado tener hijos en ese momento y por otro los de las parejas que habían retrasado el momento de la maternidad. Si es así, la Covid no habría reducido de forma permanente el número de nacimientos y el acumulado en el medio/largo plazo seria el mismo que si no hubiera habido Covid.\nPara poder responder mejor a esta pregunta, vamos a calcular la diferencia acumulada entre el número de nacimientos esperado y el real desde noviembre de 2020 hasta agosto de 2022. La diferencia acumulada crece hasta los 12300 bebés en febrero de 2021. Esta diferencia máxima se va reduciendo lentamente hasta los 4500 bebés en diciembre de 2021. Es decir, efectivamente parece que hay un efecto rebote que ha compensado a lo largo del año 2021 en un total de 7800 bebés la caída hasta febrero de ese año.\nDurante el año 2022 la diferencia acumulada ha seguido reduciéndose hasta desaparecer en agosto de 2022. Sin embargo, hay que tener en cuenta que en 2022 estamos comparando las previsiones de nuestro modelo con las estimaciones del INE, así que las conclusiones que obtengamos son muy poco fiables.\n\nCon_covid <- window(nacimientos, start = c(2020, 11))\nSin_covid <- window(prediccion$mean, start = c(2020, 11))\n\nDiferencia <- cumsum(Sin_covid - Con_covid)\nDiferencia <- ts(Diferencia, \n                 start = c(2020, 11),\n                 frequency = 12)\n\n\nautoplot(Diferencia,\n         xlab = \"\",\n         ylab = \"Bebés\",\n         main = \"\") + \n  xlim(2020.7, 2023) +\n   scale_x_continuous(breaks= seq(2020 + 11/12, 2022 + 7/12, 2/12),\n                     label = c(\"Dic-20\", \"Feb-21\", \"Abr-21\", \"Jun-21\",\n                               \"Ago-21\", \"Oct-21\", \"Dic-21\", \"Feb-22\",\n                               \"Abr-22\", \"Jun-22\", \"Ago-22\"))\n\n\n\n\nFigura 3: Déficit acumulado de nacimientos desde noviembre de 2020\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBlanes, Amand, Andreu Domingo, and Albert Esteve. 2021. “Consecuencias Demográficas de La COVID-19 En España: Entre La Novedad Excepcional y La Reincidencia Estructural.” Panorama Social 33. https://www.funcas.es/wp-content/uploads/2021/07/Esteve.pdf.\n\n\nGonzález, Libertad. 2021. “La Natalidad En España Durante La Pandemia: Actualización.” Nada Es Gratis. https://nadaesgratis.es/libertad-gonzalez/la-natalidad-en-espana-durante-la-pandemia-actualizacion.\n\nFootnotes\n\n\nLos datos para los años 2021 y 2022 fueron descargados en noviembre de 2022.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Iván Arribas",
    "section": "",
    "text": "Iván Arribas es licenciado en Ciencias Matemáticas con la especialidad de Estadística e Investigación Operativa, y doctor en Ciencias Económicas. Actualmente es Profesor Titular en el Departamento de Análisis Económico de la Universitat de València, donde lleva más de 25 años como docente e investigador."
  },
  {
    "objectID": "03-06-Ejemplo1.html#tendencia",
    "href": "03-06-Ejemplo1.html#tendencia",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "3.1 Tendencia",
    "text": "3.1 Tendencia\nHemos obtenido la serie anual de casos de defunciones causadas por enfermedades cerebrovasculares, que se muestra en la Figura 3. Se confirma la tendencia decreciente, puntualmente interrumpida algunos años. A primeros de los 80 el número de defunciones prácticamente alcanzaba las 50,000 al año, mientras que actualmente apenas superan las 25,000. Esto supone una caída media anual de 1.5% en el número de defunciones.\n\nautoplot(aggregate(DefEnfCer, FUN = sum),\n         xlab = \"\",\n         ylab = \"Casos\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1980, 2020, 2)) \n\n\n\n\nFigura 3: Tendencia de las defunciones causadas por enfermedades cerebrovasculares"
  },
  {
    "objectID": "03-06-Ejemplo1.html#estacionalidad",
    "href": "03-06-Ejemplo1.html#estacionalidad",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "3.2 Estacionalidad",
    "text": "3.2 Estacionalidad\nVeamos ahora como varía la incidencia de las muertes causadas por enfermedades cerebrovasculares según el mes del año.\n\nggsubseriesplot(DefEnfCer, \n             polar=TRUE,\n             xlab = \"\",\n             ylab = \"\",\n             main = \"\") +\n  guides(colour=FALSE)\n\n\n\n\nFigura 4: Gráfico estacional: defunciones por enfermedades cerebrovasculares\n\n\n\n\nCada subserie en la Figura 4 vuelve a mostrar la reducción en las defunciones causadas por enfermedades cerebrovasculares durante el periodo de análisis. Respecto de la estacionalidad, se aprecia que el principal determinante es la temperatura puesto que la incidencia de la enfermedad es mayor en los meses de invierno y menor en los de verano. También cabría esperar un efecto días del mes y observar más incidencia en los meses de 31 días que en los de 30, pero el efecto de la temperatura es tan dominante que camufla cualquier otro efecto. Sin embargo, para el mes de febrero se ve muy claramente el efecto días de mes. Por temperatura febrero debería situarse a medio camino entre enero y marzo, pero por ser un mes con solo 28 días (29 los años bisiestos) la media de defunciones es menor y se sitúa a nivel de marzo."
  },
  {
    "objectID": "04-02-Multiples_CS.html#descomposición",
    "href": "04-02-Multiples_CS.html#descomposición",
    "title": "Múltiples componentes estacionales",
    "section": "2.1 Descomposición",
    "text": "2.1 Descomposición\nPodemos descomponer la serie de forma análoga a como se hacia para series con una componente estacional usando la función mstl.\n\ndescomposicion <- mstl(electricidad)\nautoplot(descomposicion) \n\n\n\n\nFigura 2: Descomposición de Consumo eléctrico por hora\n\n\n\n\nEn la Figura 2 aparecen los mismos paneles que has visto en el tema 2 –datos originales, tendencia y residuo–, más los dos paneles correspondientes a las dos componentes estacionales, de orden 24 y 168.\nPara poder interpretar adecuadamente cada serie hay que fijarse en la escala de los ejes verticales. La tendencia apenas cambia en el periodo de análisis. Las dos componentes estacionales oscilan sobre un rango de valores mayor. Dentro de cada día el consumo de electricidad oscila aproximadamente 20 GW entre las horas pico y las valle: en los picos se consumen unos 7.5 GW más que la media diaria, y en los valles unos 12.5 GW menos que la media diaria. Por otro lado, el rango de variación semanal en el consumo también es de aproximadamente 20 GWh: de lunes a viernes se consumen como máximo unos 5 GW más que la media semanal y los domingo unos 15 GWh menos.\nAunque por defecto mstl permite que la componente estacional varíe en el tiempo, vamos a mostrar un detalle de la componente estacional diaria, semana y su composición para la primera semana de la serie (véase Figura 3).\n\n\n\n\n\n\n\n(a) Componente estacional diaria\n\n\n\n\n\n\n\n\n\n(b) Componente estacional semanal\n\n\n\n\n\n\n\n\n\n(c) Componente estacional diaria + semanal\n\n\n\n\nFigura 3: Componentes estacionales para Consumo eléctrico"
  },
  {
    "objectID": "04-02-Multiples_CS.html#predicción-a-partir-de-la-descomposición-y-alisado-exponencial",
    "href": "04-02-Multiples_CS.html#predicción-a-partir-de-la-descomposición-y-alisado-exponencial",
    "title": "Múltiples componentes estacionales",
    "section": "2.2 Predicción a partir de la descomposición y alisado exponencial",
    "text": "2.2 Predicción a partir de la descomposición y alisado exponencial\nExisten varios métodos para poder estimar series con estacionalidad múltiple. Uno de los más sencillos consiste en descomponer de la serie. Después, predecir las componentes estacionales por simple repetición y predecir la componente de la tendencia usando Alisado Exponencial. En último lugar, se combinan la predicción de la tendencia con las predicciones de las estacionalidades para obtener una predicción de la serie.\nLa función stlf hace todas estas operaciones de forma automática. Por defecto la tendencia se predice usando Alisado Exponencial (“ets”), pero con el argumento method se pueden especificar otros modelos alternativos, “arima”, “naive” o “rwdrift”.\nEn la figura se muestra el resultado de aplicar stlf a la serie de Demanda eléctrica. La línea negra representa la serie de Demanda y la línea azul la predicción para las dos semanas siguientes (dos primeras semanas de marzo de 2021). El título por defecto de la figura indica que la tendencia de la serie se ha ajustado usando la función ets y presenta una pendiente aditiva amortiguada y error multiplicativo.\n\npdatos_stfl <- stlf(electricidad)\n\nautoplot(pdatos_stfl, PI = FALSE) + \n  ylab(\"GW\") + \n  xlab(\"Semanas\") + \n  scale_x_continuous(breaks = seq(1,10, by = 1))\n\n\n\n\nFigura 4: Consumo eléctrico y predicción. Descomposición + Alisado"
  },
  {
    "objectID": "04-02-Multiples_CS.html#predicción-a-partir-de-asilado-exponencial-y-series-de-furier",
    "href": "04-02-Multiples_CS.html#predicción-a-partir-de-asilado-exponencial-y-series-de-furier",
    "title": "Múltiples componentes estacionales",
    "section": "2.3 Predicción a partir de Asilado Exponencial y series de Furier",
    "text": "2.3 Predicción a partir de Asilado Exponencial y series de Furier\nUno de los inconvenientes del método visto es que estima todos los elementos de cada componente estacional (\\(24 + 168\\) elementos en nuestro ejemplo) como si fueran independientes, sin tener en cuenta que, por lo general, evolucionan siguiendo una suave curva. Véase en el ejemplo de Demanda eléctrica la curva que sigue la estacionalidad diaria, donde la componente de una hora determinada está muy relacionada con la componente de la hora precedente y posterior.\nAlgunos métodos alternativos de predicción usan la dependencia observada entre los elementos de una componente estacional para ajustarlos a una curva paramétrica, por ejemplo funciones trigonométricas o series de Fourier.\nEntre los que usan funciones trigonométricas está el implementado en Livera, Hyndman, and Snyder (2011). El método de estimación que emplean los autores es complejo, requiere tiempo de computación y no siempre el ajuste obtenido es el más adecuado, así que los resultados pueden ser en ocasiones malos.\nVeamos un ejemplo de la implementación del método de Livera, Hyndman, and Snyder (2011) con la función tbats. La Figura 5 muestra la predicción para dos semanas.\n\ntmp <- Sys.time()\n\ndatos_tbats <- tbats(electricidad)\n\npdatos_tbats <- forecast(datos_tbats, \n                         h = 14 * 24,\n                         level = 95)\n\nautoplot(pdatos_tbats, PI = FALSE) + \n  ylab(\"GW\") + \n  xlab(\"Semanas\") + \n  scale_x_continuous(breaks = seq(1,6, by = 1))\n\nSys.time() - tmp\n\nTime difference of 35.63842 secs\n\n\n\n\n\nFigura 5: Consumo eléctrico y predicción. Alisado y ajuste componentes estacionales por funciones trigonométricas\n\n\n\n\n\n\n\nEntre los métodos que involucran series de Fourier una propuesta reciente es el modelo Prophet, disponible a través del paquete fable.prophet. Este modelo fue introducido por Facebook (Taylor and Letham (2018)), originalmente para pronosticar datos diarios con estacionalidad semanal y anual, además de efectos calendario. Posteriormente se amplió para cubrir más tipos de datos estacionales."
  },
  {
    "objectID": "04-04-Series_acotadas.html#predicciones-positivas",
    "href": "04-04-Series_acotadas.html#predicciones-positivas",
    "title": "Series acotadas",
    "section": "Predicciones positivas",
    "text": "Predicciones positivas\nPara imponer que las predicciones sean positivas basta trabajar con la transformación logarítmica. Por ejemplo, consideremos la serie anual de nacimientos. Vamos ha realizar predicciones a muy largo plazo (30 años) usando Alisado de Holt con y sin transformación logarítmica.\n\n\n\nEn el panel superior de la Figura 1, donde se ha usado la transformación logarítmica, no solo la predicción, sino también el intervalo es siempre positivo. Por el contrario, en el panel inferior de la Figura 1, donde no se ha usado la transformación logarítmica, las predicciones a partir de 2042 ya son negativas y el extremo inferior del intervalo de confianza es negativo desde el año 2028.\n\nConLog <- forecast(ets(nacimientos, model = \"AAN\", damped = FALSE, lambda = 0),\n                   h = 30,\n                   level = 95)\n\nSinLog <- forecast(ets(nacimientos, model = \"AAN\", damped = FALSE),\n                   h = 30,\n                   level = 95)\n\nautoplot(ConLog, main = \"\", xlab = \"\", ylab = \"Bebés (miles)\")\n\nautoplot(SinLog, main = \"\", xlab = \"\", ylab = \"Bebés (miles)\") + \n  geom_hline(yintercept=0, size = .3, linetype = 2)\n\n\n\n\n\n\n\n(a) Con transformación logarítmica\n\n\n\n\n\n\n\n\n\n(b) Sin transformación logarítmica\n\n\n\n\nFigura 1: Ajuste y predicción de Nacimientos con Holt"
  },
  {
    "objectID": "04-04-Series_acotadas.html#predicciones-dentro-de-un-intervalo",
    "href": "04-04-Series_acotadas.html#predicciones-dentro-de-un-intervalo",
    "title": "Series acotadas",
    "section": "Predicciones dentro de un intervalo",
    "text": "Predicciones dentro de un intervalo\nSupongamos que el valor de la serie es un porcentaje y que debe estar comprendido entre \\(a = 0\\) y \\(b = 100\\), como por ejemplo la serie anual consistente en el porcentaje de bebés nacidos de mujeres con nacionalidad española. La transformación que garantiza que las predicciones se mantendrán dentro del intervalo \\([a,\\;b]\\) es\n\\[z_t = \\log\\Big(\\frac{y_t - a}{b - y_t}\\Big),\\]\ndonde \\(y_t\\) es la serie original y \\(z_t\\) la serie transformada. Una vez tenemos las predicciones de la serie \\(z_t\\), tenemos que deshacer la transformación con\n\\[y_t = \\frac{a +b\\, e^{z_t}}{1 + e^{z_t}}.\\]\nEn este caso no hay un argumento lambda que nos facilite el trabajo y hay que escribir más código.\n\n\n\n\na <- 0\nb <- 100\n\nz <- log((serie - a) / (b - serie))\n\nmodelo <- ets(z, \n              model = \"AAN\", \n              damped = FALSE)\n\npz <- forecast(modelo, \n               h = 30,\n               level = 95)\n\npz[[\"mean\"]] <-  (a + b * exp(pz[[\"mean\"]]) ) / (1 + exp(pz[[\"mean\"]]))\npz[[\"lower\"]] <- (a + b * exp(pz[[\"lower\"]])) / (1 + exp(pz[[\"lower\"]]))\npz[[\"upper\"]] <- (a + b * exp(pz[[\"upper\"]])) / (1 + exp(pz[[\"upper\"]]))\npz[[\"x\"]] <- serie\n\nautoplot(pz, \n         main = \"\",\n         xlab = \"\")\n\n\n\n\nFigura 2: Predicción con Holt. Valores acotados entre 0% y 100%\n\n\n\n\nHemos solicitado una previsión a 30 años vista para poder ver mejor el efecto de acotar la serie. En la Figura 2 se observa que en nuestro ejemplo no solo la predicción, sino también el intervalo está siempre entre 0% y 100%."
  },
  {
    "objectID": "05-Recursos-R.html",
    "href": "05-Recursos-R.html",
    "title": "Series Temporales",
    "section": "",
    "text": "Durante el curso usaremos diferentes ficheros para los datos de los ejemplos y para el código en R. Desde esta página puedes descargarte todo el material\n\n\n\n\nFicheros de datos\n\nTítulos publicados: Títulos publicados en España (fuente Instituto Nacional de Estadística). Es una serie anual de 1993 a 2018.\nNacimientos: Nacimientos en España (fuente Instituto Nacional de Estadística). Serie mensual de enero de 1975 a diciembre de 2018\nConsumo eléctrico: Consumo eléctrico en España en MWh (fuente Red Eléctrica de España). Es una serie diaria desde el 1 de enero de 2019 hasta el 31 de diciembre de 2019.\nAforo de vehículos en Oropesa: Aforo de vehículos por Oropesa, carretera N-340, km. 996,48 (fuente Ministerio de Fomento). La serie es anual de 1960 a 2018.\nConsumo de alimentos per cápita: Consumo alimentario en hogar per cápita en España. Esta serie está construida a partir de la serie de consumo alimentario en hogar (fuente Ministerio de Agricultura, Alimentación y Medio Ambiente), y la serie de población (fuente Instituto Nacional de Estadística). Es una serie anual de 1987 a 2018 y la unidad es el Kg per cápita.\nProducción de Chocolate en Australia: Producción de chocolate en Australia desde enero de 1958 hasta diciembre de 1994.\nDefunciones por enfermedades cerebrovasculares: Número de defunciones causadas por enfermedades cerebrovasculares (fuente Instituto Nacional de Estadística). Es una serie mensual de enero de 1980 a diciembre de 2018.\nPasajeros en transporte urbano: Número de pasajeros en transporte urbano en España (fuente Instituto Nacional de Estadística). Serie mensual de 1996 a diciembre de 2019\n\n\n\n\nTodos los ficheros de datos en un único fichero comprimido\n\n\n\n\n\n\nCódigo R\nTienes el código para los temas de teoría aquí:\n\nCódigo para el tema 1\nCódigo para el tema 2\nCódigo para el tema 3\nCódigo para el tema 4\nCódigo para el tema 5\n\n\n\nPara el ejemplo de Defunciones por enfermedades cerebrovasculares tienes el código aquí:\n\nCódigo para el ejemplo del tema 1\nCódigo para el ejemplo del tema 2\nCódigo para el ejemplo del tema 3\nCódigo para el ejemplo del tema 4\nCódigo para el ejemplo del tema 5\n\nY para el ejemplo de Pasajeros en transporte urbano aquí.\n\n\n\nTodos los ficheros de código en un único fichero comprimido"
  },
  {
    "objectID": "03-01-Tema1.html#definición",
    "href": "03-01-Tema1.html#definición",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.1 Definición",
    "text": "2.1 Definición\nUna serie temporal es una variable medida secuencialmente en el tiempo a intervalos equiespaciados.\nLa representaremos por,\n\\[\\{y_t\\}_{t=1}^T=\\{y_1,y_2,\\ldots,y_T\\}.\\]\nLa serie aparece indexada por su fechado \\(t\\) y el subíndice \\(T\\) hará siempre referencia a la fecha del último dato.\nEl fechado varía en su frecuencia, que puede ser anual (baja frecuencia), trimestral, mensual, semanal, diario (alta frecuencia) o disponer casi de un continuo de datos."
  },
  {
    "objectID": "03-01-Tema1.html#proceso-generador-de-datos",
    "href": "03-01-Tema1.html#proceso-generador-de-datos",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.2 Proceso generador de datos",
    "text": "2.2 Proceso generador de datos\nEl proceso generador de los datos de una serie temporal es en general desconocido, pero se puede aproximar por un modelo estadístico. Estos modelos se pueden clasificar en tres grandes familias según su naturaleza: deterministas, estocásticos y ambos.\nEn ocasiones las series temporales pueden ser modeladas de forma determinista ajustando los datos a funciones matemáticas: \\[y_t=f(t)+\\varepsilon_t.\\]\nEn los procesos estocásticos las observaciones cercanas en el tiempo tienden a estar (cor)relacionadas y se puede aprovechar esta dependencia para entender la serie y predecirla: \\[y_t=f(y_{t-1},y_{t-2},\\ldots)+\\varepsilon_t.\\]\nA veces, ambas situaciones se dan simultáneamente: \\[y_t=f(t,y_{t-1},y_{t-2},\\ldots)+\\varepsilon_t.\\]\nLa Figura 1 muestra un ejemplo gráfico de estos tres procesos generadores.\n\n\n\n\n\nFigura 1: Ejemplos de procesos generadores\n\n\n\n\nEn este curso se asumirá en todo momento que la serie temporal tiene una componente estocástica."
  },
  {
    "objectID": "03-01-Tema1.html#lectura-de-datos-y-representación-gráfica",
    "href": "03-01-Tema1.html#lectura-de-datos-y-representación-gráfica",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.3 Lectura de datos y representación gráfica",
    "text": "2.3 Lectura de datos y representación gráfica\nAntes de continuar vamos a importar en R tres de las series que usaremos de ejemplo en este tema: número de títulos (libros y folletos) publicados anualmente en España, los nacimientos mensuales en España y la demanda eléctrica diaria (GWh) en España. Servirán de ejemplo para el análisis de series con diferente fechado, anual, mensual y diario, respectivamente.\n\nTítulos publicados en España\nLibros es una serie anual de 1993 a 2019 (fuente Instituto Nacional de Estadística). Los datos están disponibles en el fichero libros.csv. La primera columna tiene el año de la serie y la segunda contiene el número de títulos publicados. En la primera fila aparece el nombre de cada columna.\n\nlibros <- read.csv2(\"./series/libros.csv\", \n                    header = TRUE)\n\nlibros <- ts(libros[, 2], \n             start = 1993, \n             frequency  = 1)\n\nUsamos para leer los datos read.csv2, indicando que la primera línea tiene el nombre de las variables. Esta función asume que el separador decimal es la coma “,” y que el separador entre variables es el punto y coma “;”. Si el separador decimal es el punto “.” y el separador de variables es la coma “,”, debes usar read.csv. En cualquiera de estas funciones puedes modificar el separador decimal por medio del argumento dec; también puedes usar el argumento sep para indicar el carácter usado como separador de variables.\nLa función ts, de la librería stats, convierte un objeto (vector o matriz) a la clase serie temporal. En este caso seleccionamos solo la segunda columna, la que contiene el número de títulos publicados.\n\nCon start indicamos el fechado del primer dato.\nCon frequency indicamos la frecuencia, que en este caso es un dato por año.\n\nUsa help(ts) para obtener más información y str(libros) para ver qué contiene un objeto serie temporal.\nPodemos dibujar la serie Libros con la función plot o mejor con autoplot. Esta última está en el paquete forecast.\nEn general, las funciones gráficas que vamos a usar pertenecen a la libraría forecast, pero en ocasiones las ampliaremos con funciones de la librería ggplot2. Te recomiendo cargar estas dos librerías desde el inicio. En casi todos los casos existe una versión de la función gráfica usada en la librería stats.\n\nlibrary(forecast)\nlibrary(ggplot2)\n\n\nautoplot(libros,\n         xlab = \"\",\n         ylab = \"Títulos\",\n         main = \"\")\n\n\n\n\nFigura 2: Títulos publicados (libros y folletos)\n\n\n\n\n\n\nNacimientos en España\nNacimientos es una serie mensual de enero de 1975 a diciembre de 2019 (fuente: Instituto Nacional de Estadística). Los datos están disponibles en el fichero nacimientos.csv. La primera columna tiene la fecha y la segunda la serie propiamente. En la primera fila aparece el nombre de cada columna. De nuevo seleccionamos solo la columna con los datos de nacimientos.\n\nnacimientos <- read.csv2(\"./series/nacimientos.csv\", \n                         header = TRUE)\n\nnacimientos <- ts(nacimientos[, 2],\n                  start = c(1975, 1),\n                  frequency = 12)\n\nEn este caso:\n\nCon start indicamos que el primer dato es enero de 1975. También sería correcto start = 1975. Si el primer dato fuera, por ejemplo, marzo de 1975, deberíamos poner start = c(1975, 3) o start = 1975 + 2/12.\nCon frequency indicamos que se tienen 12 datos (meses) por año. Si la serie fuera trimestral pondríamos frequency = 4.\n\n\nautoplot(nacimientos,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\")\n\n\n\n\nFigura 3: Nacimientos mensuales\n\n\n\n\n\n\nDemanda eléctrica\nDemanda eléctrica es una serie diaria desde el 1 enero de 2021 hasta el 31 diciembre de 2021 (fuente: Red Electrica de España). Los datos están disponibles en el fichero Consumo electrico.csv. La primera columna tiene la fecha y la segunda, la seleccionada para el fechado, la serie propiamente. En la primera fila aparece el nombre de cada columna.\n\nelectricidad <- read.csv2(\"./series/Consumo electrico.csv\",\n                          header = TRUE)\n\nelectricidad <- ts(electricidad[, 2],\n                   start = c(1, 5),\n                   frequency = 7)\n\nEn este caso:\n\nCon start indicamos que el primer dato es un viernes (día 5 de la semana) de la primera semana. Como ves no hay referencia al año. También sería correcto start = 1 + 4/7.\nCon frequency indicamos que se tienen 7 datos (días) por semana. Si la serie fuera de lunes a viernes pondríamos frequency = 5.\n\n\nautoplot(electricidad,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\nFigura 4: Consumo diario de electricidad (2021)"
  },
  {
    "objectID": "03-01-Tema1.html#funciones-útiles-para-objetos-de-clase-ts",
    "href": "03-01-Tema1.html#funciones-útiles-para-objetos-de-clase-ts",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.4 Funciones útiles para objetos de clase ts",
    "text": "2.4 Funciones útiles para objetos de clase ts\nOtras funciones relacionadas con los objetos de clase serie temporal que pueden ser útiles son:\n\nstart da el fechado del primer dato, y end da el fechado del último dato.\n\n\nstart(nacimientos); end(nacimientos)\n\n[1] 1975    1\n\n\n[1] 2019   12\n\nstart(electricidad); end(electricidad)\n\n[1] 1 5\n\n\n[1] 53  5\n\n\n\nfrequency da la frecuencia de los datos.\n\n\nfrequency(nacimientos)\n\n[1] 12\n\nfrequency(electricidad)\n\n[1] 7\n\n\n\ntime crea un vector con el fechado de una serie. Observa como guarda internamente R el fechado de una serie temporal.\n\n\nhead(time(nacimientos), n = 48)  #Mostramos sólo los 4 primeros años\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1975 1975.000 1975.083 1975.167 1975.250 1975.333 1975.417 1975.500 1975.583\n1976 1976.000 1976.083 1976.167 1976.250 1976.333 1976.417 1976.500 1976.583\n1977 1977.000 1977.083 1977.167 1977.250 1977.333 1977.417 1977.500 1977.583\n1978 1978.000 1978.083 1978.167 1978.250 1978.333 1978.417 1978.500 1978.583\n          Sep      Oct      Nov      Dec\n1975 1975.667 1975.750 1975.833 1975.917\n1976 1976.667 1976.750 1976.833 1976.917\n1977 1977.667 1977.750 1977.833 1977.917\n1978 1978.667 1978.750 1978.833 1978.917\n\nhead(time(electricidad), n = 28)  #Mostramos sólo las 4 primeras semanas\n\nTime Series:\nStart = c(1, 5) \nEnd = c(5, 4) \nFrequency = 7 \n [1] 1.571429 1.714286 1.857143 2.000000 2.142857 2.285714 2.428571 2.571429\n [9] 2.714286 2.857143 3.000000 3.142857 3.285714 3.428571 3.571429 3.714286\n[17] 3.857143 4.000000 4.142857 4.285714 4.428571 4.571429 4.714286 4.857143\n[25] 5.000000 5.142857 5.285714 5.428571\n\n\n\ncycle, crea un vector con la posición en el ciclo de cada observación. Para una serie mensual sus valores van de 1 a 12, para una serie diaria sus valores irían 1 a 7.\n\n\nhead(cycle(nacimientos), n = 48) #Mostramos sólo los 4 primeros años\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1975   1   2   3   4   5   6   7   8   9  10  11  12\n1976   1   2   3   4   5   6   7   8   9  10  11  12\n1977   1   2   3   4   5   6   7   8   9  10  11  12\n1978   1   2   3   4   5   6   7   8   9  10  11  12\n\nhead(cycle(electricidad), n = 28) #Mostramos sólo las 4 primeras semanas\n\nTime Series:\nStart = c(1, 5) \nEnd = c(5, 4) \nFrequency = 7 \n [1] 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4"
  },
  {
    "objectID": "03-01-Tema1.html#tendencia-t_t",
    "href": "03-01-Tema1.html#tendencia-t_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.1 Tendencia, \\(T_t\\)",
    "text": "3.1 Tendencia, \\(T_t\\)\nDefinición: la tendencia de una serie es su comportamiento a largo plazo. Describe los cambios sistemáticos de la serie temporal que no aparentan ser periódicos.\nRespecto a la dirección del movimiento la tendencia puede ser:\n\nCreciente: a largo plazo la serie aumenta su valor\nDecreciente: a largo plazo la serie disminuye su valor\nEstacionaria: a largo plazo la serie mantiene su valor\n\nRespecto del proceso generador de la tendencia, puede ser:\n\nDeterminista: \\(T_t = f(t)\\)\nEstocástica: \\(T_t = f(T_{t-1}, T_{t-2},\\ldots)\\)\nAmbas: \\(T_t = f(t,T_{t-1}, T_{t-2},\\ldots)\\)\n\nEn la Figura 5 se muestran ejemplos de series temporales según dirección del movimiento y pendiente de la tendencia.\n\n\n\n\n\n\n\n\n\n(a) Estacionaria\n\n\n\n\n\n\n\n\n\n(b) Creciente (lineal)\n\n\n\n\n\n\n\n(c) Creciente (exponencial)\n\n\n\n\n\n\n\n(d) Creciente (logarítmica)\n\n\n\n\n\n\n\n\n\n(e) Decreciente (lineal)\n\n\n\n\n\n\n\n(f) Dereciente (exponencial)\n\n\n\n\n\n\n\n(g) Decreciente (logarítmica)\n\n\n\n\nFigura 5: Ejemplos de tendencia\n\n\n\n\nSi la serie temporal es suficientemente larga es posible observar cambios en la dirección del movimiento de la tendencia que definen los ciclos."
  },
  {
    "objectID": "03-01-Tema1.html#ciclo-c_t",
    "href": "03-01-Tema1.html#ciclo-c_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.2 Ciclo, \\(C_t\\)",
    "text": "3.2 Ciclo, \\(C_t\\)\nDefinición: Son patrones sin periodicidad fija que abarcan varios años.\nPor ejemplo, los ciclos económicos, los cambios climáticos asociados al fenómeno El Niño, o las manchas solares (véase Figura 6).\n\n\n\nFigura 6: Ciclos solares (imagen tomada de Courtillot, Lopes, and Mouël (2021))\n\n\n\n\nLa serie de Nacimientos es lo suficientemente larga como para observarse un ciclo completo, que queda identificado por dos cambios de tendencia consecutivos de signo opuesto (véase Figura 7):\n\nA finales de la década de los 90 la tendencia decreciente en los nacimientos pasa a creciente por la llegada de inmigrantes con una mayor tasa de natalidad.\nA finales de la primera década del 2000 la tendencia creciente pasa a decreciente porque la Gran Recesión provoca el regreso a sus países de origen de muchos de estos inmigrantes.\n\nDe esta forma, observamos un ciclo completo desde 1975 hasta poco antes de 2010 (periodo entre dos cambios de tendencia), y el inicio del siguiente ciclo, en el que aun estamos.\n\n\n\n\n\nFigura 7: Ciclos en la serie Nacimientos"
  },
  {
    "objectID": "03-01-Tema1.html#estacionalidad-s_t",
    "href": "03-01-Tema1.html#estacionalidad-s_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.3 Estacionalidad, \\(S_t\\)",
    "text": "3.3 Estacionalidad, \\(S_t\\)\nDefinición: Son patrones repetitivos de periodicidad fija e inferior al año.\nEl orden de la periodicidad lo denominaremos \\(m\\), por tanto el patrón estacional se repite cada \\(m\\) periodos. Lógicamente, \\(m\\) toma el valor 12 para datos mensuales, el valor 4 para datos trimestrales, 7 para datos diarios de lunes a domingo, etc.\nLa componente estacional surge por factores climatológicos, institucionales o sociales.\nEn ocasiones no es fácil determinar la existencia de estacionalidad o su orden. En este caso, se puede usar el análisis espectral, que no veremos en este curso, para analizar esta componente. La librería forecast dispone de la función findfrequency que devuelve la frecuencia dominante de una serie usando el análisis espectral.\nLa serie Nacimientos tiene una estacionalidad de orden 12, causada principalmente por el número de días del mes. Los valles en la Figura 8 corresponden a febrero, que por tener 28 días (o 29 en años bisiestos) presenta menor número de nacimientos.\n\n\n\n\n\nFigura 8: Nacimientos\n\n\n\n\n\n\nLa Figura 9 muestra la demanda eléctrica diariamente para las cuatro semanas de febrero de 2021, desde el lunes 1 hasta el domingo 28 (semanas 6 a 9 del año). Tiene, por tanto, una estacionalidad de orden 7. En el eje OX aparecen los lunes de cada semana que permiten identificar el domingo como el día de menor demanda, seguido del sábado.\n\n\n\n\n\nFigura 9: Demanda diaria de electricidad\n\n\n\n\n\n\nSi el fechado de la serie es de muy alta frecuencia, puede ocurrir que se superponga más de una componente estacional. La Figura 10 muestra la serie corresponde a la demanda eléctrica (GW) recogida cada hora durante el mes de febrero de 2021. El eje OX señala el consumo la primera hora de cada día del mes (desde media noche hasta la una de la madrugada). Se aprecia una estacionalidad diaria de orden \\(24\\), otra semanal de orden \\(7 \\times 24\\) y si mostráramos varios años de consumo, también se observaría otra mensual.\n\n\n\n\n\nFigura 10: Estacionalidad múltiple para el consumo de electricidad por hora"
  },
  {
    "objectID": "03-01-Tema1.html#intervención-i_t",
    "href": "03-01-Tema1.html#intervención-i_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.4 Intervención, \\(I_t\\)",
    "text": "3.4 Intervención, \\(I_t\\)\nDefinición: Es un factor sistemático no periódico, o irregular, que vendría determinado por fenómenos ocasionales que provocan observaciones anómalas y valores atípicos en la serie temporal.\nPor su relación con fechas concretas, podemos distinguir dos tipos:\n\nEfectos calendario: festivos en series diarias; Semana Santa, días laborales y febrero bisiesto en series mensuales.\nOtros efectos no sujetos a calendario: catástrofes, huelgas, caída del sistema eléctrico o de los servidores de una red social, etc.\n\nEn la serie mensual Nacimientos, los meses de febrero bisiestos (puntos rojos) presentan un número de nacimientos mayor que los meses de febrero no bisiestos. Para algunos años este hecho es mas claro (véase Figura 11 ).\n\n\n\n\n\nFigura 11: Efecto de los febreros bisiestos en Nacimientos\n\n\n\n\nEn la serie diaria Electricidad cuando un día entre semana es festivo, el consumo se reduce notablemente apareciendo un efecto calendario. Si se trata de un día aislado, el efecto es muy fácil de identificar y analizar. Un ejemplo, observable en la Figura 12, es el lunes 1 de noviembre de 2021, día de Todos los Santos, identificado con un punto rojo en la figura. Se aprecia que el consumo fue muy inferior al observado el lunes precedente y posterior (puntos verdes). Si los festivos abarcan varios días, el efecto sigue siendo perfectamente identificable, pero es más complejo de analizar. Un ejemplo es la última semana de diciembre donde el consumo es muy inferior al observado en las semanas previas debido, por un lado, al efecto del día festivo de Navidad (punto azul) y, por otro lado, al periodo semi-vacacional que estas festividades supone en España.\n\n\n\n\n\nFigura 12: Efectos calendario en Electricidad\n\n\n\n\nPor su naturaleza, podemos distinguir tres tipos básicos de intervención (aunque hay más):\n\nPulso (Additive Outlier, AO)\n\nEn un periodo aislado la serie toma un valor anómalo. Por ejemplo, un día entre semana es festivo y la demanda eléctrica es inferior a la usual. Véase Figura 13, panel derecho.\n\nCambio transitorio (Transitory Change, TC)\n\nEn un periodo un shock genera un valor anómalo en la serie y el efecto del shock va desapareciendo poco a poco. Por ejemplo, las redes sociales ponen de moda un producto que temporalmente aumenta sus ventas, pero conforme pasa el tiempo los consumidores se olvidan del producto y sus ventas vuelven poco a poco a su nivel natural. Véase Figura 13, panel central.\n\nCambio permanente (Level Shift, LS)\n\nEn un periodo la serie cambia de nivel y permanece de forma permanente en este nuevo nivel. Por ejemplo, enfrente de un supermercado abre la competencia, de forma que sus ventas descienden bruscamente de forma permanente. Véase Figura 13, panel izquierdo.\n\n\n\n\n\n\n\nFigura 13: Tipos de intervención"
  },
  {
    "objectID": "03-01-Tema1.html#residuo-r_t",
    "href": "03-01-Tema1.html#residuo-r_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.5 Residuo, \\(R_t\\)",
    "text": "3.5 Residuo, \\(R_t\\)\nDefinición: No presenta un comportamiento sistemático a corto, medio o largo plazo por lo que no se puede predecir de modo alguno. Es la parte de la serie que se debe a puro azar.\nAunque inicialmente no se hará ningún supuesto sobre el residuo, se espera que sea ruido blanco (media cero, incorrelado y homocedástico), es decir \\(R_t \\sim iid(0, \\sigma^2\\))."
  },
  {
    "objectID": "03-01-Tema1.html#esquema-aditivo-y-multiplicativo",
    "href": "03-01-Tema1.html#esquema-aditivo-y-multiplicativo",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.6 Esquema aditivo y multiplicativo",
    "text": "3.6 Esquema aditivo y multiplicativo\nUna serie temporal siempre tiene tendencia y residuo. La presencia de estacionalidad, ciclo e intervención depende de la naturaleza de la serie. Por ejemplo, una serie anual no tendrá nunca estacionalidad y en una serie corta no se podrá observar el ciclo.\nLas componentes de una serie temporal se pueden combinar de múltiples formas.\nEn el esquema aditivo cada componente suma su efecto sobre las demás, \\(y_t = T_t + S_t + C_t + I_t + R_t\\). La demanda diaria de electricidad es un ejemplo de este tipo de esquema (Figura 14). El gráfico superior muestra la serie en el tiempo e identificamos el esquema aditivo porque la amplitud estacional (para cada semana la diferencia entre el día de más consumo y el de menos consumo) se mantiene constante en el tiempo. En el gráfico inferior cada punto corresponde a una semana, la coordenada X es el consumo de esa semana y la coordenada Y la desviación típica del consumo para los días de esa semana. Este segundo gráfico revela el esquema aditivo de la serie porque no se observa un patrón creciente en los puntos: más consumo no implica una mayor desviación típica.\n\n\n\n\n\n\n\n(a) Consumo electrico diario\n\n\n\n\n\n\n\n\n\n(b) Gráfica media-desviación típica semanal\n\n\n\n\nFigura 14: Ejemplo de esquema aditivo\n\n\nEn el esquema multiplicativo cada componente supone un incremento porcentual respecto de las demás, \\(y_t = T_t \\cdot S_t \\cdot C_t \\cdot I_t \\cdot R_t\\). La serie Nacimientos es un ejemplo de esquema multiplicativo: en el gráfico superior de la Figura 15, que muestra la serie, se observa que según decrece el número de nacimientos, también decrece la amplitud estacional. Además, en el gráfico inferior cada punto corresponde a un año, en el eje X los nacimientos anuales y en el eje Y la desviación típica de los nacimientos mensuales. En este gráfico se observa un patrón creciente en los puntos, revelando el esquema multiplicativo de la serie.\n\n\n\n\n\n\n\n(a) Nacimientos mensuales\n\n\n\n\n\n\n\n\n\n(b) Gráfica media-desviación anual\n\n\n\n\nFigura 15: Ejemplo de esquema multiplicativo\n\n\nSi una serie presenta un esquema multiplicativo, su logaritmo lo presentará aditivo. A lo largo del curso se verán otras razones por las que puede ser aconsejable analizar el logaritmo de una serie temporal.\nEn principio, cualquier combinación entre las componentes es posible (véase Tema 5):\n\n\\(y_t = (T_t + C_t) \\cdot S_t + I_t + R_t\\)\n\\(y_t = (T_t + S_t + C_t + I_t)R_t\\)\n\\(y_t = T_t \\cdot S_t \\cdot C_t \\cdot I_t + R_t\\)\n…"
  },
  {
    "objectID": "03-01-Tema1.html#idea-general",
    "href": "03-01-Tema1.html#idea-general",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.1 Idea general",
    "text": "4.1 Idea general\nPodemos manipular una serie temporal con diferentes fines:\n\nextraer la tendencia (eliminando la estacionalidad), por ejemplo, pasando de una serie mensual a una anual, o de una serie diaria a una semanal.\nextraer la estacionalidad (eliminando la tendencia) de forma sencilla, aunque no muy precisa.\nrecortar una serie para obtener una submuestra.\nextraer una subserie correspondiente a un único periodo estacional. Por ejemplo, los nacimientos en febrero o el consumo de electricidad de los domingos.\n\nEstas operaciones nos permitirán mejorar nuestra capacidad descriptiva de la serie, identificar mejor el tipo de esquema entre las componentes, facilitar la estimación del proceso generador o ampliar las herramientas de análisis y predicción de una serie temporal."
  },
  {
    "objectID": "03-01-Tema1.html#extracción-de-la-tendencia",
    "href": "03-01-Tema1.html#extracción-de-la-tendencia",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.2 Extracción de la tendencia",
    "text": "4.2 Extracción de la tendencia\nSi tenemos una serie con estacionalidad y agregamos la serie –obteniendo un dato por año, si la serie es mensual, o un dato por semana, si es diaria– obtenemos una nueva serie sin estacionalidad, solo con tendencia.\nDependiendo de la naturaleza de la serie, convendrá agregar sumando los datos (consumo eléctrico, títulos publicados, viajeros transportados, nacimientos) o sacando la media (temperatura, número de parados, ocupación hotelera).\nVeamos como extraer la tendencia de la serie Nacimientos usando la función aggregate con el argumento FUN = sum.\n\nnacimientosAnual <- aggregate(nacimientos, FUN = sum)\nautoplot(nacimientosAnual/1000,\n         xlab = \"\",\n         ylab = \"Nacimientos (miles)\",\n         main = \"\")\n\n\n\n\nFigura 16: Nacimientos por año\n\n\n\n\nAhora para la demanda de electricidad:\n\nelectricidadSemanal <- aggregate(electricidad, FUN = sum)\nautoplot(electricidadSemanal,\n       xlab = \"\",\n       ylab = \"GWh\",\n       main = \"\")\n\n\n\n\nFigura 17: Consumo de electricidad por semana\n\n\n\n\nLa función aggregate aplicada a una serie temporal agrega los datos de cada periodo estacional completo aplicando la función especificada en FUN.\n\nUna serie trimestral o mensual la transforma en anual, una serie diaria en semanal.\nLa función a usar dependerá de la naturaleza de los datos y del objetivo perseguido (FUN=sum, FUN=mean, FUN=sd…)\nEsta función tiene un uso más amplio en R. Usa la función help para aprenderlo."
  },
  {
    "objectID": "03-01-Tema1.html#extracción-de-la-estacionalidad",
    "href": "03-01-Tema1.html#extracción-de-la-estacionalidad",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.3 Extracción de la estacionalidad",
    "text": "4.3 Extracción de la estacionalidad\nTenemos varias alternativas gráficas y numéricas para analizar la estacionalidad de una serie. Veamos un ejemplo para la serie Nacimientos.\nPodemos hacer una gráfico de la serie contra cada periodo estacional. Este gráfico permite identificar el efecto de la estacionalidad en la serie y su evolución en el tiempo. Existen varias opciones para este tipo de gráficos, veremos dos de ellas: gráfico de subseries y gráfico de líneas. Para facilitar la interpretación vamos a trabajar con la serie desde el año 2000 que denominaremos nacimientosb.\n\nnacimientosb <- window(nacimientos, start = 2000)\n\nVeamos primero un ejemplo de gráfico de subseries (Figura 18) que muestra para cada periodo estacional la subserie de valores de ese periodo y el valor medio de la subserie. Para las series con tendencia y esquema multiplicativo, el valor medio de las subseries (líneas horizontales) puede llevarnos a una interpretación incorrecta de la estacionalidad. (La función monthplot de stats realiza un gráfico similar.)\n\nggsubseriesplot(nacimientosb) +\n  ylab(\"Nacimientos\") +\n  xlab(\"\") +\n  ggtitle(\"\")\n\n\n\n\nFigura 18: Gráfico estacional de subseries\n\n\n\n\nLa Figura 19 muestra el gráfico de líneas. Si se incluye el argumento polar=TRUE, se obtiene una versión tipo tela de araña de este gráfico.\n\nggseasonplot(nacimientosb, \n             year.labels=TRUE, \n             xlab = \"\",\n             ylab = \"Nacimientos\",\n             main = \"\")\n\n\n\n\nFigura 19: Gráfico estacional de lineas\n\n\n\n\nLas gráficas ayudan a describir y entender un poco mejor la componente estacional. Sin embargo, si deseamos estimar dicha componente, debemos proceder de otra forma.\nLa siguiente sintaxis usa la función tapply para estimar numéricamente la componente estacional bajo un esquema multiplicativo. Básicamente, calcula para cada mes (argumento cycle(nacimientosb)) la media (FUN = mean) del cociente nacimientosb / mean(nacimientosb).\n\ncomponenteEstacional <- tapply(nacimientosb/mean(nacimientosb), \n                               cycle(nacimientosb), \n                               FUN = mean)\nround(componenteEstacional, 2)\n\n   1    2    3    4    5    6    7    8    9   10   11   12 \n1.00 0.91 1.00 0.97 1.01 0.97 1.03 1.02 1.04 1.05 1.00 1.00 \n\n\nLos valores de la tabla previa indican que, por ejemplo, en febrero nacen un 9% menos de bebés respecto de la media anual y que en octubre nacen un 5% más que la media anual.\nLos cambios necesarios para estimar la componente estacional bajo un esquema aditivo son mínimos. Veámoslo para la serie Demanda eléctrica.\n\ncomponenteEstacional <- tapply(electricidad - mean(electricidad), \n                               cycle(electricidad), \n                               FUN = mean)\nround(componenteEstacional, 2)\n\n     1      2      3      4      5      6      7 \n 12.58  30.28  30.32  32.09  21.34 -42.59 -84.43 \n\n\nEn domingo la demanda de electricidad cae en 84 GWh respecto de la media semanal. De lunes a viernes la demanda eléctrica es mayor que la media semanal."
  },
  {
    "objectID": "03-01-Tema1.html#extracción-de-una-subserie",
    "href": "03-01-Tema1.html#extracción-de-una-subserie",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.4 Extracción de una subserie",
    "text": "4.4 Extracción de una subserie\nR proporciona varias funciones que permiten extraer una submuestra de la serie original. Podemos:\n\nseleccionar una submuestra especificando los puntos temporales de inicio y fin.\nseleccionar una submuestra seleccionando un periodo estacional determinado.\nquitar fácilmente un conjunto de datos usando índices.\n\nVeamos algunos ejemplos de extracción con la serie Nacimientos y las funciones window y subset:\nFunción window\n\nwindow(nacimientos, start = c(2000, 1), end = c(2009, 12)) selecciona de la serie original los datos desde enero de 2000 a diciembre de 2009.\nwindow(nacimientos, start = c(2010, 3)) selecciona de la serie original los datos desde marzo de 2010 hasta el último dato (diciembre de 2019).\nwindow(nacimientos, end = c(1999, 12)) selecciona de la serie original los datos desde el primero (enero de 1975) hasta diciembre de 1999.\nwindow(nacimientos, start = c(2000, 3), freq = TRUE) selecciona de la serie original solo los meses de marzo desde 2000.\n\nFunción subset\n\nsubset(nacimientos, start = 10, end = 34) selecciona de la serie las observaciones que van desde la 10 a la 34, ambas inclusive.\nsubset(nacimientos, start = 121) selecciona de la serie las observaciones que van desde la 121 hasta la última.\nsubset(nacimientos, start = length(nacimientos) - 47) selecciona de la serie los últimos 4 años (2016 a 2019).\nsubset(nacimientos, end = length(nacimientos) - 48) selecciona de la serie todo menos los últimos 4 años. Es decir, el último dato es diciembre de 2015.\nsubset(nacimientos, season  = 5) selecciona de la serie todos los meses de mayo.\n\nAdemás, puedes usar las funciones head y tail para extraer las primeras o las últimas observaciones."
  },
  {
    "objectID": "03-01-Tema1.html#concepto",
    "href": "03-01-Tema1.html#concepto",
    "title": "Series Temporales. Definición y componentes",
    "section": "5.1 Concepto",
    "text": "5.1 Concepto\nLos métodos que hemos visto para la descripción de la tendencia y la componente estacional son muy sencillos, pero no son ni rigurosos ni precisos. Veamos métodos más adecuados para extraer de una serie sus componentes: tendencia-ciclo, estacionalidad, e intervención-residuo.\nSi la serie es demasiado corta para poder extraer el ciclo, entonces el ciclo queda recogido dentro de la tendencia. Por otro lado, las técnicas de identificación de la intervención son complejas por lo que esta componente queda incorporada al residuo. Por tanto, asumiremos que una serie tiene sólo Tendencia, Estacionalidad y Residuo:\n\nEsquema aditivo \\(y_t = T_t + C_t + S_t + I_t + R_t = (T_t + C_t) + S_t +(I_t + R_t) = T'_t + S_t + R'_t\\)\nEsquema multiplicativo \\(y_t = T_t \\cdot C_t \\cdot S_t \\cdot I_t \\cdot R_t = (T_t \\cdot C_t) \\cdot S_t \\cdot (I_t \\cdot R_t) = T'_t \\cdot S_t \\cdot R'_t\\)\n\nVeremos a continuación como extraer estas tres componentes a partir de una serie original. Este proceso se denomina descomposición.\nHay múltiples formas de realizar una descomposición. Aquí veremos dos de ellas, la más sencilla, basada en el concepto de medias móviles (decompose), y otra más versátil y compleja a partir de regresiones locales ponderadas (stl).\nAdemás, R proporciona (a través de paquetes específicos) el método de descomposición que utiliza el US Census Bureau and Statistics Canada, denominado X11, y el método que utiliza el Banco de España, denominado SEATS (Seasonal Extraction in ARIMA Time Series), aunque estos métodos solo son válidos para series mensuales y trimestrales.\nEn origen los métodos de descomposición no sirven para realizar predicciones, pero actualmente se usan también con este fin (véase las funciones stlm y stlf del paquete forecast)."
  },
  {
    "objectID": "03-01-Tema1.html#descomposición-por-medias-móviles",
    "href": "03-01-Tema1.html#descomposición-por-medias-móviles",
    "title": "Series Temporales. Definición y componentes",
    "section": "5.2 Descomposición por medias móviles",
    "text": "5.2 Descomposición por medias móviles\n\nIdeas generales\nLa función decompose estima las componentes de tendencia, estacionalidad y residuo usando el método de medias móviles (que veremos con más detalle en el Tema 4). En concreto decompose sigue los siguientes pasos para obtener cada componente:\nPaso 1: Se estima la tendencia de una serie a partir de una media móvil centrada. Si el orden estacional es par, la media móvil es ponderada de orden \\(m + 1\\); y si el orden estacional es impar, la media móvil es de orden \\(m\\). En concreto,\n\nSi \\(m=2k\\): \\(\\hat{T}_t = \\frac{\\frac{1}{2}y_{t-k} + y_{t-k+1} + \\ldots + y_t + \\ldots + y_{t+k-1} + \\frac{1}{2} y_{t+k}}{m}\\),\nSi \\(m=2k+1\\): \\(\\hat{T}_t = \\frac{y_{t-k} + y_{t-k+1} + \\ldots + y_t + \\ldots + y_{t+k-1} + y_{t+k}}{m}\\).\n\nPaso 2: Para un modelo con esquema aditivo calculamos la serie sin tendencia como \\(y_t - \\hat{T}_t\\) y para un esquema multiplicativo como \\(y_t/ \\hat{T}_t\\).\nPaso 3: Para estimar la componente estacional para cada periodo estacional, calculamos el valor medio de la serie sin tendencia (paso 2) de forma independiente para los datos de cada estación. Así, obtenemos un vector con la estimación de las \\(m\\) componentes estacionales.\nDespués estos valores se ajustan para que sumen 0 (esquema aditivo) o para que sumen \\(m\\) (esquema multiplicativo). La componente estacional se obtiene repitiendo el vector de \\(m\\) componentes ajustadas hasta alcanzar la longitud de la serie original. Esto da \\(\\hat{S}_t\\)\nPaso 4: El residuo se obtiene como \\(\\hat{R}_t = y_t - \\hat{T}_t - \\hat{S}_t\\) (esquema aditivo) o \\(\\hat{R}_t = y_t / (\\hat{T}_t \\cdot \\hat{S}_t)\\) (esquema multiplicativo)\n\n\nLa Tabla 1 muestra un ejemplo de descomposición aditiva por medias móviles para una serie simulada de orden estacional 5.\n\nLa dos primeras columnas indican la estación de cada dato y el valor de la serie, para un total de 25 datos. La columna Ten ha sido obtenida siguiendo el paso 1 como una media móvil de orden 5: \\[Ten_t = (Serie_{t-2} + Serie_{t-1} + Serie_{t} + Serie_{t+1} + Serie_{t+2})/5.\\]\nLa serie sin tendencia, columna Est + Res, se obtiene restando a la columna Serie la columna Ten, tal y como se indica en el paso 2.\nPara el cálculo de la columna Est, que repite de forma periódica la primera estimación de las 5 componentes estacionales, se sigue el paso 3. Para cada estación se promedian los valores de la columna Est + Res correspondientes a dicha estación. La suma de los cinco valores de la componente estacional así obtenidos vale 1.1.\nPara ajustar la componente estacional para que sume 0 a cada valor de la componente estacional se le resta su suma actual 1.1 dividida por 5, el número de estaciones. El resultado de este ajuste aparece en la columna Est corregida que será la componente estacional final.\nSiguiendo el paso 4, la columna Res se calcula restando a la serie original (columna Serie) la suma de la tendencia y la estacionalidad (columnas Ten y Est corregida).\n\nObserva que en el proceso de descomposición se han perdido 4 datos para la tendencia y el residuo, dos al inicio de la serie y dos al final.\n\n\n\n\n\n\n\nTabla 1: Ejemplo de descomposición por medias móviles\n\n\nEstacion\nSerie\nTen\nEst + Res\nEst\nEst corregida\nRes\n\n\n\n\n1\n17.00\nNA\nNA\n9.14\n8.92\nNA\n\n\n2\n6.72\nNA\nNA\n-10.89\n-11.11\nNA\n\n\n3\n5.08\n20.62\n-15.54\n-10.06\n-10.28\n-5.48\n\n\n4\n8.79\n27.89\n-19.10\n-9.46\n-9.68\n-9.64\n\n\n5\n65.53\n28.00\n37.53\n22.37\n22.15\n15.16\n\n\n1\n53.31\n28.58\n24.73\n9.14\n8.92\n15.59\n\n\n2\n7.28\n29.79\n-22.51\n-10.89\n-11.11\n-11.62\n\n\n3\n8.00\n25.62\n-17.62\n-10.06\n-10.28\n-7.56\n\n\n4\n14.84\n19.13\n-4.29\n-9.46\n-9.68\n5.17\n\n\n5\n44.67\n20.08\n24.59\n22.37\n22.15\n2.22\n\n\n1\n20.85\n21.78\n-0.93\n9.14\n8.92\n-10.07\n\n\n2\n12.02\n21.64\n-9.62\n-10.89\n-11.11\n1.27\n\n\n3\n16.51\n18.97\n-2.46\n-10.06\n-10.28\n7.60\n\n\n4\n14.14\n22.07\n-7.93\n-9.46\n-9.68\n1.53\n\n\n5\n31.31\n23.08\n8.23\n22.37\n22.15\n-14.14\n\n\n1\n36.37\n24.68\n11.69\n9.14\n8.92\n2.55\n\n\n2\n17.06\n26.66\n-9.60\n-10.89\n-11.11\n1.29\n\n\n3\n24.53\n30.92\n-6.39\n-10.06\n-10.28\n3.67\n\n\n4\n24.02\n30.56\n-6.54\n-9.46\n-9.68\n2.92\n\n\n5\n52.62\n33.48\n19.14\n22.37\n22.15\n-3.23\n\n\n1\n34.59\n33.51\n1.08\n9.14\n8.92\n-8.06\n\n\n2\n31.66\n33.51\n-1.85\n-10.89\n-11.11\n9.04\n\n\n3\n24.65\n32.95\n-8.30\n-10.06\n-10.28\n1.76\n\n\n4\n24.01\nNA\nNA\n-9.46\n-9.68\nNA\n\n\n5\n49.86\nNA\nNA\n22.37\n22.15\nNA\n\n\n\n\n\n\n\n\nLos principales inconvenientes de este método de descomposición son que se perderán datos al inicio y final de la serie –por ejemplo, si la serie es mensual se perderán seis datos al inicio y seis al final–, y que asume que la componente estacional no ha variado en el tiempo. Sin embargo, sabemos que para muchas series sociales y de consumo la componente estacional se ha suavizando con el tiempo.\nPor el contrario, una se las ventajas de este método, además de su sencillez de cálculo, es que se puede usar tanto para esquemas aditivos (type=\"addi\") como multiplicativos (type=\"multi\").\nLa función decompose genera un objeto con las siguientes componentes:\n\n$x para la serie original,\n$trend para la tendencia,\n$seasonal para la estacionalidad,\n$random para el residuo, y\n$figure que contiene las estimaciones de los m efectos estacionales ajustados. Es una extracción para un único año o semana de $seasonal.\n\nSiempre que generes nuevos objetos en R a partir de funciones te recomiendo que con names y str mires que hay en su interior.\nEn los métodos de descomposición que vamos a ver, para obtener las componentes individualmente puedes usar la función seasonal para la componente estacional, trendcycle para el componente de tendencia, y remainder para el residuo.\n\n\nEjemplo de esquema aditivo\nVamos a descomponer la serie Demanda eléctrica asumiendo un esquema aditivo (type = \"addi).\n\neleDesAdi <- decompose(electricidad, \n                       type = \"addi\")\n\nautoplot(eleDesAdi,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\nFigura 20: Descomposición aditiva de la Demanda eléctrica por medias móviles\n\n\n\n\nEs fácil verificar que si se suma para cada fecha la tendencia, la estacionalidad y el residuo se obtiene exactamente el valor de la serie:\n\ntmp <- trendcycle(eleDesAdi) + seasonal(eleDesAdi) + remainder(eleDesAdi)\nsummary(electricidad - tmp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0       0       0       0       0       0       6 \n\n\nA continuación, tienes un ejemplo del manejo de las componentes extraídas para hacer una gráfica.\n\nautoplot(electricidad, \n         series=\"Demanda eléctrica\",\n         xlab = \"\",\n         ylab = \"MWh\",\n         main = \"\") +\n  autolayer(trendcycle(eleDesAdi), \n            series=\"Tendencia\") +\n  scale_colour_manual(values=c(\"Demanda eléctrica\"=\"black\",\"Tendencia\"=\"red\"),\n                      breaks=c(\"Demanda eléctrica\",\"Tendencia\"))\n\n\n\n\nFigura 21: Demanda eléctrica: serie y tendencia\n\n\n\n\nTambién podemos ver las componentes estacionales y verificar que suman 0. Ojo, como la serie empieza un viernes, la primera componente que se muestra es la del viernes y la última la del jueves.\n\neleDesAdi$figure\n\n[1]  24.91077 -43.49535 -86.11384  12.17731  29.87723  30.23754  32.40634\n\nsum(eleDesAdi$figure)\n\n[1] 0.000000000000003552714\n\n\nPor último, podemos realizar una gráfica de la componente estacional.\n\ncompEstacional <- eleDesAdi$figure[c(4:7, 1:3)]\nggplot() +\n  geom_line(aes(x = 1:7, y = compEstacional)) + \n  geom_hline(yintercept = 0, colour = \"blue\", lty = 2) +\n  ggtitle(\"\") +\n  xlab(\"\") +\n  ylab(\"GWh\") +\n  scale_x_continuous(breaks= 1:7, \n                     labels = c(\"Lunes\", \"Martes\", \"Miércoles\", \"Jueves\", \n                                \"Viernes\", \"Sábado\", \"Domingo\")) \n\n\n\n\nFigura 22: Componente estacional de Electricidad (esquema aditivo)\n\n\n\n\n\n\nEjemplo de Esquema Multiplicativo\nVeamos ahora la descomposición de Nacimientos bajo un esquema multiplicativo (type = \"mult\").\n\nnacDesMul <- decompose(nacimientos, \n                       type = \"mult\")\n\nautoplot(nacDesMul,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\nFigura 23: Descomposición multiplicativa de Nacimientos por medias móviles\n\n\n\n\nObserva que por tratarse de un esquema multiplicativo en la Figura 23 la componente estacional se mueve alrededor del valor 1 y debe interpretarse como una variación porcentual. Igualmente, el residuo también gira en torno al valor 1.\nLas componentes estacionales se deben interpretar como incrementos porcentuales: en febrero nacen un 9.1% menos de niños y en octubre un 3.3% más, respecto de la media anual. Además, la suma de las componentes estacionales será 12.\n\nnacDesMul$figure\n\n [1] 0.9982724 0.9090410 1.0063202 0.9841434 1.0342330 0.9821019 1.0350563\n [8] 1.0165354 1.0300831 1.0335096 0.9780025 0.9927012\n\nsum(nacDesMul$figure)\n\n[1] 12"
  },
  {
    "objectID": "03-01-Tema1.html#descomposición-por-regresiones-locales-ponderadas",
    "href": "03-01-Tema1.html#descomposición-por-regresiones-locales-ponderadas",
    "title": "Series Temporales. Definición y componentes",
    "section": "5.3 Descomposición por regresiones locales ponderadas",
    "text": "5.3 Descomposición por regresiones locales ponderadas\n\nIdeas generales\nLa función stl estima las componentes de tendencia y estacionalidad a partir de regresiones locales ponderadas (técnica conocida como loess)\nSus ventajas son:\n\nNo se perderán datos al inicio o al final de la serie.\nAsume que tanto la tendencia como la estacionalidad pueden cambiar con el tiempo y posibilita controlar este cambio a partir de parámetros.\nEs bastante robusta frente a valores atípicos.\n\nSu principal desventaja es que esta técnica de descomposición solo es válida para esquemas aditivos. Es posible obtener con stl una descomposición multiplicativa descomponiendo primero el logaritmo de la serie, para después calcular la exponencial de las componentes.\nLa función stl genera un objeto con la componente $time.series que contiene en columna tres series temporales: seasonal, trend y remainder (de nuevo usa names y str para aprender más).\nLos dos parámetros principales que deben elegirse cuando se utiliza stl son la ventana de tendencia (t.window) y la ventana estacional (s.window). Estos parámetros controlan la rapidez con la que pueden cambiar los componentes de tendencia y estacional con el tiempo. Valores pequeños permiten cambios más rápidos, valores grandes implican que no hay cambios. Ambos parámetros deben ser números impares:\n\nt.window es el número de observaciones consecutivas que se deben utilizar al estimar la tendencia. Consulta la ayuda para ver el valor por defecto.\ns.window está relacionado con el número observaciones que se deben utilizar al estimar cada valor de la componente estacional. No hay ningún valor por defecto para este parámetro. Establecerlo como periodic equivale a que la componente estacional sea periódica (es decir, idéntica a lo largo de los años). Si es un valor numérico, debe ser impar y mayor o igual a 7.\n\n\n\nEjemplo\nVeamos un ejemplo de su uso para la serie Demanda eléctrica (Figura 24). Se ha usado el valor por defecto para t.window y se ha indicado que la estacionalidad es constante en el tiempo (s.window = \"periodic\"). Además, se ha especificado que se tenga en cuenta la posible existencia de valores atípicos (robust = TRUE).\n\neleStl <- stl(electricidad, \n              s.window = \"periodic\",\n              robust = TRUE)\nhead(eleStl$time.series)\n\nTime Series:\nStart = c(1, 5) \nEnd = c(2, 3) \nFrequency = 7 \n          seasonal    trend    remainder\n1.571429  24.60393 757.7300 -173.6180276\n1.714286 -45.25864 763.9929  -38.3018792\n1.857143 -89.85937 770.2558    4.0927667\n2.000000  16.30943 777.5211   -0.2045157\n2.142857  30.74631 784.7863   -8.9444823\n2.285714  31.92108 792.5055 -124.2819712\n\n\n\nautoplot(eleStl,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\nFigura 24: Descomposición de Electricidad por regresores locales ponderados\n\n\n\n\nPodemos ver numéricamente las componentes estacionales, que de nuevo deben sumar cero.\n\nhead(seasonal(eleStl), 7)\n\nTime Series:\nStart = c(1, 5) \nEnd = c(2, 4) \nFrequency = 7 \n[1]  24.60393 -45.25864 -89.85937  16.30943  30.74631  31.92108  31.53726\n\nsum(head(seasonal(eleStl), 7))\n\n[1] -0.0000001507404\n\n\n\n\nPara Demanda eléctrica hemos obtenido tres estimaciones de la componente estacional: la primera obtenida con tapply, la segunda obtenida con decompose y la tercera con stl. Se puede observar que las tres estimaciones son muy similares, pero no coincidentes, y aunque los métodos de descomposición son preferibles a tapply, ninguna estimación es a priori mejor que otra.\n\n# tapply\nround(as.numeric(componenteEstacional), 2)\n# decompose\nround(seasonal(eleDesAdi)[c(4:7, 1:3)], 2)\n# stl\nround(seasonal(eleStl)[c(4:7, 1:3)], 2)\n\n\n\n           [,1]  [,2]  [,3]  [,4]  [,5]   [,6]   [,7]\ntapply    12.58 30.28 30.32 32.09 21.34 -42.59 -84.43\ndecompose 12.18 29.88 30.24 32.41 24.91 -43.50 -86.11\nstl       16.31 30.75 31.92 31.54 24.60 -45.26 -89.86\n\n\n\n\nSi en lugar de periodic, fijamos el parámetro s.window a, por ejemplo, 11 (siempre un valor impar), estaremos permitiendo que la estacionalidad cambien en el tiempo. La Figura 25 muestra la componente estacional estimada previamente (bajo el supuesto de componente estacional constante) y la que se obtiene con el argumento s.window = 11. Para el periodo mostrado se observa que la componente estacional ha variando con el tiempo. Cuanto mayor es el valor (impar) de s.window más constante en el tiempo es la componente estacional.\n\n\n\n\n\nFigura 25: Componente estacional para Demanda eléctrica"
  },
  {
    "objectID": "04-03-Redes_neuronales.html",
    "href": "04-03-Redes_neuronales.html",
    "title": "Autorregresión con redes neuronales",
    "section": "",
    "text": "1 Antecedentes\nEn las dos grandes familias de modelos que permiten ajustar y predecir series temporales –Alisado Exponencial y modelos ARIMA– se ajusta un modelo a una serie temporal y el resultado del ajuste nos permite no solo predecir, sino aprender y entender el comportamiento de la serie. Por ejemplo, el resultado del ajuste por alisado nos permite saber si la pendiente de la serie cambia en el tiempo (parámetro \\(\\beta\\) del ajuste) o el tipo de esquema de la serie según que la estacionalidad sea aditiva o multiplicativa. Con los modelos ARIMA podemos estimar el impacto en la serie de un efecto calendario (Semana Santa, días laborables…).\nOtra familia de modelos muy versátiles que permiten predecir con todo tipo de datos –transversales, series temporales, imágenes, espacio-temporales…– son las redes neuronales. Estos modelos son el embrión del Deep Learning y el motor de muchas AI y los estudiaréis en detalle el próximo año en la asignatura Técnicas Avanzadas de Predicción en Negocios.\nVamos a ver muy, pero que muy por encima en que consisten las redes neuronales y como se pueden aplicar para predecir series temporales. Esto es una pequeña píldora.\n\n\n2 Arquitectura de una red neuronal de una capa\nUna red neuronal puede ser entendida como una red de neuronas dispuestas en capas. Siempre hay una capa de entrada de los datos y una capa de salida de la respuesta. Entre estas dos capas se pueden disponer de tantas capas intermedias (ocultas) como se considere necesario.\nCada capa está formada por un número determinado y potencialmente diferente de neuronas o nodos. Los nodos de una capa están conectadas a los nodos de la siguiente. Por simplicidad asumiremos que todos los nodos de una capa se conectan con los nodos de la capa siguiente.\nAquí vamos a considerar solo redes neuronales con una capa intermedia y donde la capa de salida tiene solo una neurona. La Figura 1 es un ejemplo de este tipo de redes neuronales.\n\n\n\n\n\nFigura 1: Red neuronal con una sola capa intermedia\n\n\nEn esta red cada nodo de una capa recibe entradas de los nodos de la capa previa. Dicho de otra forma, las salidas de los nodos de una capa son las entradas de los nodos de la siguiente capa. Es lo que se denomina una multilayer feed-forward network.\nLas entradas que recibe cada nodo se combinan usando una función lineal ponderada. Por ejemplo, un nodo \\(j\\) de la capa intermedia recibe las dos entradas \\(x_1\\) y \\(x_2\\) de los nodos de la capa de entrada y los combina linealmente\n\\[z_j = b_j + \\sum_{i=1}^2 w_{i,j}x_i\\] Para los nodos de la capa intermedia el valor \\(z_j\\) se transforma usando una función no lineal, por ejemplo la sigmoidea:\n\\[s_j = \\frac{1}{1 + e^{-z_j}}\\] y este valor \\(s_j\\) es la salida del nodo \\(j\\) que va al nodo de la capa de salida.\nLos valores de los pesos \\(b_1\\), \\(b_2\\), \\(w_{1,1}\\), \\(w_{1,2}\\)…\\(w_{2,5}\\) se deben ajustar a partir de los datos. Estos valores suelen estar restringidos para evitar que sean demasiado grandes. El parámetro que restringe las ponderaciones se conoce como parámetro de decaimiento, y suele ser igual a \\(0.1\\).\nLos pesos toman valores aleatorios al principio y luego se actualizan con los datos observados en un proceso de aprendizaje. Por lo tanto, hay un elemento de aleatoriedad en las predicciones producidas por una red neuronal. Por este motivo, la red suele entrenarse varias veces utilizando diferentes puntos de partida aleatorios, y los resultados se promedian.\n\n\n3 Autoregresión de redes neuronales\nEn el contexto de series temporales, los valores de entrada pueden ser valores retardados de la serie y el valor de salida deseado el valor contemporáneo. De la misma forma que en un modelo AR usamos los datos pasados para predecir el futuro.\n\n\n\nFigura 2: Red neuronal para predecir una serie temporal. El dato del periodo \\(t\\) se predice a partir de los dos datos previos.\n\n\nVamos a extender estas ideas e ir añadiendo algo de notación.\nComo hemos indicado vamos a considerar solo redes simples con una capa intermedia y una capa de salida de un solo nodo, que denominaremos \\(NNAR\\). La notación \\(NNAR(p, k)\\) indica que hay \\(p\\) valores desfasados en la capa de entrada y \\(k\\) nodos en la capa intermedia. Por ejemplo, la red de la Figura 2 es modelo \\(NNAR(2,5)\\), donde \\(y_{t-1}\\) e \\(y_{t-2}\\) son usados para predecir \\(y_t\\). Así, un modelo \\(NNAR(p, 0)\\) sería equivalente a un modelo \\(ARIMA(p,0,0)\\).\nSi la serie tiene estacionalidad es conveniente que entre los datos de entrada estén las observaciones pasadas de la misma estación que se desea predecir. Por ejemplo, para la serie diaria de consumo eléctrico un modelo \\(NNAR(2, 1, k)\\) usaría como datos de entrada \\(y_{t-1}\\), \\(y_{t-2}\\) e \\(y_{t-7}\\) para predecir \\(y_t\\). En general, \\(NNAR(p,P,k)_m\\) usa como datos de entrada \\(y_{t-1}\\),\\(y_{t-2}\\),…,\\(y_{t-p}\\),\\(y_{t-m}\\), \\(y_{t-2m}\\),…,\\(y_{t-Pm}\\) y una capa intermedia de \\(k\\) neuronas. Por lo tanto, \\(NNAR(p,P,0)_m\\) es equivalente a \\(ARIMA(p,0,0)(P,0,0)_m\\).\n\n\n4 Aplicación\nLa función nnetar de la librería forecast permite estimar modelos \\(NNAR(p,P,k)_m\\). En su forma más sencilla el usuario no tiene que especificar los valor de los parámetros \\(p\\), \\(P\\) y \\(k\\) ya que la función los identifica según ciertos criterios.\nLa siguiente gráfica muestra el consumo eléctrico en España en GWh para 17 semanas desde febrero hasta mayo de 2021. Hay una fuerte componente estacional diaria de orden \\(7\\), donde el consumo es alto de lunes a viernes, algo mas reducido el sábado y aún menor el domingo.\n\nelectricidad <- read_csv2(\"./series/Consumo electrico.csv\")\n\nelectricidad <- ts(electricidad[, 2], \n                   start = c(1, 5),\n                   frequency = 7)\n\nelectricidad <- window(electricidad,\n                       start = c(6, 1),\n                       end = c(22, 7))\n\nautoplot(electricidad) + \n  ggtitle(\"\") +\n  ylab(\"GWh\") + \n  xlab(\"Semana\")\n\n\n\n\nFigura 3: Consumo eléctrico (febrero a mayo de 2021)\n\n\n\n\nLa Figura 4 muestra la serie y su predicción para los siguientes 14 días. El modelo ajustado es \\(NNAR(8,1,4)_7\\). Es decir, para predecir el consumo del día \\(t\\), \\(y_t\\), se usa el consumo de los ocho días previos \\(y_{t-1}\\) a \\(t_{t-8}\\), y de hace una semana \\(y_{t-7}\\), y la capa intermedia tiene cuatro nodos. Observa que en la capa de entrada, con 9 nodos, el valor de \\(y{t_7}\\) entra en dos nodos.\n\nfit <- nnetar(electricidad)\n\naccuracy(fit)\n\n                      ME     RMSE      MAE         MPE      MAPE      MASE\nTraining set 0.005151663 7.175332 4.867393 -0.02438996 0.7248567 0.2444698\n                  ACF1\nTraining set 0.0141848\n\npfit <- forecast(fit, h = 14)\n\nautoplot(pfit) +\n  ylab(\"GWh\") + \n  xlab(\"Semana\")\n\n\n\n\nFigura 4: Consumo eléctrico (febrero a mayo de 2021) y predicción\n\n\n\n\nEl cálculo de intervalos de confianza con redes neuronales es un proceso complejo y costoso temporalmente.\n\ntiempo <- Sys.time()\npfit <- forecast(fit, \n                 h = 14, \n                 level = 95,\n                 PI = TRUE)\n\nSys.time() - tiempo\n\nTime difference of 11.89388 secs\n\npfit\n\n         Point Forecast    Lo 95    Hi 95\n23.00000       665.2036 651.9299 680.3791\n23.14286       690.7947 669.9189 708.0488\n23.28571       695.4350 669.1675 711.5880\n23.42857       697.1413 662.0749 714.5815\n23.57143       689.7987 655.9712 707.7241\n23.71429       622.2838 589.4415 665.3986\n23.85714       571.5552 550.2122 611.3327\n24.00000       658.7529 623.3225 693.4924\n24.14286       687.2702 647.9929 718.2528\n24.28571       693.6632 638.9706 718.5955\n24.42857       696.0177 624.4678 717.7833\n24.57143       690.8866 618.8557 712.5640\n24.71429       628.2866 588.4846 680.0722\n24.85714       573.8105 546.4726 631.6005\n\n\nLa función nnetar admite la inclusión de variables de intervención de la forma usual a través del argumento xreg."
  }
]