[
  {
    "objectID": "04-08-Series_interrumpidas.html#uso-de-modelos-muy-adaptativos",
    "href": "04-08-Series_interrumpidas.html#uso-de-modelos-muy-adaptativos",
    "title": "Prediciendo series temporales interrumpidas",
    "section": "Uso de modelos muy adaptativos",
    "text": "Uso de modelos muy adaptativos\nAlgunos modelos, por su naturaleza, reaccionan muy rápidamente ante cambios en la estructura de la serie, adaptándose a ellos. Uno de estos modelos es el Alisado exponencial.\nCuando los parámetros de un modelo de Alisado están próximos a 1, el modelo usa preferentemente la información más reciente de la serie para ajustarse y predecir. De esta forma, ante una perturbación en la serie, estos modelos pueden ajustarse a ella con sencillez y rapidez.\nLa ventaja de esta estrategia es que simplemente hay que usar un modelo conocido, que es muy sencillo y muy rápido computacionalmente. Además, el ajuste y predicción con estos modelos es automático. La desventaja principal de los modelos de Alisado es que tardan algunos periodos en adaptarse, así que si los cambios se producen de forma constante, el modelo estará constantemente inadaptado."
  },
  {
    "objectID": "04-08-Series_interrumpidas.html#uso-de-intervención",
    "href": "04-08-Series_interrumpidas.html#uso-de-intervención",
    "title": "Prediciendo series temporales interrumpidas",
    "section": "Uso de intervención",
    "text": "Uso de intervención\nSi la perturbación no es excesivamente compleja, puede incluirse en el proceso de ajuste como intervención de un modelo Arima.\nEsta estrategia implica que tenemos un buen entendimiento de la perturbación: punto de inicio y final, efecto sobre la serie temporal, etc.\nSu ventaja de nuevo es que trabajamos con modelos ya conocidos y, además, que podremos estimar la estructura de la tendencia y la estacionalidad pasadas y sus cambios con la perturbación. Ahora bien, el modelo asumirá que el comportamiento de la serie tras la perturbación es similar al observado antes de la perturbación. Si esto no es cierto, las predicciones serán del todo incorrectas. Pero incluso si el supuesto es cierto y las previsiones son acertadas, su intervalo de confianza será más estrecho de lo correcto."
  },
  {
    "objectID": "04-08-Series_interrumpidas.html#fijar-las-observaciones-durante-la-perturbacion-como-valores-perdidos",
    "href": "04-08-Series_interrumpidas.html#fijar-las-observaciones-durante-la-perturbacion-como-valores-perdidos",
    "title": "Prediciendo series temporales interrumpidas",
    "section": "Fijar las observaciones durante la perturbacion como valores perdidos",
    "text": "Fijar las observaciones durante la perturbacion como valores perdidos\nUna aproximación más radical consiste en fijar como valores perdidos todas las observaciones de la serie temporal durante el periodo de la perturbación y ajustar un modelo a la serie resultante. Las predicciones que se realicen con este modelo serían las correspondientes a una realidad donde la perturbación no ha tenido lugar.\nUna ventaja de esta aproximación es que no se usa información durante el periodo de la perturbación, por lo que el intervalo de confianza de las predicciones será amplio durante la perturbación y tras ella, y no se irá estrechando hasta que haya suficientes datos como para estimar la distribución de las predicciones con más precisión.\nEntre las desventajas de este método están que solo se pueden usar modelos que permitan estimar con datos perdidos, por ejemplo modelos Arima, y que es necesario identificar en que periodo se inicia y termina la perturbación."
  },
  {
    "objectID": "04-08-Series_interrumpidas.html#trabajar-bajo-el-escenario-qué-hubiera-pasado-si",
    "href": "04-08-Series_interrumpidas.html#trabajar-bajo-el-escenario-qué-hubiera-pasado-si",
    "title": "Prediciendo series temporales interrumpidas",
    "section": "Trabajar bajo el escenario qué hubiera pasado si",
    "text": "Trabajar bajo el escenario qué hubiera pasado si\nEsta estrategia en un poco más elaborada que las ya mencionadas dado que toma elementos de varias de ellas. La idea básica es obtener la serie temporal que correspondería a un escenario donde la perturbación no ha tenido lugar y usar esta serie para predecir.\nPara el primer paso, obtener una serie completa donde la perturbación no ha tenido lugar tenemos dos alternativas:\n\nTerminamos la serie justo antes de la perturbación, la ajustamos a un modelo y hacemos predicciones durante todo el periodo de la perturbación. Estas predicciones sustituirán los valores reales de la serie, las que han tenido lugar durante a la perturbación.\nAsignamos como valores perdidos los datos de la serie durante la perturbación y ajustamos un modelo. Luego sustituimos los valores reales de la serie durante el periodo de la perturbación por los valores estimados por el modelo durante este mismo periodo.\n\nEn cualquiera de los dos casos, el resultado es una nueva serie que coincide con la original fuera del periodo de la perturbación y durante la perturbación toma valores que hubieran podido tener lugar en un escenario donde esta no ha ocurrido.\nEsta nueva serie se ajusta por un modelo que, posteriormente, se usa para obtener las predicciones.\nLógicamente, esta estrategia comparte las ventajas y desventajas de las estrategias que usa para su implementación."
  },
  {
    "objectID": "04-08-Series_interrumpidas.html#uso-de-modelos-muy-adaptativos-1",
    "href": "04-08-Series_interrumpidas.html#uso-de-modelos-muy-adaptativos-1",
    "title": "Prediciendo series temporales interrumpidas",
    "section": "Uso de modelos muy adaptativos",
    "text": "Uso de modelos muy adaptativos\nObserva que esta aproximación básicamente consiste en no hacer nada más allá de elegir un método muy adaptativo. Por este motivo compararemos los resultados obtenidos con Alisado, un modelo muy adaptativo, y con Arima, un modelo muy poco adaptativo.\nLa Figura 2 muestra el resultado de aplicar Alisado y Arima a los datos. Las predicciones para 2020 se han realizado antes de que el efecto de la Covid-19 tenga lugar y siguen el patrón de pernoctaciones observado en el pasado. Ambos métodos ofrecen prácticamente las mismas predicciones y ambos sobrestiman la realidad.\nLas predicciones para 2021 se realizan después de observar 9 meses de fuerte caída de las pernoctaciones como efecto de la Covid-19 y ambos modelos se ajustan a esta caída. Sin embargo, ninguno es capaz de captar la recuperación ocurrida en las pernoctaciones en 2021, de forma que las previsiones de ese año se quedan por debajo de la realidad. Para el año 2022 ambos modelos son capaces de reconocer el incremento en la pernoctaciones ocurrida en los pasados meses, pero de nuevo no captan que las pernoctaciones seguirán creciendo y vuelven a realizar previsiones por debajo de la realidad. Si en 2021 las predicciones mejores son las obtenidas con Arima, en el año siguiente son la de Alisado.\nPara 2023 Alisado se ha adaptado plenamente a la casi completa recuperación ocurrida en 2022 y sus predicciones son razonablemente buenas. Por el contrario Arima realiza unas predicciones muy bajas debido a que el modelo identificado usa datos no solo de 2022 sino de años pasados. Por este mismo motivo las predicciones con Arima de 2024 y 2025 vuelven a ser muy bajas. Además, sorprende lo elevadas de las predicciones de Alisado, que por lo demás repiten razonablemente bien el patrón observado en los dos años previos.\nEn general, ninguno de los dos métodos lo hace especialmente bien, aunque con el método de Alisado se obtienen mejores predicciones una vez la perturbación ha pasado.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Predicciones para 2020\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicciones para 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Predicciones para 2022\n\n\n\n\n\n\n\n\n\n\n\n(d) Predicciones para 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Predicciones para 2024\n\n\n\n\n\n\n\n\n\n\n\n(f) Predicciones para 2025\n\n\n\n\n\n\n\nFigura 2: Modelos adaptativos. Cada panel muestra las predicciones de los doce meses de un año para la serie Pernoctaciones usando modelos de Alisado y Arima."
  },
  {
    "objectID": "04-08-Series_interrumpidas.html#uso-de-intervención-1",
    "href": "04-08-Series_interrumpidas.html#uso-de-intervención-1",
    "title": "Prediciendo series temporales interrumpidas",
    "section": "Uso de intervención",
    "text": "Uso de intervención\nAplicaremos la estrategia de la intervención usando el modelo Arima. Con tal fin vamos a crear dos variables de intervención: un primer cambio de nivel asociado al año de la pandemia, que empieza en marzo de 2020 y termina en diciembre de 2020; y un segundo cambio de nivel para el año 2021, desde enero hasta diciembre.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Predicciones para 2020\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicciones para 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Predicciones para 2022\n\n\n\n\n\n\n\n\n\n\n\n(d) Predicciones para 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Predicciones para 2024\n\n\n\n\n\n\n\n\n\n\n\n(f) Predicciones para 2025\n\n\n\n\n\n\n\nFigura 3: Intervención. Cada panel muestra las predicciones de los doce meses de un año para la serie Pernoctaciones usando el modelo Arima.\n\n\n\n\nLa Figura 3 muestra las predicciones y el intervalo de confianza al 90%. Las predicciones para 2020, realizadas sin conocimiento de la próxima pandemia, siguen el patrón observado en el pasado. Como hasta 2019 la serie era muy regular, el ajuste es muy bueno y el intervalo de confianza de las predicciones estrecho.\nLas predicciones para 2021 se realizan tras casi un año de caída en las pernoctaciones con un modelo que estima la magnitud de esta caída a partir de la variable de intervención cambio de nivel en 2020. La predicción para 2021 se corrige a la baja por la mitad de esta magnitud estimada, porque asumimos que habrá una recuperación en el turismo, pero aun así en general se subestima fuertemente la realidad. Además, se observa un intervalo de predicción mucho más ancho debido al aumento de la incertidumbre en el comportamiento de la serie. Para las predicciones del año 2022 de nuevo se aplica la mitad de la caída en las pernoctaciones estimada con la variable de intervención cambio de nivel de 2021 porque otra vez asumimos que la recuperación del turismo continua. En este caso las previsiones a veces se quedan por debajo y otras por encima de la realidad. Además, los dos años de perturbación y cambios en el patrón de la serie se reflejan en un intervalo de confianza para las predicciones aun más amplio.\nComo durante 2022 la serie ha regresado casi a la normalidad, para las predicciones de 2023 ya no se aplican ninguno de los cambios de nivel estimados. El resultado son unas predicciones mucho mejores que las obtenidas previamente aunque su amplio intervalo de confianza evidencia que todavía pesa mucho la incertidumbre observada en el pasado, con tres años consecutivos de cambios en la estructura de la serie.\nLa predicciones de los años 2024 y 2025 mejoran, aunque aun reproducen de forma imperfecta el patrón observado en la pernoctaciones los dos últimos años. Se observa que la variación estacional de las predicciones es mucho más suave que la real. Ademas, en 2025 la amplitud del intervalo de confianza es menor que el observado para las predicciones de 2024 debido a que la regularidad en la serie de los tres últimos años ha reducido la incertidumbre."
  },
  {
    "objectID": "04-08-Series_interrumpidas.html#fijar-las-observaciones-durante-la-perturbacion-como-valores-perdidos-1",
    "href": "04-08-Series_interrumpidas.html#fijar-las-observaciones-durante-la-perturbacion-como-valores-perdidos-1",
    "title": "Prediciendo series temporales interrumpidas",
    "section": "Fijar las observaciones durante la perturbacion como valores perdidos",
    "text": "Fijar las observaciones durante la perturbacion como valores perdidos\nAsignar a valores perdidos las observaciones durante el periodo de la perturbación es una forma radical de resolver el problema, pero muy sencilla de implementar. Asumiremos que el efecto de la pandemia se inició en marzo de 2020 y terminó en febrero de 2022. Las 24 observaciones de este periodo se asignarán como NA y la serie resultante se ajustará a un modelo Arima, sin necesidad de incluir intervención.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Predicciones para 2020\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicciones para 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Predicciones para 2022\n\n\n\n\n\n\n\n\n\n\n\n(d) Predicciones para 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Predicciones para 2024\n\n\n\n\n\n\n\n\n\n\n\n(f) Predicciones para 2025\n\n\n\n\n\n\n\nFigura 4: Valores perdidos. Cada panel muestra las predicciones de los doce meses de un año para la serie Pernoctaciones usando el modelo Arima.\n\n\n\n\nLa Figura 4 muestra las predicciones y el intervalo de confianza al 90% y muestra lo que hubiera pasado sin la pandemia de la Covid-19, basándose en el comportamiento de las pernoctaciones hasta 2019. Para los años 2020 y 2021 las predicciones sobrestiman tremendamente la realidad. Pero para el año 2022, cuando el efecto de la pandemia casi había pasado, las predicciones se ajustan mucho más a la serie, aunque la siguen sobrestimando ligeramente. Además, cuanto más alejado es el horizonte temporal de las predicciones, mayor es la incertidumbre y, por tanto, más amplio es el intervalo de confianza.\nFinalmente, en 2023 y años siguiente las predicciones se ajustan muy bien a la realidad."
  },
  {
    "objectID": "04-08-Series_interrumpidas.html#trabajar-bajo-el-escenario-qué-hubiera-pasado-si-1",
    "href": "04-08-Series_interrumpidas.html#trabajar-bajo-el-escenario-qué-hubiera-pasado-si-1",
    "title": "Prediciendo series temporales interrumpidas",
    "section": "Trabajar bajo el escenario qué hubiera pasado si",
    "text": "Trabajar bajo el escenario qué hubiera pasado si\nPara el escenario qué hubiera pasado si vamos a estimar un modelo Arima bajo el escenario previo (asignar a valores perdidos a las observaciones durante el periodo de la perturbación), y usarlo para estimar que podría haber pasado durante la pandemia. Después, estimamos un segundo modelo Arima con toda la nueva serie y lo usamos para obtener las predicciones.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Predicciones para 2020\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicciones para 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Predicciones para 2022\n\n\n\n\n\n\n\n\n\n\n\n(d) Predicciones para 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Predicciones para 2024\n\n\n\n\n\n\n\n\n\n\n\n(f) Predicciones para 2025\n\n\n\n\n\n\n\nFigura 5: Qué hubiera pasado si. Cada panel muestra las predicciones de los doce meses de un año para la serie Pernoctaciones usando el modelo Arima.\n\n\n\n\nBajo este escenario las predicciones resultan similares a las obtenidas bajo el supuesto previo con asignación de valores perdidos (véase Figura 5). Sin embargo, los intervalos de confianza son más estrechos. Al sustituir los valores perdidos por valores ajustados, el modelo estimado posteriormente trabaja con una serie muy regular sin incertidumbre y esto se refleja en la amplitud del intervalo de confianza."
  },
  {
    "objectID": "03-01-Tema1.html#qué-se-puede-predecir",
    "href": "03-01-Tema1.html#qué-se-puede-predecir",
    "title": "Series Temporales. Definición y componentes",
    "section": "1.1 ¿Qué se puede predecir?",
    "text": "1.1 ¿Qué se puede predecir?\nAlgunos datos son más fáciles de prever que otros. Puedes consultar múltiples páginas web con la hora en que sale el Sol en Valencia desde mañana hasta dentro de un año y todas ellas coinciden. Sin embargo, si buscas páginas con la predicción del valor de las acciones en bolsa de INDRA para los próximos días, encontrarás valores muy dispares. En medio están las múltiples páginas que predicen el consumo eléctrico en el largo plazo (dentro de varios años) que ofrecen valores similares, pero no coincidentes.\nLa calidad de una predicción depende de varios factores, entre ellos:\n\nnuestro conocimiento sobre los determinantes del acontecimiento,\nla cantidad de información pasada disponible,\nla semejanza del pasado con el futuro,\nsi las previsiones afectan a la variable que estamos prediciendo."
  },
  {
    "objectID": "03-01-Tema1.html#para-qué-predecir",
    "href": "03-01-Tema1.html#para-qué-predecir",
    "title": "Series Temporales. Definición y componentes",
    "section": "1.2 ¿Para qué predecir?",
    "text": "1.2 ¿Para qué predecir?\nAlgunas de las razones por las que empresas y organismos necesitan hacer predicciones son:\n\nTomar decisiones en el presente sobre el futuro con más información reduciendo la incertidumbre.\nRed eléctrica española predice la demanda de electricidad de los próximos días para gestionar el complejo sistema producción de electricidad y ajustar la demanda a la oferta.\nUn supermercado predice la demanda de sus productos para evitar tanto las rupturas de stock en el lineal como las pérdidas por caducidad.\nUna aerolínea predice la demanda de asientos en sus vuelos para gestionar su flota de aviones y ajustar los precios de los billetes.\nMedir el efecto de un shock en el mercado, un cambio en la regulación o una política empresarial.\n¿La Covid-19 ha producido un cambio en la decisión de las parejas de tener hijos? ¿Se ha reducido o aumentado la natalidad tras la Covid-19?\nLa nueva regulación de apartamentos turísticos en la ciudad, ¿cómo afecta la ocupación hotelera o el precio de los alquileres?\n¿En que medida un cambio en los parámetros o características de un juego de ordenador aumenta el tiempo de juego o la compra de tokens?\nEvaluar un efecto calendario.\nLa variación en el número de turistas en marzo y abril, según como caiga la Semana Santa.\nEl efecto de un febrero bisiesto sobre las ventas.\nEl efecto de la temperatura sobre una enfermedad respiratoria o el consumo de electricidad."
  },
  {
    "objectID": "03-01-Tema1.html#qué-haremos-y-qué-no-haremos-en-este-curso",
    "href": "03-01-Tema1.html#qué-haremos-y-qué-no-haremos-en-este-curso",
    "title": "Series Temporales. Definición y componentes",
    "section": "1.3 ¿Qué haremos y qué no haremos en este curso?",
    "text": "1.3 ¿Qué haremos y qué no haremos en este curso?\nEn este curso nos vamos a centrar en los métodos objetivos más fiables para obtener predicciones. Métodos que son replicables, cuyos resultados –las predicciones– son verificables y que son usados ampliamente por parte de organizaciones y empresas.\nNo podremos ver todos los métodos existentes, ¡ni aunque hubiera un grado solo para predicciones! Pero los que veremos te darán la base para aprender otros métodos de previsión: alisado exponencial con doble estacionalidad, el vecino más próximo, vectores autorregresivos, redes neuronales; y para aprender otros enfoques a la hora de hacer previsiones: modelos jerárquicos de series, boostrapping, bagging…\nNo vamos a aprender a predecir series finacieras (cotizaciones de bolsa, relaciones entre divisas o ratios financieros). La metodología para predecir estas series es completamente diferente y muy poco exitosa. Si fuera fácil, estaríamos todos invirtiendo, no dando clases (yo) o recibiéndolas (vosotros).\nPredeciremos las series individualmente, asumiendo que son independientes. Este supuesto no siempre se da, pero simplifica enormemente el análisis de una serie. Por ejemplo, predeciremos la serie de pernoctaciones en España en hoteles usando solo datos de esta serie. Sin embargo, sería de ayuda predecir de forma simultánea la serie de turistas llegados a España porque el volumen de pernoctaciones depende del número de turistas. Otra estrategia sería predecir las pernoctaciones en cada comunidad autónoma y luego sumarlas para obtener el total nacional."
  },
  {
    "objectID": "03-01-Tema1.html#definición",
    "href": "03-01-Tema1.html#definición",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.1 Definición",
    "text": "2.1 Definición\nUna serie temporal es una variable medida secuencialmente en el tiempo a intervalos equiespaciados.\nLa representaremos por,\n\\[\\{y_t\\}_{t=1}^T=\\{y_1,y_2,\\ldots,y_T\\}.\\]\nLa serie aparece indexada por su fechado \\(t\\) y el valor \\(T\\) hará siempre referencia a la fecha del último dato.\nEl fechado varía en su frecuencia, que puede ser anual (baja frecuencia), trimestral, mensual, semanal, diario (alta frecuencia) o disponer casi de un continuo de datos."
  },
  {
    "objectID": "03-01-Tema1.html#proceso-generador-de-datos",
    "href": "03-01-Tema1.html#proceso-generador-de-datos",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.2 Proceso generador de datos",
    "text": "2.2 Proceso generador de datos\nEl proceso generador de los datos de una serie temporal es en general desconocido, pero se puede aproximar por un modelo estadístico. Estos modelos se pueden clasificar en tres grandes familias según su naturaleza: deterministas, estocásticos y mixtos\nEn ocasiones las series temporales pueden ser modeladas de forma determinista ajustando los datos a funciones matemáticas: \\[y_t=f(t)+\\varepsilon_t.\\]\nSi las observaciones cercanas en el tiempo tienden a estar (cor)relacionadas, se puede aprovechar esta dependencia para entender la serie y predecirla con un modelo estocástico: \\[y_t=f(y_{t-1},y_{t-2},\\ldots)+\\varepsilon_t.\\]\nA veces, ambas situaciones se dan simultáneamente (modelos mixtos): \\[y_t=f(t,y_{t-1},y_{t-2},\\ldots)+\\varepsilon_t.\\]\nLa Figura 1 muestra un ejemplo gráfico de estos tres modelos estadísticos. La complejidad de la serie es mayor en los modelos con parte estocástica.\n\n\n\n\n\n\n\n\nFigura 1: Ejemplos de procesos generadores\n\n\n\n\n\nEn este curso se asumirá en todo momento que la serie temporal tiene una componente estocástica. Para series deterministas puedes usar los modelos de regresión que has visto en Modelos lineales."
  },
  {
    "objectID": "03-01-Tema1.html#lectura-de-datos-y-representación-gráfica",
    "href": "03-01-Tema1.html#lectura-de-datos-y-representación-gráfica",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.3 Lectura de datos y representación gráfica",
    "text": "2.3 Lectura de datos y representación gráfica\nAntes de continuar vamos a importar con R tres de las series que usaremos de ejemplo en este tema: generación anual de residuos municipales per cápita (kg per cápita), los nacimientos mensuales y la demanda eléctrica diaria (GWh). Las tres series se refieren a España y servirán de ejemplo para el análisis de series con diferente fechado: anual, mensual y diario, respectivamente.\n\nGeneración de residuos municipales per cápita\nResiduos es una serie anual de 1995 a 2023 (fuente Instituto Nacional de Estadística) que muestra los residuos recogidos por o en nombre de las autoridades municipales y eliminados a través del sistema de gestión de residuos. Los datos están disponibles en el fichero Residuos.csv, que contiene dos variables o columnas de datos. La primera columna corresponde al año de la serie y la segunda contiene los residuos recogidos en kg per cápita. En la primera fila del fichero aparece el nombre de cada columna. Los valores no tienen decimales y los datos de cada fila vienen separados por un punto y coma.\n\nresiduos &lt;- read.csv2(\"./series/Residuos.csv\", \n                      header = TRUE)\n\nresiduos &lt;- ts(residuos[, 2], \n               start = 1995, \n               frequency  = 1)\n\n\n\n\n\n\n\nOjo con la lectura de datos\n\n\n\nUsamos para leer los datos read.csv2, indicando que la primera línea tiene el nombre de las variables. Esta función asume que el separador decimal es la coma “,” y que el separador entre variables es el punto y coma “;”.\nSi el separador decimal es el punto “.” y el separador de variables es la coma “,”, debes usar read.csv.\nEn cualquiera de estas funciones puedes modificar el separador decimal por medio del argumento dec; también puedes usar el argumento sep para indicar el carácter usado como separador de variables.\n\n\nLa función ts, de la librería stats, convierte un objeto (vector o matriz) en la clase serie temporal. En este caso seleccionamos solo la segunda columna, la que contiene el número de residuos generados.\n\nCon start indicamos el fechado del primer dato.\nCon frequency indicamos la frecuencia, que en este caso es un dato por año.\n\nUsa help(ts) para obtener más información y str(residuos) para ver qué contiene un objeto serie temporal.\n\n\n\n\n\n\nNombre del fichero y nombre de la serie\n\n\n\n\n\nPor comodidad (o pereza), muchas veces se usa el mismo nombre para el fichero con los datos, el data.frame con su contenido en R y la serie temporal. No tiene por qué ser así, aunque en estos apuntes es lo más habitual.\nEn este caso, observa que primero creamos “residuos”, una base de datos (data.frame) con el contenido del fichero “Residuos.csv” que tiene dos variables (fechado y valores de la serie). Luego creamos la serie temporal “residuos” extrayendo de la base de datos la segunda columna (valores de la serie) y fechándola. Por tanto, estamos machacando la base de datos al poner a la serie el mismo nombre.\n\n\n\nPodemos dibujar la serie Residuos con la función plot o mejor con autoplot. Esta última está en el paquete forecast.\nEn general, las funciones gráficas que vamos a usar pertenecen a la librería forecast, pero en ocasiones las ampliaremos con funciones de la librería ggplot2. Te recomiendo cargar estas dos librerías desde el inicio. En casi todos los casos existe una versión de la función gráfica usada en la librería stats.\n\nlibrary(forecast)\nlibrary(ggplot2)\n\n\nautoplot(residuos,\n         xlab = \"\",\n         ylab = \"Kg per cápita\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 2: Residuos recogidos\n\n\n\n\n\n\n\nNacimientos en España\nNacimientos es una serie mensual de enero de 1975 a junio de 2025 (fuente: Instituto Nacional de Estadística). Los datos están disponibles en el fichero Nacimientos.csv. La primera columna del fichero tiene la fecha y la segunda la serie propiamente. En la primera fila aparece el nombre de cada columna. De nuevo seleccionamos solo la columna con los datos de nacimientos.\n\nnacimientos &lt;- read.csv2(\"./series/Nacimientos.csv\", \n                         header = TRUE)\n\nnacimientos &lt;- ts(nacimientos[, 2],\n                  start = c(1975, 1),\n                  frequency = 12)\n\n\n\n\n\n\n\n¿Sabrías responder a estas preguntas?\n\n\n\n¿Por qué se usa el argumento “header = TRUE”?\n¿Por qué la serie corresponde a la segunda columna de la base de datos “nacimientos”?\n\n\nEn este caso:\n\nCon start indicamos que el primer dato es enero de 1975. También sería correcto start = 1975. Si el primer dato fuera, por ejemplo, marzo de 1975, deberíamos escribir start = c(1975, 3) o start = 1975 + 2/12.\nCon frequency indicamos que se tienen 12 datos (meses) por año. Si la serie fuera trimestral pondríamos frequency = 4.\n\n\nautoplot(nacimientos,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 3: Nacimientos mensuales\n\n\n\n\n\n\n\nDemanda eléctrica\nDemanda eléctrica es una serie diaria desde el lunes 1 enero de 2024 hasta el martes 31 diciembre de 2024 (fuente: Red Electrica de España). Los datos están disponibles en el fichero Consumo electrico.csv y las unidades son GWh (Gigawatios hora). El fichero solo tiene una columna con los datos de la serie, encabezada con un nombre, y el carácter decimal es el punto.\n\nelectricidad &lt;- read.csv(\"./series/Consumo electrico.csv\", \n                         header = TRUE)\n\nelectricidad &lt;- ts(electricidad[, 1],\n                   start = c(1, 1), #Semana 1 - Día 1 de la semana (Lunes)\n                   frequency = 7)\n\nEn este caso:\n\nCon start = c(1, 1) indicamos que el primer dato (primer día) corresponde a la primera semana del año y fue un lunes (día 1 de la semana). Como ves no hay referencia al año. También sería correcto start = 1.\nCon frequency indicamos que se tienen 7 datos (días) por semana. Si la serie fuera de lunes a viernes pondríamos frequency = 5.\n\n\nautoplot(electricidad,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 4: Consumo diario de electricidad"
  },
  {
    "objectID": "03-01-Tema1.html#funciones-útiles-para-objetos-de-clase-ts",
    "href": "03-01-Tema1.html#funciones-útiles-para-objetos-de-clase-ts",
    "title": "Series Temporales. Definición y componentes",
    "section": "2.4 Funciones útiles para objetos de clase ts",
    "text": "2.4 Funciones útiles para objetos de clase ts\nOtras funciones relacionadas con los objetos de clase serie temporal que pueden ser útiles son:\n\nstart da el fechado del primer dato, y end da el fechado del último dato.\n\n\nstart(nacimientos); end(nacimientos)\n\n[1] 1975    1\n\n\n[1] 2025    6\n\nstart(electricidad); end(electricidad)\n\n[1] 1 1\n\n\n[1] 53  2\n\n\n\nfrequency da la frecuencia de los datos.\n\n\nfrequency(nacimientos)\n\n[1] 12\n\nfrequency(electricidad)\n\n[1] 7\n\n\n\ntime crea un vector con el fechado de una serie. Observa como guarda internamente R el fechado de una serie temporal.\n\n\nhead(time(nacimientos), n = 48)  #Mostramos sólo los 4 primeros años\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1975 1975.000 1975.083 1975.167 1975.250 1975.333 1975.417 1975.500 1975.583\n1976 1976.000 1976.083 1976.167 1976.250 1976.333 1976.417 1976.500 1976.583\n1977 1977.000 1977.083 1977.167 1977.250 1977.333 1977.417 1977.500 1977.583\n1978 1978.000 1978.083 1978.167 1978.250 1978.333 1978.417 1978.500 1978.583\n          Sep      Oct      Nov      Dec\n1975 1975.667 1975.750 1975.833 1975.917\n1976 1976.667 1976.750 1976.833 1976.917\n1977 1977.667 1977.750 1977.833 1977.917\n1978 1978.667 1978.750 1978.833 1978.917\n\nhead(time(electricidad), n = 28)  #Mostramos sólo las 4 primeras semanas\n\nTime Series:\nStart = c(1, 1) \nEnd = c(4, 7) \nFrequency = 7 \n [1] 1.000000 1.142857 1.285714 1.428571 1.571429 1.714286 1.857143 2.000000\n [9] 2.142857 2.285714 2.428571 2.571429 2.714286 2.857143 3.000000 3.142857\n[17] 3.285714 3.428571 3.571429 3.714286 3.857143 4.000000 4.142857 4.285714\n[25] 4.428571 4.571429 4.714286 4.857143\n\n\n\n\n\n\n\n\nFechado de la serie\n\n\n\n¿Tienes claro cómo guarda R internamente el fechado de una serie? ¿Qué indican los valores decimales?\n\n\n\ncycle, crea un vector con la posición en el ciclo de cada observación. Para una serie mensual sus valores van de 1 a 12, empezando por la posición del mes correspondiente al primer dato; para una serie diaria sus valores irían 1 a 7, empezando por la posición del día del primer dato.\n\n\nhead(cycle(nacimientos), n = 48) #Mostramos sólo los 4 primeros años\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1975   1   2   3   4   5   6   7   8   9  10  11  12\n1976   1   2   3   4   5   6   7   8   9  10  11  12\n1977   1   2   3   4   5   6   7   8   9  10  11  12\n1978   1   2   3   4   5   6   7   8   9  10  11  12\n\nhead(cycle(electricidad), n = 28) #Mostramos sólo las 4 primeras semanas\n\nTime Series:\nStart = c(1, 1) \nEnd = c(4, 7) \nFrequency = 7 \n [1] 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7"
  },
  {
    "objectID": "03-01-Tema1.html#tendencia-t_t",
    "href": "03-01-Tema1.html#tendencia-t_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.1 Tendencia, \\(T_t\\)",
    "text": "3.1 Tendencia, \\(T_t\\)\nDefinición: la tendencia de una serie es su comportamiento a largo plazo (varios años). Describe los cambios sistemáticos de la serie temporal que no aparentan ser periódicos.\nRespecto a la dirección del movimiento la tendencia puede ser:\n\nCreciente: a largo plazo la serie aumenta su valor\nDecreciente: a largo plazo la serie disminuye su valor\nEstacionaria: a largo plazo la serie mantiene su valor\n\nEn la Figura 5 se muestran ejemplos de series temporales según dirección del movimiento.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Estacionaria\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Creciente (lineal)\n\n\n\n\n\n\n\n\n\n\n\n(c) Creciente (exponencial)\n\n\n\n\n\n\n\n\n\n\n\n(d) Creciente (logarítmica)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Decreciente (lineal)\n\n\n\n\n\n\n\n\n\n\n\n(f) Decreciente (exponencial)\n\n\n\n\n\n\n\n\n\n\n\n(g) Decreciente (logarítmica)\n\n\n\n\n\n\n\nFigura 5: Ejemplos de tendencia\n\n\n\n\n\n\nSi la serie temporal es suficientemente larga es posible observar cambios en la dirección del movimiento de la tendencia que definen los ciclos."
  },
  {
    "objectID": "03-01-Tema1.html#ciclo-c_t",
    "href": "03-01-Tema1.html#ciclo-c_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.2 Ciclo, \\(C_t\\)",
    "text": "3.2 Ciclo, \\(C_t\\)\nDefinición: Son patrones sin periodicidad fija que abarcan varios años.\nPor ejemplo, los ciclos económicos, los cambios climáticos asociados al fenómeno El Niño, o las manchas solares (véase Figura 6).\n\n\n\n\n\n\nFigura 6: Ciclos solares. Imagen tomada de Courtillot, Lopes, and Mouël (2021)\n\n\n\n\n\nLa serie de Nacimientos es lo suficientemente larga como para observarse un ciclo completo, que queda identificado por dos cambios de tendencia consecutivos de signo opuesto (véase Figura 7):\n\nA finales de la década de los 90 la tendencia decreciente en los nacimientos pasa a creciente por la llegada de inmigrantes con una mayor tasa de natalidad.\nA finales de la primera década del 2000 la tendencia creciente pasa a decreciente porque la Gran Recesión provoca el regreso a sus países de origen de muchos de estos inmigrantes.\n\nDe esta forma, observamos un ciclo completo desde 1975 hasta poco antes de 2010 (periodo entre dos cambios de tendencia), y el inicio del siguiente ciclo, en el que aun estamos.\n\n\n\n\n\n\n\n\nFigura 7: Ciclos en la serie Nacimientos"
  },
  {
    "objectID": "03-01-Tema1.html#estacionalidad-s_t",
    "href": "03-01-Tema1.html#estacionalidad-s_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.3 Estacionalidad, \\(S_t\\)",
    "text": "3.3 Estacionalidad, \\(S_t\\)\nDefinición: Son patrones repetitivos de periodicidad fija e inferior al año.\nEl orden de la periodicidad lo denominaremos \\(m\\), por tanto el patrón estacional se repite cada \\(m\\) periodos. Lógicamente, \\(m\\) toma el valor 12 para datos mensuales, el valor 4 para datos trimestrales, 7 para datos diarios de lunes a domingo, etc.\nLa componente estacional surge por factores climatológicos, institucionales o sociales.\nEn ocasiones no es fácil determinar la existencia de estacionalidad o su orden. En este caso, se puede usar el análisis espectral, que no veremos en este curso, para analizar esta componente. La librería forecast dispone de la función findfrequency que devuelve la frecuencia dominante de una serie usando el análisis espectral.\nLa serie Nacimientos tiene una estacionalidad de orden 12, causada principalmente por el número de días del mes (véase la Figura 8). Cada año se repite una misma pauta: los meses de febrero, por tener 28 (o 29 días en años bisiestos), presentan el menor número de nacimientos y son los valles en la Figura 8; y, en general, en los meses de 31 días hay más nacimientos que en los demás meses del año.\n\n\n\n\n\n\n\n\nFigura 8: Nacimientos\n\n\n\n\n\n\n\nLa Figura 9 muestra la demanda eléctrica diariamente para cuatro semanas, desde el lunes 5 de febrero hasta el domingo 3 de marzo (semanas 6 a 9 del año). Tiene, por tanto, una estacionalidad de orden 7. En el eje OX aparecen etiquetados los lunes de cada semana que permiten identificar el siguiente patrón semanal: de lunes a viernes un consumo similar, seguido de una caída en el consumo el sábado, y el domingo como el día de menor consumo\n\n\n\n\n\n\n\n\nFigura 9: Demanda diaria de electricidad\n\n\n\n\n\nSi el fechado de la serie es de muy alta frecuencia, puede ocurrir que se superponga más de una componente estacional. La Figura 10 muestra la serie corresponde a la demanda eléctrica (GW) recogida cada hora durante el mes de febrero de 2021. El eje OX señala el consumo de la primera hora de cada día del mes (desde media noche hasta la una de la madrugada). Se aprecia una estacionalidad diaria de orden \\(24\\), otra semanal de orden \\(168 =7 \\times 24\\) y si mostráramos varios años de consumo, también se observaría otra mensual.\n\n\n\n\n\n\n\n\nFigura 10: Estacionalidad múltiple para el consumo de electricidad por hora\n\n\n\n\n\n\n\n\n\n\n\nSeries con más de una componente estacional\n\n\n\nEn este curso no analizaremos series con más de una componente estacional. Si quieres tener una primera aproximación de como se hace, puedes ir a la píldora Múltiples componentes estacionales."
  },
  {
    "objectID": "03-01-Tema1.html#intervención-i_t",
    "href": "03-01-Tema1.html#intervención-i_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.4 Intervención, \\(I_t\\)",
    "text": "3.4 Intervención, \\(I_t\\)\nDefinición: Es un factor sistemático no periódico, o irregular, que vendría determinado por fenómenos ocasionales que provocan observaciones anómalas y valores atípicos en la serie temporal.\nPor su relación con fechas concretas, podemos distinguir dos tipos:\n\nEfectos calendario: irregularidades específicas en la serie temporal que se producen durante determinados periodos de tiempo. Por ejemplos los días festivos en series diarias; días en que un producto está de oferta en un supermercado; la Semana Santa, número de días laborales y febrero bisiesto en series mensuales.\nOtros efectos no sujetos a calendario: irregularidades no específicas ni sujetas a fechas concretas. Por ejemplo, pandemias, catástrofes naturales, huelgas, caída del sistema eléctrico o de los servidores de una red social, etc.\n\nEn la serie mensual Nacimientos, los meses de febrero bisiestos (puntos rojos) presentan un número de nacimientos mayor que los meses de febrero no bisiestos (efecto calendario). Para algunos años este hecho es mas claro (véase Figura 11). Además, a finales del año 2020 y principios del 2021 se observa una caída inusual en el número de nacimientos (puntos verdes) debido a que la pandemia causada por la Covid-19 retrasó la decisión de tener hijos de muchas parejas. Este es un ejemplo de intervención no sujeta a calendario.\n\n\n\n\n\n\n\n\nFigura 11: Efecto de los febreros bisiestos y de la Covid-19 en Nacimientos\n\n\n\n\n\nEn la serie diaria Electricidad cuando un día entre semana es festivo, el consumo se reduce notablemente, apareciendo un efecto calendario. Si se trata de un día aislado, el efecto es muy fácil de identificar y analizar. Un ejemplo, observable en la Figura 12, es el viernes 1 de noviembre de 2024, día de Todos los Santos, identificado con un punto rojo en la figura. Se aprecia que el consumo fue muy inferior al observado el miércoles precedente y posterior (puntos verdes). Si los festivos abarcan varios días, el efecto sigue siendo perfectamente identificable, pero es más complejo de analizar. Un ejemplo es la última semana de diciembre donde el consumo es muy inferior al observado en las semanas previas debido, por un lado, al efecto del día festivo de Navidad (punto azul) y, por otro lado, al periodo semi-vacacional que estas festividades supone en España. También se aprecia la intervención asociada al viernes 6 de diciembre, día de la Constitución. Sin embargo, el efecto del día festivo de la Inmaculada (8 de diciembre) no es apreciable por caer en domingo.\n\n\n\n\n\n\n\n\nFigura 12: Efectos calendario en Electricidad\n\n\n\n\n\nPor su naturaleza, podemos distinguir tres tipos básicos de intervención (aunque hay más):\n\nPulso (Additive Outlier, AO)\n\nEn un periodo aislado la serie toma un valor anómalo (véase Figura 13, panel izquierdo). Por ejemplo, un día entre semana es festivo y la demanda eléctrica es inferior a la usual; o el mes de agosto de 2024 la Olimpiadas de París incrementan significativamente el turismo y la ocupación hotelera en la ciudad respecto de agostos precedentes.\n\nCambio transitorio (Transitory Change, TC)\n\nEn un periodo un shock genera un valor anómalo en la serie y el efecto del shock va desapareciendo poco a poco (véase Figura 13, panel central). Por ejemplo, las redes sociales ponen de moda un producto que temporalmente aumenta sus ventas. Pero, conforme pasa el tiempo, los consumidores se olvidan del producto y sus ventas vuelven poco a poco a su nivel previo.\n\nCambio permanente (Level Shift, LS)\n\nEn un periodo la serie cambia de nivel y permanece de forma permanente en este nuevo nivel (véase Figura 13, panel derecho). Por ejemplo, enfrente de un supermercado abre la competencia, de forma que sus ventas descienden bruscamente de forma permanente. La apertura en una ciudad de una nueva línea de tranvía incrementa de forma permanente los usuarios del transporte público en esa ciudad.\n\n\n\n\n\n\n\n\n\n\nFigura 13: Tipos de intervención"
  },
  {
    "objectID": "03-01-Tema1.html#residuo-r_t",
    "href": "03-01-Tema1.html#residuo-r_t",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.5 Residuo, \\(R_t\\)",
    "text": "3.5 Residuo, \\(R_t\\)\nDefinición: No presenta un comportamiento sistemático a corto, medio o largo plazo por lo que no se puede predecir de modo alguno. Es la parte de la serie que se debe a puro azar.\nAunque inicialmente no se hará ningún supuesto sobre el residuo, se espera que sea ruido blanco: media cero, incorrelado y homocedástico. Es decir \\(R_t \\sim iid(0, \\sigma^2\\))."
  },
  {
    "objectID": "03-01-Tema1.html#esquema-aditivo-y-multiplicativo",
    "href": "03-01-Tema1.html#esquema-aditivo-y-multiplicativo",
    "title": "Series Temporales. Definición y componentes",
    "section": "3.6 Esquema aditivo y multiplicativo",
    "text": "3.6 Esquema aditivo y multiplicativo\nUna serie temporal siempre tiene tendencia y residuo. La presencia de estacionalidad, ciclo e intervención depende de la naturaleza de la serie. Por ejemplo, una serie anual no tendrá nunca estacionalidad y en una serie corta no se podrá observar el ciclo.\nLas componentes de una serie temporal se pueden combinar de múltiples formas.\nEn el esquema aditivo cada componente suma su efecto sobre las demás, \\[y_t = T_t + S_t + C_t + I_t + R_t.\\] La demanda diaria de electricidad es un ejemplo de este tipo de esquema (Figura 14). El panel superior muestra la serie en el tiempo e identificamos el esquema aditivo porque la amplitud estacional (para cada semana la diferencia entre el día de más consumo y el de menos consumo) se mantiene constante en el tiempo. En el panel inferior tenemos una gráfica media-varianza, donde cada punto corresponde a una semana, la coordenada X es el consumo de esa semana y la coordenada Y la desviación típica del consumo para los días de esa semana (variabilidad intrasemanal). Este segundo panel revela el esquema aditivo de la serie porque no se observa un patrón creciente en los puntos: más consumo no implica una mayor desviación típica.\n\n\n\n\n\n\n\n\n\n\n\n(a) Consumo eléctrico diario\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Gráfica media-varianza\n\n\n\n\n\n\n\nFigura 14: Ejemplo de esquema aditivo\n\n\n\nEn el esquema multiplicativo cada componente supone un incremento porcentual respecto de las demás, \\[y_t = T_t \\cdot S_t \\cdot C_t \\cdot I_t \\cdot R_t.\\] La serie Nacimientos es un ejemplo de esquema multiplicativo. En el panel superior de la Figura 15, que muestra la serie, se observa que según decrece el número de nacimientos, también decrece la amplitud estacional. Además, en el panel inferior cada punto corresponde a un año: en el eje X los nacimientos de ese año y en el eje Y la desviación típica de los nacimientos mensuales de ese año (variación intraanual). En este panel se observa un patrón creciente en los puntos, revelando el esquema multiplicativo de la serie.\n\n\n\n\n\n\n\n\n\n\n\n(a) Nacimientos mensuales\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Gráfica media-desviación anual\n\n\n\n\n\n\n\nFigura 15: Ejemplo de esquema multiplicativo\n\n\n\nSi una serie presenta un esquema multiplicativo, su logaritmo lo presentará aditivo. A lo largo del curso se verán otras razones por las que puede ser aconsejable analizar el logaritmo de una serie temporal.\nEn principio, cualquier combinación entre las componentes es posible:\n\n\\(y_t = (T_t + C_t) \\cdot S_t + I_t + R_t\\)\n\\(y_t = (T_t + S_t + C_t + I_t)R_t\\)\n\\(y_t = T_t \\cdot S_t \\cdot C_t \\cdot I_t + R_t\\)\n…"
  },
  {
    "objectID": "03-01-Tema1.html#idea-general",
    "href": "03-01-Tema1.html#idea-general",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.1 Idea general",
    "text": "4.1 Idea general\nPodemos manipular una serie temporal con diferentes fines:\n\nextraer la tendencia (eliminando la estacionalidad), por ejemplo, pasando de una serie mensual a una anual, o de una serie diaria a una semanal.\nextraer la estacionalidad (eliminando la tendencia) de forma sencilla, aunque no muy precisa.\nrecortar una serie para obtener una submuestra.\nextraer una subserie correspondiente a un único periodo estacional. Por ejemplo, los nacimientos en febrero o el consumo de electricidad de los domingos.\n\nEstas operaciones nos permitirán mejorar nuestra capacidad descriptiva de la serie, identificar mejor el tipo de esquema entre las componentes, facilitar la estimación del proceso generador o ampliar las herramientas de análisis y predicción de una serie temporal."
  },
  {
    "objectID": "03-01-Tema1.html#extracción-de-la-tendencia-método-sencillo",
    "href": "03-01-Tema1.html#extracción-de-la-tendencia-método-sencillo",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.2 Extracción de la tendencia (método sencillo)",
    "text": "4.2 Extracción de la tendencia (método sencillo)\nSi tenemos una serie con estacionalidad y agregamos la serie –obteniendo un dato por año, si la serie es mensual, o un dato por semana, si es diaria– obtenemos una nueva serie sin estacionalidad, solo con tendencia.\nDependiendo de la naturaleza de la serie, convendrá agregar sumando los datos (consumo eléctrico, residuos generados, viajeros transportados, nacimientos) o sacando la media (temperatura, número de parados, ocupación hotelera).\nVeamos como extraer la tendencia de la serie Nacimientos usando la función aggregate con el argumento FUN = sum.\n\nnacimientosAnual &lt;- aggregate(nacimientos, FUN = sum)\nautoplot(nacimientosAnual/1000,\n         xlab = \"\",\n         ylab = \"Nacimientos (miles)\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 16: Nacimientos anuales\n\n\n\n\n\nAhora para la demanda de electricidad.\n\nelectricidadSemanal &lt;- aggregate(electricidad, FUN = sum)\nautoplot(electricidadSemanal,\n       xlab = \"\",\n       ylab = \"GWh\",\n       main = \"\")\n\n\n\n\n\n\n\nFigura 17: Consumo de electricidad por semana\n\n\n\n\n\nLa función aggregate aplicada a una serie temporal agrega los datos de cada periodo estacional completo aplicando la función especificada en FUN.\n\nUna serie trimestral o mensual la transforma en anual, una serie diaria en semanal.\nLa función a usar dependerá de la naturaleza de los datos y del objetivo perseguido (FUN=sum, FUN=mean, FUN=sd…)\nLa función aggregate tiene un uso más amplio en R. Usa la función help para aprenderlo.\n\n\n\n\n\n\n\nAtención\n\n\n\nLa agregación no tiene por qué corresponderse con un fechado natural. Por ejemplo, en 2023 el primer día del año fue el domingo 1 de enero. Si tenemos una serie es diaria para ese año, el primer dato de la serie agregada irá del domingo 1 de enero al sábado 7 de enero, un periodo estacional completo, pero no una semana natural; el segundo dato de la serie agregada ira del domingo 8 al sábado 14 de enero; y así sucesivamente. El último dato de la serie agregada será del domingo 24 al sábado 30 de diciembre. El domingo 31 de diciembre no formará parte de la serie agregada."
  },
  {
    "objectID": "03-01-Tema1.html#visualización-del-patrón-estacional-y-extracción-de-la-componente-estacionalidad-método-sencillo",
    "href": "03-01-Tema1.html#visualización-del-patrón-estacional-y-extracción-de-la-componente-estacionalidad-método-sencillo",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.3 Visualización del patrón estacional y extracción de la componente estacionalidad (método sencillo)",
    "text": "4.3 Visualización del patrón estacional y extracción de la componente estacionalidad (método sencillo)\nTenemos alternativas gráficas para visualizar el patrón estacional de una serie y métodos numéricas para calcular la componente estacional.\nVisualización del patrón estacionalidad\nPodemos hacer un gráfico de la serie contra cada periodo estacional para identificar el patrón estacional de la serie y su evolución en el tiempo. Existen varias opciones para este tipo de gráficos, veremos dos de ellas: gráfico de subseries y gráfico de líneas.\nVeamos primero un ejemplo de gráfico de subseries, Figura 18, que muestra para cada periodo estacional la subserie de valores de ese periodo (líneas negras) y el valor medio de la subserie (líneas azules). La función utilizada, ggsubseriesplot, pertenece al paquete forecast, pero monthplot de stats realiza un gráfico similar.\nPara la serie Nacimientos, recortada desde el año 2000 para facilitar la interpretación, la Figura 18 muestra como el mes de febrero es el que presenta menor número de nacimientos. Por contra, los meses de 31 días presentan en media un número de nacimientos mayor que los meses de 30 días. La excepción es septiembre, un mes de 30 días pero donde el número de nacimientos supera el del mes de agosto de 31 días. Una explicación posible es que muchos nacimientos por cesárea que corresponderían a finales de agosto se programen en septiembre para esquivar este mes festivo.\n\nnacimientosb &lt;- window(nacimientos, start = 2000)\n\nggsubseriesplot(nacimientosb) +\n  labs(x = \"\", y = \"Nacimientos\", title = \"\") \n\n\n\n\n\n\n\nFigura 18: Gráfico estacional de subseries para Nacimientos\n\n\n\n\n\nPara las series con tendencia o esquema multiplicativo, el valor medio de las subseries puede llevarnos a una interpretación incorrecta de la estacionalidad.\nUna alternativa es el gráfico de líneas, donde cada linea muestra la evolución de un periodo estacional completo. La Figura 19 muestra los nacimientos desde 2018 a 2024, donde cada año es una línea. Se observa perfectamente el efecto de la pandemia sobre el número de nacimientos desde noviembre de 2020. Si se incluye el argumento polar=TRUE, se obtiene una versión tipo tela de araña de este gráfico.\n\nggseasonplot(window(nacimientos, start = 2018, end = c(2024, 12)),\n             year.labels = TRUE) +\n  labs(x = \"\", y = \"Nacimientos\", title = \"\") \n\n\n\n\n\n\n\nFigura 19: Gráfico estacional de lineas para Nacimientos\n\n\n\n\n\nCálculo de la componente estacionalidad\nLas gráficas ayudan a describir y entender un poco mejor el patrón estacional. Sin embargo, si deseamos estimar la componente estacional, debemos proceder de otra forma.\nLa siguiente sintaxis usa la función tapply para estimar numéricamente la componente estacional bajo un esquema multiplicativo. Básicamente, calcula para cada mes (argumento cycle(nacimientosb)) la media (FUN = mean) del cociente nacimientosb / mean(nacimientosb).\n\ncomponenteEstacionalNac &lt;- tapply(nacimientosb/mean(nacimientosb), \n                                  cycle(nacimientosb), \n                                  FUN = mean)\nround(componenteEstacionalNac, 2)\n\n   1    2    3    4    5    6    7    8    9   10   11   12 \n1.00 0.91 0.99 0.96 1.00 0.97 1.04 1.03 1.04 1.06 1.00 1.00 \n\n\nLos valores de la tabla previa indican que en febrero hay un 9% menos de nacimientos respecto de la media anual y que, en general, los meses de 30 días están en la media o debajo de la media general. Por contra, los meses de 31 días están en o por encima de la media general, destacando octubre que muestra un 6% más de nacimientos respecto de la media anual. La anomalía es septiembre que, siendo un mes de 30 días, tiene un 4% más de nacimientos que la media anual.\nLos cambios necesarios para estimar la componente estacional bajo un esquema aditivo son mínimos. Veámoslo para la serie Demanda eléctrica.\n\ncomponenteEstacionalElec &lt;- tapply(electricidad - mean(electricidad), \n                                   cycle(electricidad), \n                                   FUN = mean)\nround(componenteEstacionalElec, 2)\n\n     1      2      3      4      5      6      7 \n 13.49  30.91  31.73  32.47  23.14 -47.25 -85.34 \n\n\nDe lunes a viernes la demanda eléctrica está por encima de la media semanal, especialmente de martes a jueves con un consumo más de 30 GWh por encima de la media. El sábado la demanda cae 47 GWh y el domingo se da la menor demanda, 85 GWh por debajo de la media semanal."
  },
  {
    "objectID": "03-01-Tema1.html#recorte-de-una-serie-y-extracción-de-una-subserie",
    "href": "03-01-Tema1.html#recorte-de-una-serie-y-extracción-de-una-subserie",
    "title": "Series Temporales. Definición y componentes",
    "section": "4.4 Recorte de una serie y extracción de una subserie",
    "text": "4.4 Recorte de una serie y extracción de una subserie\nR proporciona varias funciones que permiten extraer una submuestra de la serie original. Podemos:\n\nseleccionar una submuestra especificando los puntos temporales de inicio y fin.\nseleccionar una submuestra seleccionando un periodo estacional determinado.\nquitar fácilmente un conjunto de datos usando índices.\n\nVeamos algunos ejemplos de extracción con las funciones window y subset para la serie Nacimientos.\nFunción window, que recorta especificando fechados exactos.\n\nwindow(nacimientos, start = c(2000, 1), end = c(2009, 12)) selecciona de la serie original los datos desde enero de 2000 a diciembre de 2009.\nwindow(nacimientos, start = c(2010, 3)) selecciona de la serie original los datos desde marzo de 2010 hasta el último dato (junio de 2025).\nwindow(nacimientos, end = c(1999, 12)) selecciona de la serie original los datos desde el primero (enero de 1975) hasta diciembre de 1999.\nwindow(nacimientos, start = c(2000, 3), freq = TRUE) selecciona de la serie original solo los meses de marzo desde 2000.\n\nFunción subset, que recorta especificando posiciones de las observaciones.\n\nsubset(nacimientos, start = 10, end = 34) selecciona de la serie las observaciones que van desde la 10 a la 34, ambas inclusive.\nsubset(nacimientos, start = 121) selecciona de la serie las observaciones que van desde la 121 hasta la última.\nsubset(nacimientos, start = length(nacimientos) - 47) selecciona de la serie los últimos 48 meses (4 años), es decir, desde julio de 2021 a junio de 2025 (recuerda que la serie nacimientos termina en junio de 2025).\nsubset(nacimientos, end = length(nacimientos) - 48) selecciona de la serie todo menos los últimos 48 meses Es decir, el último dato es junio de 2021.\nsubset(nacimientos, season  = 5) selecciona de la serie todos los meses de mayo.\n\n\n\n\n\n\n\nwindow y subset\n\n\n\nDurante el curso haremos un uso constante de la funciones window y subset. Practícalas para familiarizarte con su uso.\n\n\nAdemás, puedes usar las funciones head y tail para extraer las primeras o las últimas observaciones."
  },
  {
    "objectID": "03-01-Tema1.html#concepto",
    "href": "03-01-Tema1.html#concepto",
    "title": "Series Temporales. Definición y componentes",
    "section": "5.1 Concepto",
    "text": "5.1 Concepto\nLos métodos que hemos visto para la descripción de la tendencia y la componente estacional son muy sencillos, pero no son ni rigurosos ni precisos. En este epígrafe veremos métodos más adecuados para extraer de una serie sus componentes.\nSi la serie es demasiado corta para poder extraer el ciclo, entonces el ciclo queda recogido dentro de la tendencia. Por otro lado, las técnicas de identificación de la intervención son complejas por lo que esta componente queda incorporada al residuo. Por tanto, en este epígrafe, asumiremos que una serie tiene sólo Tendencia, Estacionalidad y Residuo:\n\nEsquema aditivo \\(y_t = T_t + C_t + S_t + I_t + R_t = (T_t + C_t) + S_t +(I_t + R_t) = T'_t + S_t + R'_t\\)\nEsquema multiplicativo \\(y_t = T_t \\cdot C_t \\cdot S_t \\cdot I_t \\cdot R_t = (T_t \\cdot C_t) \\cdot S_t \\cdot (I_t \\cdot R_t) = T'_t \\cdot S_t \\cdot R'_t\\)\n\nVeremos a continuación como extraer estas tres componentes a partir de una serie original. Este proceso se denomina descomposición.\nHay múltiples formas de realizar una descomposición. Aquí veremos dos de ellas, la más sencilla, basada en el concepto de medias móviles (decompose), y otra más versátil y compleja a partir de regresiones locales ponderadas (stl).\nAdemás, R proporciona (a través de paquetes específicos) el método de descomposición que utiliza el US Census Bureau and Statistics Canada, denominado X11, y el método que utiliza el Banco de España, denominado SEATS (Seasonal Extraction in ARIMA Time Series), aunque estos métodos solo son válidos para series mensuales y trimestrales.\nEn origen, los métodos de descomposición no sirven para realizar predicciones, pero actualmente se usan también con este fin (véase las funciones stlm y stlf del paquete forecast)."
  },
  {
    "objectID": "03-01-Tema1.html#descomposición-por-medias-móviles",
    "href": "03-01-Tema1.html#descomposición-por-medias-móviles",
    "title": "Series Temporales. Definición y componentes",
    "section": "5.2 Descomposición por medias móviles",
    "text": "5.2 Descomposición por medias móviles\n\nIdeas generales\nLa función decompose estima las componentes de tendencia, estacionalidad y residuo usando el método de medias móviles. En concreto decompose sigue los siguientes pasos para obtener cada componente:\nPaso 1: Se estima la tendencia de una serie a partir de una media móvil centrada. Si el orden estacional es par, la media móvil es ponderada de orden \\(m + 1\\); y si el orden estacional es impar, la media móvil es de orden \\(m\\). En concreto,\n\nSi \\(m=2k\\): \\(\\hat{T}_t = \\frac{\\frac{1}{2}y_{t-k} + y_{t-k+1} + \\ldots + y_t + \\ldots + y_{t+k-1} + \\frac{1}{2} y_{t+k}}{m}\\),\nSi \\(m=2k+1\\): \\(\\hat{T}_t = \\frac{y_{t-k} + y_{t-k+1} + \\ldots + y_t + \\ldots + y_{t+k-1} + y_{t+k}}{m}\\).\n\nPaso 2: Para un modelo con esquema aditivo calculamos la serie sin tendencia como \\(y_t - \\hat{T}_t\\) y para un esquema multiplicativo como \\(y_t/ \\hat{T}_t\\).\nPaso 3: Para estimar la componente estacional, calculamos el valor medio de la serie sin tendencia (paso 2) de forma independiente para los datos de cada periodo estacional. Así, obtenemos un vector con la estimación de las \\(m\\) componentes estacionales.\nDespués estos valores se ajustan para que sumen 0, si el esquema es aditivo; o para que sumen \\(m\\), si el esquema multiplicativo. La componente estacional se obtiene repitiendo el vector de \\(m\\) componentes ajustadas hasta alcanzar la longitud de la serie original. Esto da \\(\\hat{S}_t\\)\nPaso 4: El residuo se obtiene como \\(\\hat{R}_t = y_t - \\hat{T}_t - \\hat{S}_t\\) en el esquema aditivo, o \\(\\hat{R}_t = y_t / (\\hat{T}_t \\cdot \\hat{S}_t)\\) en el esquema multiplicativo.\nLa Tabla 1 muestra un ejemplo de descomposición aditiva por medias móviles para una serie simulada de orden estacional 5.\n\nLa dos primeras columnas indican la estación de cada dato y el valor de la serie, para un total de 25 datos. La columna Ten ha sido obtenida siguiendo el paso 1 como una media móvil de orden 5: \\[Ten_t = (Serie_{t-2} + Serie_{t-1} + Serie_{t} + Serie_{t+1} + Serie_{t+2})/5.\\]\nLa serie sin tendencia, columna Est + Res, se obtiene restando a la columna Serie la columna Ten, tal y como se indica en el paso 2.\nPara el cálculo de la columna Est, que repite de forma periódica la primera estimación de las 5 componentes estacionales, se sigue el paso 3. Para cada estación se promedian los valores de la columna Est + Res correspondientes a dicha estación.\nPuedes comprobar que la suma de los cinco valores de la componente estacional obtenidos en la columna Est vale 1.1. Para ajustar la componente estacional para que sume 0, a cada valor de la componente estacional se le resta su suma actual de 1.1 dividida por 5, el número de estaciones. El resultado de este ajuste aparece en la columna Est corregida que será la componente estacional final.\nSiguiendo el paso 4, la columna Res se calcula restando a la serie original (columna Serie) la tendencia y la estacionalidad (columnas Ten y Est corregida).\n\nObserva que en el proceso de descomposición se han perdido 4 datos para la tendencia y el residuo, dos al inicio de la serie y dos al final.\n\n\n\n\nTabla 1: Ejemplo de descomposición por medias móviles\n\n\n\n\n\n\nEstacion\nSerie\nTen\nEst + Res\nEst\nEst corregida\nRes\n\n\n\n\n1\n17.00\nNA\nNA\n9.14\n8.92\nNA\n\n\n2\n6.72\nNA\nNA\n-10.89\n-11.11\nNA\n\n\n3\n5.08\n20.62\n-15.54\n-10.06\n-10.28\n-5.48\n\n\n4\n8.79\n27.89\n-19.10\n-9.46\n-9.68\n-9.64\n\n\n5\n65.53\n28.00\n37.53\n22.37\n22.15\n15.16\n\n\n1\n53.31\n28.58\n24.73\n9.14\n8.92\n15.59\n\n\n2\n7.28\n29.79\n-22.51\n-10.89\n-11.11\n-11.62\n\n\n3\n8.00\n25.62\n-17.62\n-10.06\n-10.28\n-7.56\n\n\n4\n14.84\n19.13\n-4.29\n-9.46\n-9.68\n5.17\n\n\n5\n44.67\n20.08\n24.59\n22.37\n22.15\n2.22\n\n\n1\n20.85\n21.78\n-0.93\n9.14\n8.92\n-10.07\n\n\n2\n12.02\n21.64\n-9.62\n-10.89\n-11.11\n1.27\n\n\n3\n16.51\n18.97\n-2.46\n-10.06\n-10.28\n7.60\n\n\n4\n14.14\n22.07\n-7.93\n-9.46\n-9.68\n1.53\n\n\n5\n31.31\n23.08\n8.23\n22.37\n22.15\n-14.14\n\n\n1\n36.37\n24.68\n11.69\n9.14\n8.92\n2.55\n\n\n2\n17.06\n26.66\n-9.60\n-10.89\n-11.11\n1.29\n\n\n3\n24.53\n30.92\n-6.39\n-10.06\n-10.28\n3.67\n\n\n4\n24.02\n30.56\n-6.54\n-9.46\n-9.68\n2.92\n\n\n5\n52.62\n33.48\n19.14\n22.37\n22.15\n-3.23\n\n\n1\n34.59\n33.51\n1.08\n9.14\n8.92\n-8.06\n\n\n2\n31.66\n33.51\n-1.85\n-10.89\n-11.11\n9.04\n\n\n3\n24.65\n32.95\n-8.30\n-10.06\n-10.28\n1.76\n\n\n4\n24.01\nNA\nNA\n-9.46\n-9.68\nNA\n\n\n5\n49.86\nNA\nNA\n22.37\n22.15\nNA\n\n\n\n\n\n\n\n\n\n\nLos principales inconvenientes de este método de descomposición son: i) se perderán datos al inicio y final de la serie –por ejemplo, si la serie es mensual se perderán seis datos al inicio y seis al final; ii) asume que la componente estacional no ha variado en el tiempo, cuando sabemos que para muchas series sociales y de consumo la componente estacional se ha suavizando con el tiempo; y iii) el cálculo de las componentes de tendencia y estacionalidad puede verse muy afectado por la presencia de datos anómalos.\nPor el contrario, una de las ventajas de este método, además de su sencillez de cálculo, es que se puede usar tanto para esquemas aditivos (type=\"addi\") como multiplicativos (type=\"multi\").\nLa función decompose genera un objeto con las siguientes componentes:\n\n$x para la serie original,\n$trend para la tendencia,\n$seasonal para la estacionalidad,\n$random para el residuo, y\n$figure que contiene las estimaciones de los m efectos estacionales ajustados. Es una extracción para un único año o semana de $seasonal.\n\nSiempre que generes nuevos objetos en R a partir de funciones te recomiendo que con names y str mires qué hay en su interior.\nEn los métodos de descomposición que vamos a ver, para extraer las componentes individualmente puedes usar la función trendcycle para la tendencia, seasonal para la componente estacional y remainder para el residuo.\n\n\nEjemplo de esquema aditivo\nVamos a descomponer la serie Demanda eléctrica asumiendo un esquema aditivo (type = \"addi).\n\neleDesAdi &lt;- decompose(electricidad, \n                       type = \"addi\")\n\nautoplot(eleDesAdi,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 20: Descomposición aditiva de la Demanda eléctrica por medias móviles\n\n\n\n\n\nLa Figura 20 muestra la serie original (panel superior) y sus tres componentes: tendencia (segundo panel), estacionalidad (tercer panel) y el residuo (panel inferior). Por tratarse de un esquema aditivo, la estacionalidad y el residuo se mueven en torno al valor de 0.\nSolo con propósitos didácticos, vamos a verificar que si se suma para cada fecha la tendencia, la estacionalidad y el residuo se obtiene exactamente el valor de la serie:\n\ntmp &lt;- trendcycle(eleDesAdi) + seasonal(eleDesAdi) + remainder(eleDesAdi)\nsummary(electricidad - tmp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n      0       0       0       0       0       0       6 \n\n\nTambién podemos ver la componente estacional y verificar que suma 0:\n\ncomponenteEstacionalElecDes &lt;- seasonal(eleDesAdi)[1:7]\ncomponenteEstacionalElecDes\n\n[1]  15.89627  31.65288  31.18041  32.45959  22.75266 -47.54283 -86.39897\n\nsum(componenteEstacionalElecDes)\n\n[1] 7.105427e-15\n\n\nSi lo preferimos, podemos mostrar la componente estacional gráficamente en lugar de numéricamente (véase la Figura 21).\n\nggplot() +\n  geom_line(aes(x = 1:7, y = componenteEstacionalElecDes)) + \n  geom_hline(yintercept = 0, colour = \"blue\", lty = 2) +\n  labs(x = \"\", y = \"GWh\", title = \"\") +\n  scale_x_continuous(breaks= 1:7, \n                     labels = c(\"Lunes\", \"Martes\", \"Miércoles\", \"Jueves\", \n                                \"Viernes\", \"Sábado\", \"Domingo\")) \n\n\n\n\n\n\n\nFigura 21: Componente estacional de Electricidad (esquema aditivo)\n\n\n\n\n\n\n\nEjemplo de Esquema Multiplicativo\nVeamos ahora la descomposición de Nacimientos bajo un esquema multiplicativo (type = \"mult\").\n\nnacDesMul &lt;- decompose(nacimientos, \n                       type = \"mult\")\n\nautoplot(nacDesMul,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 22: Descomposición multiplicativa de Nacimientos por medias móviles\n\n\n\n\n\nObserva que por tratarse de un esquema multiplicativo en la Figura 22 la componente estacional se mueve alrededor del valor 1 y debe interpretarse como una variación porcentual. Igualmente, el residuo también gira en torno al valor 1.\nLos valores de la componente estacional se deben interpretar como variaciones porcentuales: en febrero hay un 9.1% menos de nacimientos y en octubre un 3.7% más, respecto de la media anual. Además, comprobamos que la suma de los valores de la componente estacional es 12.\n\ncomponenteEstacionalNacDes &lt;- seasonal(nacDesMul)[1:12]\ncomponenteEstacionalNacDes\n\n [1] 0.9963429 0.9090738 1.0055191 0.9794254 1.0286681 0.9818254 1.0368778\n [8] 1.0204358 1.0319982 1.0373876 0.9815158 0.9909301\n\nsum(componenteEstacionalNacDes)\n\n[1] 12"
  },
  {
    "objectID": "03-01-Tema1.html#descomposición-por-regresiones-locales-ponderadas",
    "href": "03-01-Tema1.html#descomposición-por-regresiones-locales-ponderadas",
    "title": "Series Temporales. Definición y componentes",
    "section": "5.3 Descomposición por regresiones locales ponderadas",
    "text": "5.3 Descomposición por regresiones locales ponderadas\n\nIdeas generales\nLa función stl estima las componentes de tendencia y estacionalidad a partir de regresiones locales ponderadas (técnica conocida como loess)\nSus ventajas son:\n\nNo se perderán datos al inicio o al final de la serie.\nAsume que tanto la tendencia como la estacionalidad pueden cambiar con el tiempo y posibilita controlar este cambio a partir de parámetros.\nEs bastante robusta frente a valores atípicos.\n\nSu principal desventaja es que esta técnica de descomposición solo es válida para esquemas aditivos. Es posible obtener con stl una descomposición multiplicativa descomponiendo primero el logaritmo de la serie, para después calcular la exponencial de las componentes.\nLa función stl genera un objeto con la componente $time.series que contiene en columna tres series temporales: seasonal, trend y remainder (de nuevo usa names y str para aprender más).\nLos dos parámetros principales que debes elegir cuando utilices stl son la ventana de tendencia (t.window) y la ventana estacional (s.window). Estos parámetros controlan la rapidez con la que las componentes de tendencia y estacionalidad pueden cambiar con el tiempo. Valores pequeños permiten cambios más rápidos, valores grandes implican que no hay cambios. Ambos parámetros deben ser números impares:\n\nt.window es el número de observaciones consecutivas que se deben utilizar al estimar la tendencia. Consulta la ayuda para ver el valor por defecto.\ns.window está relacionado con el número observaciones que se deben utilizar al estimar cada valor de la componente estacional. No hay ningún valor por defecto para este parámetro. Establecerlo como periodic equivale a que la componente estacional sea periódica (es decir, idéntica a lo largo de los años). Si es un valor numérico, debe ser impar y mayor o igual a 7.\n\n\n\nEjemplo\nVeamos un ejemplo de su uso para la serie Demanda eléctrica (Figura 23). Se ha usado el valor por defecto para t.window y se ha indicado que la estacionalidad es constante en el tiempo (s.window = \"periodic\"). Además, se ha especificado que se tenga en cuenta la posible existencia de valores atípicos (robust = TRUE).\n\neleStl &lt;- stl(electricidad, \n              s.window = \"periodic\",\n              robust = TRUE)\n\n\nautoplot(eleStl,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 23: Descomposición de Electricidad por regresiones locales ponderadas\n\n\n\n\n\nEl error indica que hay bastantes días atípicos en el año, con un consumo de electricidad inferior al esperado, que corresponden a los días festivos (véase el panel inferior de la Figura 23).\nPodemos ver la componente estacional estimada con este método de descomposición y comprobar que sus valores suman 0.\n\ncomponenteEstacionalElecStl &lt;- seasonal(eleStl)[1:7]\ncomponenteEstacionalElecStl\n\n[1]  13.80309  30.60225  32.72148  34.94806  27.76881 -50.08196 -89.76173\n\nsum(componenteEstacionalElecStl)\n\n[1] -3.139946e-07\n\n\n\n\nPara Demanda eléctrica hemos obtenido tres estimaciones de la componente estacional: la primera obtenida con tapply, la segunda obtenida con decompose y la tercera con stl. Se puede observar que las tres estimaciones son muy similares, pero no coincidentes, y aunque los métodos de descomposición son preferibles a tapply, ninguna estimación es a priori mejor que otra.\n\n# tapply\nround(as.numeric(componenteEstacionalElec), 2)\n# decompose\nround(componenteEstacionalElecDes, 2)\n# stl\nround(componenteEstacionalElecStl, 2)\n\n\n\n              L     M     X     J     V      S      D\ntapply    13.49 30.91 31.73 32.47 23.14 -47.25 -85.34\ndecompose 15.90 31.65 31.18 32.46 22.75 -47.54 -86.40\nstl       13.80 30.60 32.72 34.95 27.77 -50.08 -89.76\n\n\n\n\nSi en lugar de periodic, fijamos el parámetro s.window a, por ejemplo, 11 (siempre un valor impar), estaremos permitiendo que la estacionalidad cambie en el tiempo. La Figura 24 muestra la componente estacional estimada previamente (bajo el supuesto de componente estacional constante) y la que se obtiene con el argumento s.window = 11. Para el periodo mostrado se observa que la componente estacional ha variado con el tiempo. Cuanto mayor es el valor (impar) de s.window más constante en el tiempo es la componente estacional.\n\n\n\n\n\n\n\n\nFigura 24: Componente estacional para Demanda eléctrica"
  },
  {
    "objectID": "04-01-Bootstrapping.html#idea-general",
    "href": "04-01-Bootstrapping.html#idea-general",
    "title": "Bootstrapping para obtener intervalos de predicción",
    "section": "Idea general",
    "text": "Idea general\nVeamos primero la idea general y luego los detalles:\n\nPartimos de una serie temporal \\(\\{y_t\\}_{t=1}^T\\) y un horizonte de predicción \\(h\\).\nA partir de la serie original vamos a generar una nueva serie que es similar a la original. Luego veremos cómo.\nAjustamos nuestro modelo a la nueva serie y obtenemos una predicción \\(h\\) periodos hacia adelante, que llamaremos \\(\\hat y_{T+h|T}^1\\).\nRepetimos los pasos 2 y 3 un numero \\(n\\) de veces (típicamente \\(n=5000\\)), de forma que al final del proceso tenemos \\(n\\) predicciones \\(h\\) periodos hacia adelante, \\(\\hat y_{T+h|T}^1,\\; \\hat y_{T+h|T}^2,\\; \\ldots,\\; \\hat y_{T+h|T}^n\\), obtenidas a partir de \\(n\\) series similares a la original.\nPor último, obtenemos el intervalo de predicción calculando los percentiles correspondientes a partir de estas \\(n\\) predicciones.\n\nHay que repetir este proceso para cada horizonte de predicción en que estemos interesados."
  },
  {
    "objectID": "04-01-Bootstrapping.html#detalles",
    "href": "04-01-Bootstrapping.html#detalles",
    "title": "Bootstrapping para obtener intervalos de predicción",
    "section": "Detalles",
    "text": "Detalles\nLa clave del proceso es el paso 2, donde se obtiene una nueva serie similar a la original. También conviene aclarar un poco más el paso 5.\nPaso 2: nueva serie\nEn lo que viene a continuación no seré riguroso para no perdernos en cuestiones matemáticas, pero sí suficientemente preciso para entender bien el proceso.\n\nDada la serie original, la descomponemos en sus tres componentes: tendencia, estacionalidad y error.\nA continuación, obtenemos una versión barajada de la componente del error. (Aquí es donde no estoy siendo preciso porque el proceso de barajado se tiene que hacer por bloques y es con reemplazamiento.)\nAhora se combinan –sumando o multiplicando, según el esquema– la tendencia, la estacionalidad y el error barajado para obtener una nueva serie que se parecerá a la original porque tiene la misma tendencia y estacionalidad, pero diferente error.\n\nVeamos un ejemplo muy sencillo para una serie simulada de orden estacional 5.\n\nLa dos primeras columnas indican la estación de cada dato y el valor de la serie, para un total de 15 observaciones.\nLas columnas Tendencia, Estacionalidad y Error han sido obtenidas aplicando el método de descomposición por regresiones locales ponderadas. Observa que cada dato de la serie es la suma de estas tres componentes.\nLa columna ErrorBootstrapping se ha obtenido como un muestreo con reemplazamiento de los datos de la columna Error. Como es una muestra con reemplazamiento, hay algunos errores repetidos.\nPor último, la nueva serie (columna NuevaSerie) se ha obtenido sumando las columnas Tendencia, Estacionalidad y ErrorBootstrapping. Esta serie tiene la misma tendencia y estacionalidad que la serie original y solo se diferencia en el error, así que la nueva serie debería parecerse a la serie original.\n\n\n\n   Estacion Serie Tendencia Estacionalidad  Error ErrorBootstrapping NuevaSerie\n1         1 27.00     19.96           9.60  -2.56              -2.96      26.59\n2         2 16.72     25.12         -13.33   4.93               9.43      21.22\n3         3 15.08     30.14         -12.09  -2.96              -2.56      15.49\n4         4 18.79     34.77          -9.11  -6.87               3.10      28.76\n5         5 75.53     38.58          24.94  12.01              12.01      75.53\n6         1 63.31     39.24           9.60  14.47               3.10      51.94\n7         2 17.28     38.48         -13.33  -7.87               9.43      34.58\n8         3 18.00     34.84         -12.09  -4.75              -6.87      15.88\n9         4 24.84     30.85          -9.11   3.10              -0.40      21.34\n10        5 54.67     30.13          24.94  -0.40              -6.87      48.21\n11        1 30.85     31.11           9.60  -9.86              12.01      52.72\n12        2 22.02     30.52         -13.33   4.84              -9.86       7.32\n13        3 26.51     29.17         -12.09   9.43               3.10      20.18\n14        4 24.14     27.94          -9.11   5.31              -7.87      10.96\n15        5 41.31     26.54          24.94 -10.17              12.01      63.49\n\n\nPaso 5: Predicción por intervalos\nPartimos de \\(n\\) predicciones a \\(h\\) periodos vista (\\(\\hat y_{T+h|T}^1,\\; \\hat y_{T+h|T}^2,\\; \\ldots,\\; \\hat y_{T+h|T}^n\\)) y queremos obtener a partir de ellas el intervalo de confianza.\nSupongamos que el nivel de confianza deseado es del 95%. Entonces, debemos calcular para las predicciones el percentil 2.5% y 97.5%. Recuerda que el percentil 2.5% es el valor numérico que deja un 2.5% de las predicciones por debajo de él; y que el percentil 97.5% es el valor numérico que deja un 97.5% de las predicciones por debajo de él. La función de R quantile() permite obtener estos valores.\nSi denominamos \\(l_h\\) al percentil 2.5% y \\(u_h\\) al percentil 97.5%, el intervalo de confianza de la predicción a \\(h\\) periodos vista es \\([l_h,\\; u_h]\\).\n¿Y la prediccion puntual?\nPara la predicción puntual tenemos dos opciones: obtener la predicción a partir de la serie original (como hemos visto en clase); u obtenerla como media de las \\(n\\) predicciones obtenidas\n\\[\\frac{1}{n}\\sum_{j=1}^n \\hat y_{T+h|T}^j\\]\nEste segundo método es el usual y se denomina bagging de bootstrap aggregating."
  },
  {
    "objectID": "03-07-Ejemplo2.html",
    "href": "03-07-Ejemplo2.html",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "",
    "text": "1 Introducción\nConsideremos la serie temporal correspondiente al número de defunciones causadas por enfermedades cerebrovasculares, Esta serie está disponible en el Instituto Nacional de Estadística desde enero de 1980 hasta diciembre de 2024, un total de 45 años o 550 meses.\nLa serie presenta tendencia decreciente y estacionalidad de orden 12 en un claro esquema multiplicativo (véase Figura 1). Ya vimos en el análisis descriptivo que el determinante principal del patrón estacional es la temperatura.\nAdemás, la descomposición revelaba la presencia de varios valores atípicos concentrados al inicio de la serie. Por este motivo, para el análisis por técnicas de Alisado Exponencial, vamos a recortar la serie y considerarla solo desde enero de 1990, 35 años o 420 meses.\n\nDefEnfCer &lt;- read.csv2(\"./series/Enfermedades cerebrovasculares.csv\", \n                       header = TRUE)\n\nDefEnfCer &lt;- ts(DefEnfCer[,2], \n                start = 1980, \n                frequency = 12)\n\nDefEnfCer &lt;- window(DefEnfCer, \n                    start = 1990)\n\nautoplot(DefEnfCer,\n         xlab = \"\",\n         ylab = \"Defunciones\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1990, 2024, 4)) \n\n\n\n\n\n\n\nFigura 1: Defunciones causadas por enfermedades cerebrovasculares\n\n\n\n\n\n\n\n\n\n\n2 Ajuste por alisado exponencial e interpretación\nVamos a aplicar la metodología de Alisado Exponencial a la serie de defunciones. Si se estima el modelo sin imponer ninguna restricción ets identifica como modelo óptimo ETS(M,Ad,M), con un valor de \\(\\phi = 0.98\\), su valor máximo. Por tanto, se opta por solicitar de nuevo el mejor modelo excluyendo aquellos con amortiguamiento.\n\nDefEnfCerEts &lt;- ets(DefEnfCer, \n                    damped = FALSE)\n\nsummary(DefEnfCerEts) \n\nETS(M,N,M) \n\nCall:\nets(y = DefEnfCer, damped = FALSE)\n\n  Smoothing parameters:\n    alpha = 0.2573 \n    gamma = 0.0545 \n\n  Initial states:\n    l = 3628.3267 \n    s = 1.1312 0.9787 0.942 0.8367 0.9089 0.9492\n           0.8882 0.9623 0.9722 1.0902 1.0919 1.2485\n\n  sigma:  0.052\n\n     AIC     AICc      BIC \n6703.329 6704.517 6763.933 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -17.67424 149.6521 109.2939 -0.8252858 3.910041 0.6900533\n                  ACF1\nTraining set 0.1117412\n\n\nEl modelo estimado, “MNM”, no tiene pendiente y tiene estacionalidad y error multiplicativos: \\[y_{t+1} = l_t \\cdot s_{t+1-m} \\cdot (1 + \\varepsilon_{t+1}).\\]\nEl valor de \\(\\alpha\\) indica que el nivel de la serie ha ido variando en el tiempo. El valor de \\(\\gamma\\), cercano a cero, indica que la estacionalidad cambia muy lentamente con el tiempo. En concreto, la amplitud estacional se ha ido reduciendo (véase Figura 2).\n\nautoplot(DefEnfCerEts,\n         xlab = \"\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 2: Componentes por Alisado Exponencial para defunciones por enfermedades cerebrovasculares\n\n\n\n\n\nLa calidad del ajuste es bastante buena, con un MAPE de 3.9% y un RMSE de 150 defunciones (o 109 si usamos el MAE). Además, según el MASE, el modelo de Alisado supone una mejora del 31% respecto al método ingenuo con estacionalidad, el más sencillo que podríamos aplicar. No hay sesgo y la ACF1 indica que las estimaciones por intervalo de las previsiones no serán del todo correctas.\nLos últimos valores estimados del nivel y la estacionalidad, que corresponden a diciembre de 2024, nos permiten mostrar gráficamente la componente estacional (Figura 3).\n\ntail(DefEnfCerEts$states, 1)\n\n\n\n               l  s1   s2   s3   s4   s5   s6   s7   s8   s9  s10  s11 s12\nDec 2024 1901.13 1.1 0.97 0.94 0.85 0.91 0.94 0.89 0.95 0.97 1.08 1.05 1.2\n\n\n\ncomponenteEstacional &lt;- tail(DefEnfCerEts$states, 1)[13:2]\n\nggplot() +\n  geom_line(aes(x = 1:12, y = componenteEstacional)) + \n  geom_hline(yintercept = 1, colour = \"blue\", lty = 2) +\n  labs(title = \"\", x = \"\",  y = \"Efecto estacional\") +\n  scale_x_continuous(breaks= 1:12, \n                     labels = c(\"Ene\", \"Feb\", \"Mar\", \"Abr\", \"May\", \"Jun\", \n                                \"Jul\", \"Ago\", \"Sep\", \"Oct\", \"Nov\", \"Dic\")) \n\n\n\n\n\n\n\nFigura 3: Componente estacional\n\n\n\n\n\nEl nivel de las defunciones por enfermedades cerebrovasculares en diciembre de 2024 (última observación) es de 1901 casos. La mayor incidencia de las defunciones por enfermedades cerebrovasculares tiene lugar en invierno, en los meses de diciembre a marzo. En concreto, destaca el mes enero con un incremento del 20% (s12) en las defunciones por enfermedades cerebrovasculares respecto a la media anual. La incidencia en verano es menor que la media anual, observándose en septiembre un 15% menos de casos (s4). El efecto estacional estimado por el método de Alisado es menos acusado que el estimado durante el análisis descriptivo de la serie.\n\n\n\n\n\n3 Predicción\nSi pedimos los valores de predicción y su intervalo de confianza al 95% para los próximos tres años, tenemos (numéricamente sólo se muestra el primer año):\n\nDefEnfCerEtsPre &lt;- forecast(DefEnfCerEts, \n                            h = 36, \n                            level = 95)\n\nDefEnfCerEtsPre\n\nRecuerda que las fórmulas usadas para el cálculo del intervalo de confianza de las predicciones no son adecuadas.\n\n\n         Point Forecast    Lo 95    Hi 95\nJan 2025       2284.014 2051.163 2516.864\nFeb 2025       2002.689 1791.850 2213.528\nMar 2025       2058.367 1835.021 2281.714\nApr 2025       1842.032 1636.384 2047.681\nMay 2025       1813.028 1605.086 2020.969\nJun 2025       1699.574 1499.594 1899.554\nJul 2025       1778.807 1564.347 1993.267\nAug 2025       1723.591 1510.907 1936.275\nSep 2025       1610.969 1407.721 1814.216\nOct 2025       1782.882 1553.114 2012.649\nNov 2025       1848.502 1605.371 2091.632\nDec 2025       2090.256 1809.888 2370.623\n\n\n\nautoplot(DefEnfCerEtsPre,\n         xlab = \"\",\n         ylab = \"Casos\",\n         main = \"\",\n         xlim = c(2000, 2028))\n\n\n\n\n\n\n\nFigura 4: Defunciones por enf. cerebrovasculares (2000-2024) y predicción (2025-2027)\n\n\n\n\n\nEl nivel de las predicciones es constante en el tiempo dado que el modelo de Alisado estimado no tiene pendiente (véase Figura 4).\n\n\n\n\n\n4 Análisis del error\nLa Figura 5 muestra el residuo del modelo. Se aprecian cuatro meses en los que el residuo supera las tres desviaciones típicas porque el número de muertes por enfermedades cerebrovasculares fue muy superior al estimado por el modelo: febrero de 1999, mayo de 2001, junio 2003 y febrero de 2012. El error en este último mes, febrero de 2012, es especialmente acusado —supera las 5 desviaciones típicas y es el único identificado por la prueba de Tukey. Durante ese mes se registraron temperaturas muy bajas en gran parte del país, incluso algunas nevadas inusuales en zonas donde no era común ver nieve, y se reportaron mínimas récord en varias ciudades españolas. Estas temperaturas extremas causaron un incremento significativo en el número de defunciones por enfermedades cerebrovasculares.\n\nerror &lt;- residuals(DefEnfCerEts)\nsderror &lt;- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"black\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(1990, 2024, 2)) \n\nfechas &lt;- format(seq(as.Date(\"1990-01-01\"), as.Date(\"2024-12-01\"), \"month\"), \"%Y-%m\")\nfechas[abs(error) &gt; 3 * sderror]\n\n[1] \"1999-02\" \"2001-05\" \"2003-06\" \"2012-02\"\n\natipicos &lt;- tsoutliers(error)\nfechas[atipicos$index]\n\n[1] \"2012-02\"\n\n\n\n\n\n\n\n\nFigura 5: Error + Intervención\n\n\n\n\n\n\n\n\n\n\n5 Precisión de la predicciones\nPara determinar la precisión de las predicciones aplicaremos la metodología de origen de prediccion móvil. Asumimos que se precisan diez años para hacer una buena estimación, \\(k=120\\), y que el horizonte temporal es un año, \\(h = 12\\) meses. La siguiente rutina permite obtener el MAPE para previsiones con un horizonte temporal desde 1 mes hasta 12 meses.\n\nk &lt;- 120                 \nh &lt;- 12                  \nTT &lt;- length(DefEnfCer)  \ns &lt;- TT - k - h          \n\nmapeAlisado &lt;- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set &lt;- subset(DefEnfCer, start = i + 1, end = i + k)\n  test.set &lt;-  subset(DefEnfCer, start = i + k + 1, end = i + k + h)\n  \n  fit &lt;- ets(train.set, model = \"MNM\")\n  fcast&lt;-forecast(fit, h = h)\n  mapeAlisado[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n}\n\nerrorAlisado &lt;- colMeans(mapeAlisado)\nerrorAlisado\n\n [1] 4.147739 4.435996 4.691025 4.660116 4.772368 4.920592 4.941163 4.972012\n [9] 4.866892 4.882214 4.886867 4.975686\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorAlisado)) +\n  labs(title = \"Error de predicción según horizonte temporal\",\n       x = \"Horizonte temporal de predicción\", \n       y = \"%\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\n\n\n\nFigura 6: Error de predicción según horizonte temporal\n\n\n\n\n\nLa Figura 6 muestra que aunque el error aumenta con el horizonte temporal de previsión, se mueve en una banda muy estrecha entre el 4.1% y 5%.\n\n\n\n\n\n6 Modelos alternativos\n¿Podemos reducir el error extramuestral de previsión, si cambiamos las opciones por defecto de ets o la serie a analizar? La Figura 7 muestra el error de previsión extramuestral según el horizonte de previsión para los siguientes modelos (todos sin amortiguamiento y en caso de transformación logarítmica, sin ajuste para insesgadez):\n\n\n\nId\nTransformación\nModelo\n\n\n\n\n1\nNinguna\nMNM\n\n\n2\nNinguna\nMAM\n\n\n3\nLogaritmo\nANA\n\n\n4\nLogaritmo\nAAA\n\n\n5\nDefunciones por día\nMNM\n\n\n6\nDefunciones por día\nMAM\n\n\n\nEn concreto, los comandos utilizados han sido:\n\nModelo 1: ets(x, model = \"MNM\")\nModelo 2: ets(x, model = \"MAM\", damped = FALSE)\nModelo 3: ets(x, model = \"ANA\", lambda = 0)\nModelo 4: ets(x, model = \"AAA\", lambda = 0, damped = FALSE)\nModelo 5: ets(x/monthdays(x), model = \"MNM\")\nModelo 6: ets(x/monthdays(x), model = \"MAM\", damped = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nFigura 7: Errores de previsión extramuestral. Varios modelos\n\n\n\n\n\nDe la Figura 7 deducimos que, aunque todos los métodos resultan similares en la precisión de las predicciones extramuestrales en el corto plazo, al pasar al medio y largo plazo las diferencias pueden ser significativas: la mayor diferencia entre los modelos se da para la previsión a ocho meses vista y es de 0.85 puntos porcentuales.\nSi queremos entrar en matices:\n\nEl modelo que ofrece las previsiones más precisas es el modelo 6, un modelo sobre la serie defunciones por día con pendiente aditiva y estacionalidad multiplicativa (model = \"MAM). El segundo mejor modelo, el modelo 2, sigue el mismo esquema que el modelo 6 pero sobre la serie original.\nLos modelos que tienen tendencia (modelos 2, 4 y 6) resultan mejores que sus homólogos sin tendencia (modelos 1, 3 y 5, respectivamente).\nEl uso del logaritmo no mejora la precisión en las predicciones.\n\nEs decir, tanto la estrategia de predecir la serie de defunciones medias por día (en lugar de la serie original) como la de forzar a que el modelo tenga tendencia, que es lo que se observa gráficamente, mejoran la calidad de las previsiones extramuestrales. La combinación de estas dos estrategias es la óptima."
  },
  {
    "objectID": "03-06-Ejemplo1.html#tendencia",
    "href": "03-06-Ejemplo1.html#tendencia",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "3.1 Tendencia",
    "text": "3.1 Tendencia\nHemos obtenido la serie anual de casos de defunciones causadas por enfermedades cerebrovasculares, que se muestra en la Figura 3 y que permite analizar mejor la tendencia decreciente. A primeros de los 80 el número de defunciones prácticamente alcanzaba las 50,000 al año, mientras que actualmente están por debajo de las 25,000. Esto supone una caída media anual aproximada de 550 defunciones. Si nos centramos en la evolución de la serie en 2020 y años sucesivos, parece que la Covid-19 no ha tenido ningún efecto sobre el número de defunciones.\n\nautoplot(CasosAnual,\n         xlab = \"\",\n         ylab = \"Casos\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1980, 2024, 4)) \n\n\n\n\n\n\n\nFigura 3: Defunciones anuales causadas por enfermedades cerebrovasculares"
  },
  {
    "objectID": "03-06-Ejemplo1.html#estacionalidad",
    "href": "03-06-Ejemplo1.html#estacionalidad",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "3.2 Estacionalidad",
    "text": "3.2 Estacionalidad\nVeamos ahora como varía la incidencia de las muertes causadas por enfermedades cerebrovasculares según el mes del año.\n\nggsubseriesplot(DefEnfCer, \n             xlab = \"\",\n             ylab = \"\",\n             main = \"\") +\n  guides(colour=FALSE)\n\n\n\n\n\n\n\nFigura 4: Evolución mensual de las defunciones por enf. cerebrovasculares\n\n\n\n\n\nEn la Figura 4 cada subserie corresponde a un mes y las líneas azules indican la media mensual, revelando un marcado patrón estacional. Se aprecia que el principal determinante es la temperatura puesto que la incidencia de la enfermedad es mayor en los meses de invierno y menor en los de verano. También cabría esperar un efecto días del mes y observar más incidencia en los meses de 31 días que en los de 30, pero el efecto de la temperatura es tan dominante que camufla cualquier otro efecto. Sin embargo, para el mes de febrero se ve muy claramente el efecto días de mes. Por temperatura febrero debería situarse a medio camino entre enero y marzo, pero por ser un mes con solo 28 días (29 los años bisiestos) la media de defunciones es menor y se sitúa algo por debajo del nivel que marzo."
  },
  {
    "objectID": "04-06-Covid_Nacimientos.html#ajuste-a-un-modelo-y-predicción",
    "href": "04-06-Covid_Nacimientos.html#ajuste-a-un-modelo-y-predicción",
    "title": "Efecto de la Covid-19 sobre la serie Nacimientos",
    "section": "Ajuste a un modelo y predicción",
    "text": "Ajuste a un modelo y predicción\nVamos a considerar la serie de nacimientos desde enero de 2000 hasta octubre de 2020, ajustarla a un modelo ARIMA y predecir hasta diciembre de 2022.\nEl siguiente código estima el mismo modelo visto en el tema de ARIMA con estacionalidad para la serie nacimientos. La única diferencia es que en esta ocasión la serie termina en octubre de 2020. El ajuste, con un error porcentual del 1.6% y sin sesgo, es muy bueno.\n\nnacimientos2 &lt;- window(nacimientos, \n                       end = c(2020, 10))\n\nDiasMes &lt;- monthdays(nacimientos2)\nSemanaSanta &lt;- easter(nacimientos2)\n\nfechas &lt;- format(seq(as.Date(\"2000-01-01\"), as.Date(\"2020-10-01\"), by = 'month'), \n                 \"%Y-%m\")\n\nd1210 &lt;- 1*(fechas == \"2010-12\")\nd0111 &lt;- 1*(fechas == \"2011-01\")\n\nd12100111 &lt;- d1210 - d0111\n\nmodelo &lt;- Arima(nacimientos2, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 1),\n                 lambda = 0,\n                 xreg = cbind(DiasMes, SemanaSanta, d12100111))\n\nsummary(modelo)\n\nSeries: nacimientos2 \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasMes  SemanaSanta  d12100111\n      -0.5242  -0.7553   0.0302      -0.0166     0.0602\ns.e.   0.0563   0.0457   0.0076       0.0050     0.0105\n\nsigma^2 = 0.0004176:  log likelihood = 584.48\nAIC=-1156.97   AICc=-1156.6   BIC=-1136.16\n\nTraining set error measures:\n                    ME    RMSE     MAE        MPE     MAPE      MASE\nTraining set -79.96303 732.924 578.707 -0.2386596 1.575176 0.4225862\n                    ACF1\nTraining set 0.005974681\n\n\nAhora vamos a predecir la serie desde noviembre de 2020 hasta diciembre de 2022 (26 meses).\n\ntmp &lt;- ts(rep(0, 26), start = c(2020, 11), freq = 12)\npdm &lt;- monthdays(tmp)\npss &lt;- easter(tmp)\nprediccion &lt;- forecast(modelo, \n                       h = 26,\n                       xreg = cbind(pdm, pss, rep(0, 26)))\n\nLa Figura 2 muestra la serie original de nacimientos desde 2018 y la predicción. Por la metodología seguida, desde noviembre de 2020 disponemos para cada mes de dos datos: los nacimientos en el mundo real con Covid (observaciones) y los nacimientos en un mundo sin Covid (previsiones).\n\nautoplot(nacimientos,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\", \n         series = \"Con Covid\") + \n  xlim(2018, 2023) +\n  ylim(20000, 35000) + \n  autolayer(prediccion, series = \"Sin Covid\", PI = FALSE) + \n  labs(colour = \"Nacimientos\") + \n  theme(legend.position=c(0.9,0.85)) \n\n\n\n\n\n\n\nFigura 2: Nacimientos mensuales (enero 2018 - diciembre 2022) y previsiones (noviembre 2020 - diciembre 2022)\n\n\n\n\n\nUna lectura rápida de la Figura 2 muestra que efectivamente, el número observado de nacimientos entre noviembre de 2020 y febrero de 2021 (en rojo) fue muy inferior a los valores esperados (en azul). Sin embargo, el resto del año 2021 el número de nacimientos fue superior al esperado, apuntando a un ligero efecto rebote. En 2022 el efecto de la pandemia no parece haber desaparecido totalmente, observándose que el número de nacimientos real superaba la mayoría de los meses el valor predicho. Veamos estas observaciones en detalle."
  },
  {
    "objectID": "04-06-Covid_Nacimientos.html#efecto-del-confinamiento-sobre-los-nacimientos",
    "href": "04-06-Covid_Nacimientos.html#efecto-del-confinamiento-sobre-los-nacimientos",
    "title": "Efecto de la Covid-19 sobre la serie Nacimientos",
    "section": "Efecto del confinamiento sobre los nacimientos",
    "text": "Efecto del confinamiento sobre los nacimientos\nEn primer lugar, veamos cuál ha sido la caída en el número de nacimientos entre noviembre de 2020 y febrero de 2021.\n\nCon_covid &lt;- as.numeric(window(nacimientos, \n                               start = c(2020, 11), \n                               end = c(2021, 2)))\nSin_covid &lt;- as.numeric(window(prediccion$mean, \n                               start = c(2020, 11), \n                               end = c(2021, 2)))\n\n# Caida porcentual\nround(100 * (Con_covid - Sin_covid)/Con_covid, 1)\n\n[1]  -6.9 -20.7 -19.5  -3.5\n\n#Caida en el total de nacidos\nsum(Con_covid - Sin_covid)\n\n[1] -12219.18\n\n\nLa mayor caída porcentual en el número de nacimientos tuvo lugar en los meses de diciembre de 2020 y enero de 2021 (20.7% y 19.5%, respectivamente) y nuestras estimaciones coinciden con las aportadas en González (2021) y Blanes, Domingo, and Esteve (2021). Respecto del número de nacimientos, nosotros estimamos una reducción de 12200 nacidos, un valor algo inferior a la estimación en González (2021) y muy superior a la estimación en Blanes, Domingo, and Esteve (2021)."
  },
  {
    "objectID": "04-06-Covid_Nacimientos.html#efecto-rebote",
    "href": "04-06-Covid_Nacimientos.html#efecto-rebote",
    "title": "Efecto de la Covid-19 sobre la serie Nacimientos",
    "section": "Efecto rebote",
    "text": "Efecto rebote\nUna posibilidad es que el confinamiento no hizo que las parejas decidieran no tener hijos de forma permanente, sino que simplemente retrasó la decisión de tenerlos. Si esto es así, cabria esperar a mediados o finales de 2021 un número de nacimientos superior al esperado: por un lado tendríamos los nacimientos de las parejas que tenían pensado tener hijos en ese momento y por otro los de las parejas que habían retrasado el momento de la maternidad. Si es así, la Covid no habría reducido de forma permanente el número de nacimientos y el acumulado en el medio/largo plazo seria el mismo que si no hubiera habido Covid.\nPara poder responder mejor a esta pregunta, vamos a calcular el déficit de nacimientos: la diferencia acumulada entre el número de nacimientos esperado y el real desde noviembre de 2020 hasta diciembre de 2022. La Figura 3 muestra que el déficit de nacimientos crece hasta los 12200 bebés en febrero de 2021. Esta diferencia máxima se va reduciendo lentamente hasta los 3600 bebés en diciembre de 2021. Es decir, efectivamente parece que hay un efecto rebote que ha compensado a lo largo del año 2021 en un total de 8600 bebés la caída hasta febrero de ese año.\nDurante el año 2022 el déficit de nacimientos ha seguido reduciéndose hasta desaparecer hacia agosto de 2022. Sin embargo, en los siguientes meses esta diferencia se hace negativa, alcanzado los -2600 bebes en diciembre de 2022. Este último resultado apunta a que durante 2022 el número de nacimientos ha sido inferior al esperado. Es difícil identificar la causa, que puede ir desde el error propio de nuestras previsiones para 2022, al efecto que la inestabilidad política y económica pueda tener sobre la decisión de las parejas de tener hijos (guerra ruso-ucraniana, incremento de los precios energéticos, inflación…).\n\nCon_covid &lt;- window(nacimientos, start = c(2020, 11))\nSin_covid &lt;- window(prediccion$mean, start = c(2020, 11))\n\nDiferencia &lt;- cumsum(Sin_covid - Con_covid)\nDiferencia &lt;- ts(Diferencia, \n                 start = c(2020, 11),\n                 frequency = 12)\n\n\nautoplot(Diferencia,\n         xlab = \"\",\n         ylab = \"Deficit de nacimientos\",\n         main = \"\") + \n  geom_hline(yintercept = 0, col = \"red\") + \n  xlim(2020.7, 2023) +\n   scale_x_continuous(breaks= seq(2020 + 11/12, 2022 + 11/12, 3/12),\n                     label = c(\"Dic-20\", \n                               \"Mar-21\", \"Jun-21\", \"Sep-21\", \"Dic-21\", \n                               \"Mar-22\", \"Jun-22\", \"Sep-22\", \"Dic-22\"))\n\n\n\n\n\n\n\nFigura 3: Déficit acumulado de nacimientos desde noviembre de 2020"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Series Temporales",
    "section": "",
    "text": "Series Temporales (42229 Modelización estadística). Máster de Bioestadística\n\n\nHola a tod@s y bienvenid@s a la página web del curso.\n\nEsta es la página web del curso de Series Temporales (Máster en Bioestadística) de la Universitat de València. El repositorio para crear esta web lo tenéis aquí en Github.\nResumen del curso: la idea principal es aprender a manejarse con datos con estructura temporal (en contraposición a datos de corte transversal o espacial, que veréis en Modelos lineales y Estadística espacial). Veremos cómo describir una serie temporal, como ajustarla a un modelo, cómo hacer predicciones y, lo que es más importante, a valorar la calidad de las predicciones.\nPara más información de este módulo, visita la sección Guía del curso de esta página web.\n\n– Iván Arribas"
  },
  {
    "objectID": "06-Practica.html",
    "href": "06-Practica.html",
    "title": "Prácticas de evaluación",
    "section": "",
    "text": "Durante todo el módulo realizarás una serie de ejercicios que aplicarán las técnicas vistas en cada tema. Lee con atención las siguientes instrucciones generales.\nTu primera tarea es descargarte tu serie temporal. Descárgate aquí el fichero .pdf con la descripción de las series y localiza en él el nombre de tu serie. El este fichero encontrarás también una descripción detallada de la serie que vas a analizar: nombre, definición, unidades, fuente, fechado…\nPor otro lado, descárgate aquí el fichero comprimido con todas las series, descomprímelo y localiza el fichero ‘’csv’’ con tu serie.\n\n\n\n\n\nCaracterísticas generales de las entregas\n\n\n\nLa siguiente descripción de las prácticas es provisional e incompleta. El contenido final se indicará en clase al finalizar cada tema.\nLa fecha máxima de entrega de cada trabajo se indicará al finalizar cada tema. Procura no retrasarte. Cada día de retraso reducirá un punto la nota de la práctica y si el retraso supera la semana, tu nota será de cero.\nEn Aulavirtual, en la tarea “Practica evaluación n” debes subir un fichero .pdf con los resultados de la práctica n-ésima. Sólo se admite un único fichero en formato .pdf.\nPiensa que además de los contenidos también su ordenación, estructura, sintaxis, comentarios, etc. son parte de la evaluación. Aquí tienes qué aspectos voy a valorar y la puntuación de cada uno.\nIncluye siempre todo el código en R utilizado en el trabajo. Si utilizas Markdown (o similar), haz que el código sea visible; en otro caso, añádelo como un apéndice al final del documento. Si no se incluye el código, la práctica se corregirá sobre una nota máxima de 5.\n\n\n\n\n\n\nPráctica 1: Descripción de la serie temporal\n\n\n\nHaz una descripción detallada de tu serie (tendencia, estacionalidad, intervención, tipo de esquema…).\nAyúdate para ello de todos los elementos numéricos y gráficos que hemos visto: gráfica de tu serie original, gráfica de su agregación anual, gráfica localización/dispersión, descomposición…\n\nSi el análisis realizado así lo aconseja, indica si vas a recortar tu serie para posteriores análisis.\n\n\n\n\n\nPráctica 2: Alisado exponencial\n\n\n\nAjusta el modelo de Alisado más adecuado a tus datos.\n\nDescribe el modelo obtenido.\nObtén los indicadores de la calidad del modelo y coméntalos.\nRealiza un predicción a tres años vista y muestra los resultados de la predicción gráficamente.\nObtén la calidad de las previsiones con origen de previsión móvil.\nObtén el residuo e identifica la presencia de valores extremos.\nCompara el modelo ajustado por Alisado exponencial con el método ingenuo con estacionalidad\n\n\n\n\n\n\n\nPráctica 3: Procesos ARIMA sin estacionalidad\nA grandes rasgos el ejercicio consistirá en analizar tu serie temporal sin estacionalidad (agregación anual), siguiendo la metodología de Box y Jenkins. Por tanto, para esta práctica debes considerar solo tu serie con fechado anual.\nA continuación se indican los contenidos mínimos que se deben presentar. Eres libre de elegir el orden de la presentación de los contenidos, así como la información finalmente presentada.\n\nTransforma la serie para que sea estacionaria y ergódica\n\nObtén su gráfica de la serie y de su primera diferencia.\nRealiza el contraste de raíces unitarias.\nDiferencia la serie para obtener otra que sea estacionaria y ergódica.\nValor el uso de la transformación logarítmica.\n\nIdentificación del modelo ARIMA\n\nIdentificación de la parte estructural, análisis de intervención y valores atípicos.\n\nEstimación del modelo definitivo\n\nEstimación de los coeficientes del modelo.\nDesarrollo de la ecuación estimada.\nInterpretación de los parámetros y de la ecuación estimada.\n\nValidación completa del modelo\n\nSignificatividad de los coeficientes.\nHipótesis de homocedasticidad e incorrelación.\nValores extremos.\nCálculo de la calidad del ajuste y de las predicciones.\n\nPrevisión de la serie (al menos tres años)\nValoración crítica del modelo ARIMA respecto de un modelo de Alisado\n\nComparación del modelo estimado en esta práctica el mejor modelo de Alisado para la serie anual.\nComparación de la calidad de los modelos.\nComparación de las previsiones.\n\n\n\n\n\n\n\nPráctica 4: Procesos ARIMA con estacionalidad\nLos objetivos de esta práctica son idénticos a los de la práctica previa, pero considerando tu serie original con estacionalidad.\n\nTransforma la serie para que sea estacionaria y ergódica.\n\nValora el uso de la transformación logarítmica.\nObtén la gráfica de la serie y de su primera diferencia (regular y/o estacional)\nAdemás, calcula la FAC de la serie y de su primera diferencia (regular y/o estacional).\nDiferencia la serie para obtener otra que sea estacionaria y ergódica.\n\nIdentificación del modelo ARIMA\n\nIdentificación de la parte estructural, análisis de intervención y valores atípicos.\n\nEstimación del modelo definitivo\n\nEstimación de los coeficientes del modelo.\nDesarrollo de la ecuación estimada.\nInterpretación de la ecuación estimada.\n\nValidación completa del modelo\n\nSignificatividad de los coeficientes.\nHipótesis de homocedasticidad e incorrelación.\nValores extremos.\nCálculo de la calidad del ajuste y de las predicciones.\n\nPrevisión de la serie (al menos tres años, 36 meses)\nValoración crítica del modelo ARIMA respecto de un modelo de Alisado:\n\nComparación del modelo estimado en esta práctica con el modelo de la práctica 2\nComparación de la calidad de los modelos.\nComparación de las previsiones."
  },
  {
    "objectID": "03-05-Tema5.html#procesos-autorregresivos-ar_mp",
    "href": "03-05-Tema5.html#procesos-autorregresivos-ar_mp",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "2.1 Procesos autorregresivos \\(AR_m(P)\\)",
    "text": "2.1 Procesos autorregresivos \\(AR_m(P)\\)\n\nDefinición\nEl modelo general autorregresivo estacional de orden P, \\(y_t \\sim AR_m(P)\\), viene definido por \\[y_t=c + \\phi_m y_{t-m} + \\phi_{2m} y_{t-2m} + \\ldots + \\phi_{Pm} y_{t-Pm} + \\varepsilon_t,\\] que usando el operador retardo queda \\[(1 - \\phi_m L^m - \\phi_{2m} L^{2m} - \\ldots - \\phi_{Pm} L^{Pm})y_t = c + \\varepsilon_t.\\]\n\n\nPropiedades\nEl proceso es estacionario si quedan fuera del círculo de radio unidad todas las raíces del polinomio \\[\\Phi_P(z) = 1 - \\phi_m z^m - \\phi_{2m} z^{2m} - \\ldots - \\phi_{Pm} z^{Pm}.\\]\nEs invertible siempre.\nSobre todo,\n\nEn la FAC las autocorrelaciones de orden múltiplo de m \\((m,2m,\\ldots)\\) decaen exponencialmente a partir del orden P.\nEn la FACP las autocorrelaciones parciales de orden múltiplo de m verifican que los P primeros valores son no nulos y todos los demás valen cero.\n\n\n\nEjemplos\n\n\\(y_t \\sim AR_{12}(1):\\;\\;y_t = c + \\phi_{12} y_{t-12} + \\varepsilon_t\\) o \\((1 - \\phi_{12} L^{12})y_t = c + \\varepsilon_t\\)\n\\(y_t \\sim AR_7(2):\\;\\;y_t = c + \\phi_7 y_{t-7} + \\phi_{14} y_{t-14} + \\varepsilon_t\\) o \\((1 - \\phi_7 L^7 - \\phi_{14} L^{14})y_t = c + \\varepsilon_t\\)\n\n\n\nSimulaciones de procesos autorregresivos \\(AR_m(P)\\)\nLa Figura 1 muestra una simulación del proceso \\(AR_{4}(2)\\) \\(y_t = 0.7y_{t-4} - 0.7y_{t-8} + \\varepsilon_t\\) (panel superior), y del proceso \\(AR_{12}(1)\\) \\(y_t = 0.6y_{t-12} + \\varepsilon_t\\) (panel inferior). En ambos casos la muestra es de tamaño 500 y \\(\\varepsilon_t\\) se distribuye como una normal con media cero y varianza la unidad. (Todas las simulaciones se han realizado con la función arima.sim de la librería stats.)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) AR(2) - m = 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) AR(1) - m = 12\n\n\n\n\n\n\n\nFigura 1: Simulación de dos procesos AR estacionales con diferente orden estacional"
  },
  {
    "objectID": "03-05-Tema5.html#procesos-en-medias-móviles-ma_mq",
    "href": "03-05-Tema5.html#procesos-en-medias-móviles-ma_mq",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "2.2 Procesos en medias móviles \\(MA_m(Q)\\)",
    "text": "2.2 Procesos en medias móviles \\(MA_m(Q)\\)\n\nDefinición\nEl modelo general en medias móviles estacional de orden Q, \\(y_t \\sim MA_m(Q)\\), viene definido por \\[y_t=c + \\varepsilon_t + \\theta_m \\varepsilon_{t-m} + \\theta_{2m} \\varepsilon_{t-2m} + \\ldots +\n  \\theta_{Qm} \\varepsilon_{t-Qm},\\] que usando el operador retardo queda \\[y_t = c + (1 + \\theta_m L^m + \\theta_{2m} L^{2m} + \\ldots + \\theta_{Qm} L^{Qm}) \\varepsilon_t.\\]\n\n\nPropiedades\nEl proceso es invertible si quedan fuera del círculo de radio unidad todas las raíces del polinomio \\[\\Theta_Q(z) = 1 + \\theta_m z^m + \\theta_{2m} z^{2m} + \\ldots + \\theta_{Qm} z^{Qm}.\\]\nEs estacionario siempre.\nSobre todo,\n\nEn la FAC las autocorrelaciones de orden múltiplo de m verifican que los Q primeros valores son no nulos y todos los demás valen cero.\nEn la FACP las autocorrelaciones parciales de orden múltiplo de m decaen exponencialmente a partir del orden Q.\n\n\n\nEjemplos\n\n\\(y_t \\sim MA_7(1):\\;\\;y_t = c + \\varepsilon_t + \\theta_7 \\varepsilon_{t-7}\\) o \\(y_t = c + (1 + \\theta_7 L^7)\\varepsilon_t\\)\n\\(y_t \\sim MA_{12}(2):\\;\\;y_t=c + \\varepsilon_t + \\theta_{12} \\varepsilon_{t-12} + \\theta_{24} \\varepsilon_{t-24}\\) o \\(y_t = c + (1 + \\theta_{12} L^{12} + \\theta_{24} L^{24})\\varepsilon_t\\)\n\n\n\nSimulaciones de procesos en medias móviles \\(MA_m(Q)\\)\nLa Figura 2 muestra una simulación del proceso \\(MA_{4}(2)\\) \\(y_t = 0.7\\varepsilon_{t-4} - 0.7\\varepsilon_{t-8} + \\varepsilon_t\\) (panel superior), y del proceso \\(MA_{12}(1)\\) \\(y_t = 0.6\\varepsilon_{t-12} + \\varepsilon_t\\) (panel inferior). En ambos casos la muestra es de tamaño 500 y \\(\\varepsilon_t\\) se distribuye como una normal con media cero y varianza igual a la unidad.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) MA(2) - m = 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) MA(1) - m = 12\n\n\n\n\n\n\n\nFigura 2: Simulación de dos procesos MA estacionales con diferente orden estacional"
  },
  {
    "objectID": "03-05-Tema5.html#procesos-arma_mpq",
    "href": "03-05-Tema5.html#procesos-arma_mpq",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "2.3 Procesos \\(ARMA_m(P,Q)\\)",
    "text": "2.3 Procesos \\(ARMA_m(P,Q)\\)\n\nDefinición\nEl modelo general \\(y_t \\sim ARMA_m(P,Q)\\) viene definido por\n\\[y_t = c + \\phi_m y_{t-m} + \\phi_{2m} y_{t-2m} + \\ldots + \\phi_{Pm} y_{t-Pm}  +\n  \\varepsilon_t + \\theta_m \\varepsilon_{t-m} + \\theta_{2m} \\varepsilon_{t-2m} + \\ldots +\n  \\theta_{Qm} \\varepsilon_{t-Qm},\\] que usando el operador retardo queda\n\\[(1 - \\phi_m L^m - \\ldots - \\phi_{Pm} L^{Pm})y_t = c + (1 + \\theta_m L^m + \\ldots + \\theta_{Qm} L^{Qm}) \\varepsilon_t.\\]\nEl proceso más simple es el \\(ARMA_m(1,1)\\): \\(y_t = c  + \\phi_m y_{t-m} + \\theta_m \\varepsilon_{t-m} + \\varepsilon_{t}.\\)\n\n\nPropiedades\nEl proceso es estacionario si quedan fuera del círculo de radio unidad todas las raíces del polinomio \\[\\Phi_P(z) = 1 - \\phi_m z^m - \\phi_{2m} z^{2m} - \\ldots - \\phi_{Pm} z^{Pm}.\\]\nEl proceso es invertible si quedan fuera del círculo de radio unidad todas las raíces del polinomio \\[\\Theta_Q(z) = 1 + \\theta_m z^m + \\theta_{2m} z^{2m} + \\ldots + \\theta_{Qm} z^{Qm}.\\]\nSobre todo,\n\nEn la FAC las autocorrelaciones de orden múltiplo de m decaen exponencialmente a partir del orden P.\nEn la FACP las autocorrelaciones parciales de orden múltiplo de m decaen exponencialmente a partir del orden Q.\n\n\n\nEjemplos\n\n\\(y_t \\sim ARMA_7(1, 1):\\;\\;y_t = c  + \\phi_7 y_{t-7} + \\theta_7 \\varepsilon_{t-7} + \\varepsilon_{t}\\) o \\((1 - \\phi_7 L^7)y_t = c + (1 + \\theta_7 L^7)\\varepsilon_t\\).\n\\(y_t \\sim ARMA_{12}(1, 1):\\;\\;y_t = c  + \\phi_{12} y_{t-12} + \\theta_{12} \\varepsilon_{t-12} + \\varepsilon_{t}\\) o \\((1 - \\phi_{12} L^{12})y_t = c + (1 + \\theta_{12} L^{12})\\varepsilon_t\\).\n\n\n\nSimulación de un proceso \\(ARMA_m(P,Q)\\)\nLa Figura 3 muestra una simulación de tamaño 500 para el proceso \\(ARMA_7(1,1)\\) \\(y_t = 0.7y_{t-7} - 0.5\\varepsilon_{t-7} + \\varepsilon_t\\).\n\n\n\n\n\n\n\n\nFigura 3: Simulación de un proceso ARMA(1,1) estacional con m = 7"
  },
  {
    "objectID": "03-05-Tema5.html#procesos-arima_mpdq",
    "href": "03-05-Tema5.html#procesos-arima_mpdq",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "2.4 Procesos \\(ARIMA_m(P,D,Q)\\)",
    "text": "2.4 Procesos \\(ARIMA_m(P,D,Q)\\)\nSi la serie \\(y_t\\) no es estacionaria en su parte estacional, pero tras diferenciarla \\(D\\) veces se hace estacionaria, diremos que la serie es integrada estacionalmente de orden \\(D\\): \\(y_t \\sim I_m(D)\\). Por tanto,\n\nuna serie estacionaria estacionalmente se indicará como \\(y_t \\sim I_m(0)\\).\n\\(y_t \\sim I_m(1)\\) es equivalente a \\(\\nabla_m y_t = (1 - L^m)y_t \\sim I_m(0)\\)\n\nUna serie \\(y_t\\) sigue un proceso \\(ARIMA_m(P,D,Q)\\) si:\n\nhay que diferenciarla estacionalmente \\(D\\) veces para hacerla estacionaria, \\(y_t \\sim I_m(D)\\); y\nla serie diferenciada sigue un proceso ARMA(P, Q), \\(\\nabla_m^D y_t \\sim ARMA_m(P,Q)\\).\n\nEntonces, podemos escribir \\(y_t \\sim ARIMA_m(P,D,Q)\\) como \\[\\begin{equation*}\n\\begin{array}{c@{\\quad}ccc}\n  (1 - \\phi_m L^m - \\ldots - \\phi_{Pm} L^{Pm}) & (1- L^m)^D y_t & = & c + (1 + \\theta_m L^m + \\ldots + \\theta_{Qm} L^{Qm}) \\varepsilon_t \\\\\n  \\uparrow                                     & \\uparrow       &   & \\uparrow \\\\\n   AR_m(P)                                     & I_m(D)         &   & MA_m(Q)\n\\end{array}\n\\end{equation*}\\]\n\nEjemplo\n\n\\(y_t \\sim ARIMA_{12}(1, 1, 0)\\): \\[\n\\begin{aligned}\n(1 - \\phi_{12} L^{12})(1- L^{12}) y_t & = c +  \\varepsilon_t \\\\\ny_t & = c + y_{t-12} + \\phi_{12}(y_{t-12} - y_{t-24}) + \\varepsilon_t\n\\end{aligned}\n\\]\n\\(log(y_t) \\sim ARIMA_{12}(0, 1, 1)\\): \\[\n\\begin{aligned}\n(1- L^{12}) log(y_t) & = TVAy_t  = c + (1 + \\theta_{12} L^{12}) \\varepsilon_t \\\\\nTVAy_t & = c + \\theta_{12} \\varepsilon_{t-12} + \\varepsilon_t\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "03-05-Tema5.html#proceso-arima_mpdqpdq",
    "href": "03-05-Tema5.html#proceso-arima_mpdqpdq",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "2.5 Proceso \\(ARIMA_m(p,d,q)(P,D,Q)\\)",
    "text": "2.5 Proceso \\(ARIMA_m(p,d,q)(P,D,Q)\\)\nLa realidad nos muestra que la mayoría de las series con estacionalidad se ajustan a una combinación de procesos regulares y estacionales.\nEl proceso \\(ARIMA_m(p, d, q)(P, D, Q)\\) puede ser expresado de forma abreviada como \\[\\Phi_p(L)\\Phi_P(L^m)\\nabla^d\\nabla_m^D  y_t = c + \\Theta_q(L)\\Theta_Q(L^m)\\varepsilon_t,\\]\no menos sucintamente como \\[\\begin{equation*}\n\\begin{array}{ccccc}\n  AR(p) & AR_m(P) & I(d) & I_m(D) &  \\\\\n  \\downarrow & \\downarrow & \\downarrow & \\downarrow  &  \\\\\n  (1 - \\phi_1 L - \\ldots - \\phi_p L^p) & (1 - \\phi_m L^m - \\ldots - \\phi_{Pm} L^{Pm}) & (1 - L)^d & (1- L^m)^Dy_t & = \\\\\n  c + (1 + \\theta_1 L + \\ldots + \\theta_q L^q) & (1 + \\theta_m L^m + \\ldots + \\theta_{Qm} L^{Qm}) \\varepsilon_t & & & \\\\\n  \\uparrow & \\uparrow & & & \\\\\n  MA(q) & MA_m(Q)  & & &\n\\end{array}\n\\end{equation*}\\]\nPor ejemplo, entre las series mensuales uno de los procesos más comunes es \\(ARIMA_{12}(0, 1, 1)(0, 1, 1)\\), denominado modelo de las aerolíneas por ser el proceso generador de datos de muchas series mensuales de transporte de pasajeros, en concreto la serie mensual de pasajeros de avión. La ecuación de este modelo es\n\\[(1-L)(1-L^{12})y_t = (1+ \\theta_1L)(1 + \\theta_{12}L^{12})\\varepsilon_t\\] que si desarrollamos queda \\[y_t = y_{t-1} + (y_{t-12} - y_{t-13}) + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12} + \\theta_{1}\\theta_{12} \\varepsilon_{t-13} + \\varepsilon_t.\\]\n\nEl número de pasajeros del mes \\(t\\) es el mismo que el del mes previo \\(t-1\\), más la diferencia entre estos meses observada el año pasado.\nSi en los meses usados para la predicción (\\(t-1\\), \\(t-12\\) y \\(t-13\\)) ha ocurrido algo extraordinario, hay que tenerlo en cuenta a la hora de afinar la predicción.\n\nSi usamos la transformación logarítmica, tendríamos \\[(1-L)(1-L^{12})\\log(y_t) = (1+ \\theta_1L)(1 + \\theta_{12}L^{12})\\varepsilon_t\\] o \\[(1-L)TVAy_t = (1+ \\theta_1L)(1 + \\theta_{12}L^{12})\\varepsilon_t\\] que desarrollando queda \\[TVAy_t = TVAy_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12} + \\theta_{1}\\theta_{12} \\varepsilon_{t-13} + \\varepsilon_t.\\]\n\nLa tasa de variación anual en el número de pasajeros del mes \\(t\\) es la misma que la del mes previo \\(t-1\\).\nSi en los meses usados para la predicción (\\(t-1\\), \\(t-12\\) y \\(t-13\\)) ha ocurrido algo extraordinario, hay que tenerlo en cuenta a la hora de afinar la predicción.\n\n\nSimulaciones de un proceso \\(ARIMA_m(p,d,q)(P,D,Q)\\)\nLa Figura 4 muestra una simulación de tamaño 1000 para el modelo de las aerolíneas, donde se ha supuesto que \\(\\theta_1=0.7\\) y \\(\\theta_{12}=-0.5\\).\n\n\n\n\n\n\n\n\nFigura 4: Simulación del proceso de las aerolineas ARIMA(0, 1, 1)(0, 1, 1), m = 12"
  },
  {
    "objectID": "03-05-Tema5.html#nacimientos",
    "href": "03-05-Tema5.html#nacimientos",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "3.1 Nacimientos",
    "text": "3.1 Nacimientos\nVamos a aplicar la metodología de Box-Jenkins a la serie mensual de nacimientos en España desde el año 2000 (véase Figura 5).\n\nnacimientos &lt;- read.csv2(\"./series/Nacimientos.csv\", \n                         header = TRUE)\n\nnacimientos &lt;- ts(nacimientos[, 2],\n                  start = c(1975, 1),\n                  freq = 12)\n\nnacimientos &lt;- window(nacimientos, start = 2000)\n\nautoplot(nacimientos,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 5: Nacimientos mensuales\n\n\n\n\n\n\nTransformación de la serie\nYa vimos en el Tema 3) que para que la serie sea estacionaria y ergódica había que diferenciarla tanto regular como estacionalmente (d = D = 1). Además, trabajaremos con el logaritmo de la serie para reducir la posible heterocedasticidad y ganar en interpretabilidad. Es decir, trabajaremos con la siguiente serie transformada\n\\[\\nabla\\nabla_{12}\\log(nacimientos_t) \\sim I(0)I_{12}(0).\\]\n\n\nIdentificación\nTras transformar la serie, vamos a identificar los valores de \\(p\\), \\(q\\), \\(P\\) y \\(Q\\) a partir de la FAC y la FACP.\n\nggtsdisplay(diff(diff(log(nacimientos), lag = 12)), lag = 48)\n\n\n\n\n\n\n\nFigura 6: Nacimientos mensuales\n\n\n\n\n\nAnalizando la FAC y la FACP (Figura 6) observamos que no es fácil la identificación.\n\n\n¿Qué nos indica auto.arima? Primero vamos a generar e incluir en el proceso de autoidentificación las variables asociadas a los efectos de intervención que hemos detectado en los temas previos. En concreto, hemos visto que el número de días del mes explica el número de nacimientos. Este efecto era muy claro para los meses de febrero bisiestos. Para el calculo de la variable que recoge el número de días del mes usaremos la función monthdays de la librería forecast que devuelve el número de días de cada mes o trimestre de una serie.\n\nmonthdays(nacimientos)\n\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2000  31  29  31  30  31  30  31  31  30  31  30  31\n2001  31  28  31  30  31  30  31  31  30  31  30  31\n2002  31  28  31  30  31  30  31  31  30  31  30  31\n2003  31  28  31  30  31  30  31  31  30  31  30  31\n2004  31  29  31  30  31  30  31  31  30  31  30  31\n\n\nPor otro lado, los periodos vacacionales pueden afectar la programación de las cesáreas e influir en el número de nacimientos. Como la Semana Santa es un periodo festivo que puede caer en marzo o abril, dependiendo del año, los nacimientos en estos dos meses pueden variar según como cae la Semana Santa. Para capturar este efecto, usaremos la función easter de la librería forecast que devuelve para cada mes la proporción de días de la Semana Santa que contiene (considerando solo del Viernes Santo al Domingo de Resurrección, tres días).\n\neaster(nacimientos)\n\n\n\n      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n2015 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2016 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2017 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2018 0.00 0.00 0.67 0.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n2019 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n\n\nAdemás, vimos en el tema de Alisado que en enero de 2011, diciembre de 2020 y febrero y marzo de 2021 el número de nacimientos era atípico.\n\nDiasMes &lt;- monthdays(nacimientos)\nSemanaSanta &lt;- easter(nacimientos)\n\nfechas &lt;- format(seq(as.Date(\"2000-1-1\"), as.Date(\"2025-06-1\"), \"month\"), \"%Y-%m\")\n\nd0111 &lt;- 1*(fechas == \"2011-01\")\nd1220 &lt;- 1*(fechas == \"2020-12\")\nd0221 &lt;- 1*(fechas == \"2021-02\")\nd0321 &lt;- 1*(fechas == \"2021-03\")\n\nauto.arima(nacimientos, \n           d = 1, \n           D = 1, \n           lambda = 0,\n           xreg = cbind(DiasMes, SemanaSanta, \n                        d0111, d1220, d0221, d0321))\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,2)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1     sma2  DiasMes  SemanaSanta    d0111    d1220   d0221\n      -0.4015  -0.7175  -0.1102   0.0321      -0.0116  -0.0721  -0.1111  0.0337\ns.e.   0.0713   0.0607   0.0589   0.0078       0.0049   0.0188   0.0211  0.0218\n       d0321\n      0.0741\ns.e.  0.0198\n\nsigma^2 = 0.0005661:  log likelihood = 678.68\nAIC=-1337.37   AICc=-1336.59   BIC=-1300.57\n\n\nIndica el modelo, \\(ARIMA_{12}(0,1,1)(0,1,2)\\), donde el coeficiente sma2 no parece ser significativo. Si esto es así, estaríamos ante el modelo de la aerolíneas. Por otro lado, parece que casi todos los coeficientes del modelo asociados a las variables de intervención son significativas.\n\n\nUna alternativa a auto.arima es la función seas de la librería seasonal. La función seas tiene como ventajas que también analiza automáticamente la conveniencia de usar la transformación logarítmica, que identifica posibles efectos calendario y valores extremos, y que suele ser más parsimoniosa que auto.arima. Su desventaja es que sólo se puede aplicar para series mensuales o trimestrales.\nLa aplicación de seas sobre la serie indica que no es necesaria la transformación logarítmica así que solicitamos la identificación automática forzando la transformación. Veamos que identificación ofrece seas:\n\n#summary(seas(nacimientos))\nsummary(seas(nacimientos, transform.function = \"log\"))\n\n\nCall:\nseas(x = nacimientos, transform.function = \"log\")\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \nWeekday            0.0027386  0.0002503  10.943  &lt; 2e-16 ***\nEaster[1]         -0.0088432  0.0035895  -2.464   0.0138 *  \nAO2010.Dec         0.0660688  0.0137365   4.810 1.51e-06 ***\nAO2020.Nov        -0.0773159  0.0150703  -5.130 2.89e-07 ***\nAO2020.Dec        -0.2174839  0.0159940 -13.598  &lt; 2e-16 ***\nAO2021.Jan        -0.1800358  0.0160473 -11.219  &lt; 2e-16 ***\nAO2021.Feb        -0.0624484  0.0150596  -4.147 3.37e-05 ***\nMA-Nonseasonal-01  0.4149448  0.0524267   7.915 2.48e-15 ***\nMA-Seasonal-12     0.7536275  0.0397069  18.980  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 306  Transform: log\nAICc:  4607, BIC:  4643  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 19.74   Shapiro (normality): 0.9936  \n\n\nEn primer lugar, la función identifica el modelo de las aerolíneas con la transformación logarítmica de Nacimientos. Además, un efecto calendario Semana Santa, un efecto calendario días laborables del mes (que podemos entender similar a nuestro efecto días del mes) y cinco meses atípicos en diciembre de 2010, noviembre y diciembre de 2020, y enero y febrero de 2021.\nTras las dos autoidentificaciones complementarias, decidimos que la identificación de partida es \\(ARIMA_{12}(0,1,1)(0,1,1) + AI\\), donde AI recoge las variables de intervención incluidas en auto.arima y seas.\n\n\nEstimación (y valores extremos)\nVamos a estimar el modelo identificado, incluidas las nuevas variables de intervención identificadas con la función seas.\n\nd1210 &lt;- 1*(fechas == \"2010-12\")\nd1120 &lt;- 1*(fechas == \"2020-11\")\nd0121 &lt;- 1*(fechas == \"2021-01\")\n\nnac.ar1 &lt;- Arima(nacimientos, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 1),\n                 lambda = 0,\n                 xreg = cbind(DiasMes, SemanaSanta, \n                              d1210,  d0111, d1120, d1220, \n                              d0121, d0221, d0321)) \nnac.ar1\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasMes  SemanaSanta   d1210   d0111    d1120    d1220\n      -0.5501  -0.7601   0.0323      -0.0124  0.0661  -0.054  -0.0824  -0.2049\ns.e.   0.0493   0.0387   0.0070       0.0046  0.0170   0.017   0.0177   0.0183\n        d0121    d0221   d0321\n      -0.1894  -0.0570  0.0226\ns.e.   0.0186   0.0185  0.0177\n\nsigma^2 = 0.0004156:  log likelihood = 726.57\nAIC=-1429.14   AICc=-1428.03   BIC=-1384.98\n\n\nYa tenemos un modelo de partida, aunque parece que el coeficiente de la intervención en marzo de 2021 no es significativo. Veamos si es necesaria más intervención. Dado el elevado número de datos de la serie, es fácil que aparezcan errores elevados debido al azar y no a una causa externa. Por este motivo, vamos a trabajar con el umbral de 3 a la hora de identificar nuevos valores extremos.1\n\nerror &lt;- residuals(nac.ar1)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, 3)*sderror, \n             colour = c(\"red\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(2000, 2026, 2)) \n\nfechas[abs(error) &gt; 3 * sderror]\n\n[1] \"2016-04\"\n\n\n\n\n\n\n\n\nFigura 7: Error + Intervención\n\n\n\n\n\nSolo se observa un candidato a valor atípico, abril de 2016. Dado que se desconoce qué pudo pasar ese mes para que resulte atípico y que un valor atípico entra dentro de lo probable por mero azar, no crearemos una variable asociada a este mes. Además, vamos a excluir la variable de intervención de marzo de 2021 que no era significativa.\n\nnac.ar2 &lt;- Arima(nacimientos, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 1),\n                 lambda = 0,\n                 xreg = cbind(DiasMes, SemanaSanta, \n                              d1210,  d0111, d1120, \n                              d1220, d0121, d0221))\nnac.ar2\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasMes  SemanaSanta   d1210    d0111    d1120    d1220\n      -0.5483  -0.7614   0.0322      -0.0129  0.0661  -0.0541  -0.0854  -0.2093\ns.e.   0.0496   0.0386   0.0070       0.0046  0.0171   0.0170   0.0175   0.0181\n        d0121    d0221\n      -0.1952  -0.0642\ns.e.   0.0181   0.0177\n\nsigma^2 = 0.0004163:  log likelihood = 725.76\nAIC=-1429.51   AICc=-1428.57   BIC=-1389.03\n\n\n\n\nCompensación\nPodemos observar que en todos los modelos estimados los coeficientes de las variables de intervención de los meses consecutivos diciembre de 2010 y enero de 2011 son similares pero de signo opuesto. A este tipo de intervención se le denomina compensación: el efecto extraordinario en un periodo se compensa con un efecto de similar magnitud pero signo opuesto en el periodo siguiente. La causa detrás de esta compensación puede ser tan prosaica como que por error muchos nacimientos ocurridos en enero de 2011 se asignaron informáticamente a diciembre de 2010. O quizás algo pasó en esos meses que adelantó un número considerable de nacimientos.\nVamos a crear una variable de intervención asociada a esta compensación. Es tan sencillo como definir una variable ficticia que valga cero siempre excepto para los meses de diciembre de 2010 y enero de 2011 que valdrá 1 y - 1 respectivamente.\n\n# Creacion del la compensacion\nd12100111 &lt;- d1210 - d0111\n\nRespecto al efecto de la pandemia en la serie, las dos intervenciones asociadas al inicio y final del efecto (noviembre de 2020 y febrero de 2021) tienen un valor similar, y las dos intervenciones asociadas al corazón del efecto (diciembre de 2020 y enero de 2021) también tienen un valor parecido. En este caso las dos variables ficticias que vamos a crear deben valer cero siempre excepto para dos meses que valdrán 1. La primera variable valdrá 1 en noviembre de 2020 y febrero de 2021, y la segunda variable valdrá 1 en diciembre de 2020 y enero de 2021.\n\n# Variables de intervencion asociadas a la Covid-19\nd11200221 &lt;- d1120 + d0221\nd12200121 &lt;- d1220 + d0121\n\nAhora vamos a sustituir las seis variables ficticias d1210, d0111, d1120, d1220, d0121 y d0221 del modelo por las nuevas variables d12100111, d11200221 y d12200121.\n\nnac.ar3 &lt;- Arima(nacimientos, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 1),\n                 lambda = 0,\n                 xreg = cbind(DiasMes, SemanaSanta, \n                              d12100111, d11200221, d12200121))\n\nnac.ar3\n\nSeries: nacimientos \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasMes  SemanaSanta  d12100111  d11200221  d12200121\n      -0.5420  -0.7629   0.0318      -0.0128     0.0601    -0.0749    -0.2023\ns.e.   0.0496   0.0385   0.0070       0.0046     0.0105     0.0134     0.0147\n\nsigma^2 = 0.0004138:  log likelihood = 725.06\nAIC=-1434.13   AICc=-1433.62   BIC=-1404.69\n\n\nLos coeficientes estimados en este modelo son prácticamente iguales a los obtenidos en el modelo previo.\n\n\nValidación\nCoeficientes \nVeamos si todos los coeficientes del modelo son significativos.\n\ncoeftest(nac.ar3)\n\n\nz test of coefficients:\n\n              Estimate Std. Error  z value  Pr(&gt;|z|)    \nma1         -0.5420213  0.0495541 -10.9380 &lt; 2.2e-16 ***\nsma1        -0.7628787  0.0385188 -19.8054 &lt; 2.2e-16 ***\nDiasMes      0.0317655  0.0069671   4.5593 5.132e-06 ***\nSemanaSanta -0.0128472  0.0045937  -2.7967  0.005163 ** \nd12100111    0.0601140  0.0105257   5.7111 1.122e-08 ***\nd11200221   -0.0749059  0.0134142  -5.5841 2.350e-08 ***\nd12200121   -0.2023358  0.0147066 -13.7581 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTodos los coeficientes son significativos al 5%.\nMedidas de bondad de ajuste\nEl error medio de -55, muy bajo en comparación con el valor medio de la serie, y el MPE de -0.16 indican que no hay sesgo. Además, el valor tan reducido de ACF1 indica que las previsiones por intervalo estarán correctamente calculadas.\nEn media nos equivocamos en 701 nacimientos (RMSE) y el error porcentual medio es del 1.6%.\n\naccuracy(nac.ar3)\n\n\n\n                 ME   RMSE    MAE   MPE MAPE MASE ACF1\nTraining set -55.33 701.46 546.21 -0.16 1.56 0.41 0.01\n\n\n\n\nError de predicción extra-muestral según horizonte temporal\nAsumimos que se precisan quince años para hacer una buena estimación, \\(k = 180\\), y fijaremos el horizonte temporal en un año, \\(h = 12\\) meses.\nEl código es aun más complejo que el visto en el tema previo. Por un lado, hemos de tener en cuenta que hay variables de intervención de efecto calendario y por otro lado que la función Arima podría fallar en el proceso de estimación.\n\nk &lt;- 180                   \nh &lt;- 12                    \nT &lt;- length(nacimientos)   \ns&lt;-T - k - h               \n\nmapeArima &lt;- matrix(NA, s + 1, h)\n\nX &lt;- data.frame(cbind(DiasMes, SemanaSanta))\n\nfor (i in 0:s) {\n  train.set &lt;- subset(nacimientos, start = i + 1, end = i + k)\n  test.set &lt;-  subset(nacimientos, start = i + k + 1, end = i + k + h) \n  \n  X.train &lt;- as.matrix(X[(i + 1):(i + k),])\n  X.test &lt;- as.matrix(X[(i + k + 1):(i + k + h),])\n  \n  fit &lt;- try(Arima(train.set, \n                   order = c(0, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   lambda = 0,\n                   xreg = X.train))\n  \n  if(!is.element(\"try-error\", class(fit))) {\n    fcast &lt;- forecast(fit, h = h, xreg = X.test) \n    mapeArima[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n\nComo para el cálculo de los errores de predicción no se ha tenido en cuenta la intervención no sujeta a afectos calendario (la compensación a finales de 2010, la Covid-19…), vamos a obtener el error mediano como medida de precisión.\n\nerrorArima &lt;- apply(mapeArima, MARGIN = 2, FUN = median, na.rm = TRUE)\nerrorArima\n\n [1] 1.424007 1.471345 1.763494 1.928600 1.982522 1.830978 2.124116 2.175056\n [9] 2.155018 2.081567 1.862215 2.194894\n\n\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorArima), colour = \"Blue\") +\n  labs(x = \"Horizonte temporal de predicción\", y = \"\", title = \"\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\n\n\n\nFigura 8: Error de predicción (MedAPE) según horizonte temporal\n\n\n\n\n\nLa Figura 8 revela que el error de predicción aumenta según aumenta el horizonte de predicción, pero incluso a un año vista, se mantiene cercano al 2%.\n\n\nIncorrelación, Homocedasticidad y Normalidad\nVeamos ahora si el residuo es ruido blanco.\n\nerror &lt;- residuals(nac.ar3)\n\nBox.test(error, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 0.47865, df = 2, p-value = 0.7872\n\nBox.test(error, lag = 24, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 141.79, df = 24, p-value &lt; 2.2e-16\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 3.3031, df = 2, p-value = 0.1917\n\nBox.test(error^2, lag = 24, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 30.221, df = 24, p-value = 0.1775\n\njarque.bera.test(error) \n\n\n    Jarque Bera Test\n\ndata:  error\nX-squared = 0.41894, df = 2, p-value = 0.811\n\nggAcf(error, lag = 36, ylim = c(-0.3, 0.3), main = \"\")\n\n\n\n\n\n\n\nFigura 9: FAC del error del modelo\n\n\n\n\n\nClaramente hay autocorrelaciones significativas, pero en retardos no relevantes (véase Figura 9). El error muestra ser homocedástico y seguir una distribución normal.\n\n\nInterpretación\nEl modelo teórico es \\(log(nacimientos) \\sim ARIMA_{12}(0, 1, 1)(0, 1, 1) + AI\\),\n\\[(1 - L^{12})(1 - L)\\log(nacimientos_t) = (1 + \\theta_1 L)(1 + \\theta_{12} L^{12})\\varepsilon_t + AI.\\]\nSi sustituimos \\((1 - L^{12})\\log(nacimientos_t)\\) por \\(TVA\\_nac_t\\), la tasa de variación anual de los nacimientos, y desarrollamos queda \\[\n\\begin{aligned}\nTVA\\_nac_t  = & TVA\\_nac_{t - 1} + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12} + \\theta_1\\theta_{12} \\varepsilon_{t-13} +  \\varepsilon_{t}+ \\\\\n& \\gamma_1 \\cdot DiasMes_t + \\gamma_2 \\cdot SemanaSanta  + \\\\\n& \\gamma_3 \\cdot d12100111_t + \\gamma_4 \\cdot d11200221_t + \\gamma_5 \\cdot d12200121_t.\n\\end{aligned}\n\\]\nFinalmente, el modelo estimado es, \\[\n\\begin{aligned}\n\\widehat{TVA}\\_nac_t = & TVA\\_nac_{t-1}  - 0.54\\varepsilon_{t-1} - 0.76\\varepsilon_{t-12} + 0.41\\varepsilon_{t-13} + \\\\\n& 0.032 \\cdot DiasMes_t - 0.013 \\cdot SemanaSanta +  \\\\\n& + 0.060 \\cdot d12100111_t - 0.075 \\cdot d11200221_t - 0.20  \\cdot d12200121_t.\n\\end{aligned}\n\\]\n\nEn cada mes, la tasa de variación anual de los nacimientos es la misma que la del mes pasado (\\(\\widehat{TVA}\\_nac_t = TVA\\_nac_{t-1}\\)).\nAdemás, si algunos de los meses necesarios para predecir fue anómalo, el error hay que tenerlo en cuenta para afinar la previsión.\nRespecto de los efectos calendario, cada día adicional de un mes se incrementan los nacimientos un 3.2%. El mes en que cae la Semana Santa los nacimientos caen un 1.3%.\nEn diciembre de 2010 hubo un 6% más de nacimientos de lo esperado que fue compensado en enero de 2011.\nRespecto de la pandemia, nueve meses después del confinamiento hubo una reducción transitoria en el número de nacimientos. En concreto, en noviembre de 2020 se redujeron un 7.5% y en los dos meses siguientes la caída fue de un 20%. En febrero de 2021 los nacimientos se recuperaron parcialmente y la caída fue del 7.5%. A partir de marzo de 2021 los nacimientos recuperaron los valores prepandemia.\n\n\n\nPredicción de la serie\nUna vez dado por válido el modelo podemos pasar a realizar predicciones. Hay que tener en cuenta que hay cinco variables de intervención. Para las dos que son efectos calendario (DiasMes y SemanaSanta) debemos indicar qué valores tomarán en el periodo de predicción. Vamos a fijar el horizonte de predicción desde julio de 2025 hasta diciembre de 2029, 54 meses y mostrar los resultados numéricamente (solo para el primer año) y gráficamente (Figura 10).\n\ntmp &lt;- ts(rep(0, 54), start = c(2025, 7), freq = 12)\npdm &lt;- monthdays(tmp)\npss &lt;- easter(tmp)\n\npnac.ar3 &lt;- forecast(nac.ar3, \n                     h = 54,\n                     xreg = cbind(pdm, pss, \n                                  rep(0,54), rep(0,54), rep(0,54)), \n                     level = 95)\n\npnac.ar3\n\n\n\n         Point Forecast    Lo 95    Hi 95\nJul 2025       27903.51 26812.87 29038.50\nAug 2025       27934.65 26736.11 29186.92\nSep 2025       27690.77 26406.14 29037.89\nOct 2025       28340.07 26933.90 29819.66\nNov 2025       27035.00 25612.09 28536.96\nDec 2025       26748.36 25264.62 28319.22\nJan 2026       26768.21 25211.42 28421.13\nFeb 2026       24064.03 22602.93 25619.57\nMar 2026       26238.46 24581.17 28007.49\nApr 2026       24892.97 23262.27 26637.98\nMay 2026       25856.65 24104.48 27736.19\nJun 2026       25659.49 23864.85 27589.09\n\n\n\nautoplot(pnac.ar3, \n     ylab = \"Nacimientos\",\n     main = \"\") +\n  scale_x_continuous(breaks= seq(2000, 2030, 2)) \n\n\n\n\n\n\n\nFigura 10: Nacimientos (2000-2025) y predicción (2025-2029)"
  },
  {
    "objectID": "03-05-Tema5.html#exportaciones",
    "href": "03-05-Tema5.html#exportaciones",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "3.2 Exportaciones",
    "text": "3.2 Exportaciones\nConsideremos la serie de exportaciones de bienes desde España hacía la UE27 (conjunto de 27 países de la Unión Europea, con Reino Unido ya ha excluido). La serie va de enero de 1999 hasta julio de 2025 y está en millones de euros.\n\nexportaciones &lt;- read.csv(\"./series/Exportaciones.csv\", \n                           header = TRUE)\n\nexportaciones &lt;- ts(exportaciones,\n                    start = c(1999, 1),\n                    freq = 12)\n\nautoplot(exportaciones,\n         xlab = \"\",\n         ylab = \"Millones de €\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 11: Exportaciones de España a la EU27\n\n\n\n\n\n\nTransformación de la serie\nLa Figura 11 deja claro que la serie debe ser diferenciada para ser estacionaria. También muestra dos periodos con una marcada intervención: durante la Gran Recesión (2008-2014) y durante el periodo más duro de la Covid-19 en el año 2020. Además, por la naturaleza de la serie es previsible que exista un efecto días del mes o días laborables y un efecto Semana Santa.\nPor otro lado, la Figura 11 muestra que la serie tiene un esquema multiplicativo, así que trabajaremos con la transformación logarítmica.\n\nggAcf(log(exportaciones), lag = 48, ylim = c(-1, 1))\nggAcf(diff(log(exportaciones)), lag = 48, ylim = c(-1, 1))\nggAcf(diff(log(exportaciones), lag = 12), lag = 48, ylim = c(-1, 1))\nggAcf(diff(diff(log(exportaciones), lag = 12)), lag = 48, ylim = c(-1, 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Serie\n\n\n\n\n\n\n\n\n\n\n\n(b) Diferencia regular\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Diferencia estacional\n\n\n\n\n\n\n\n\n\n\n\n(d) Diferencia regular y estacional\n\n\n\n\n\n\n\nFigura 12: FAC para Exportaciones (log)\n\n\n\n\n\nndiffs(log(exportaciones))\n\n[1] 1\n\nnsdiffs(log(exportaciones))       \n\n[1] 1\n\n\nEl análisis de la Figura 12 revela la necesidad de la doble diferenciación, que es confirmada por las funciones ndiffs y nsdiffs. Concluimos que para que la serie sea estacionaria y ergódica es necesaria la doble diferenciación regular y estacional. Es decir, trabajaremos con la siguiente serie transformada \\[\\nabla\\nabla_{12}\\log(exportaciones_t) \\sim I(0)I_{12}(0).\\]\n\n\nIdentificación\nSi probamos con la función auto.arima, indicando la doble diferenciación y añadiendo el efecto de los días del mes y la Semana Santa, nos sugiere \\(ARIMA_{12}(1,1,1)(2,1,2)\\). Demasiado complejo.\nVeamos qué nos indica seas.\n\nsummary(seas(exportaciones))\n\n\nCall:\nseas(x = exportaciones)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \nMon               -0.001609   0.003777  -0.426  0.67016    \nTue                0.015174   0.003731   4.067 4.75e-05 ***\nWed                0.008041   0.003729   2.157  0.03103 *  \nThu                0.011527   0.003702   3.114  0.00185 ** \nFri                0.010624   0.003706   2.866  0.00415 ** \nSat               -0.016715   0.003675  -4.548 5.42e-06 ***\nEaster[1]         -0.068249   0.007398  -9.226  &lt; 2e-16 ***\nLS2008.Dec        -0.187822   0.030765  -6.105 1.03e-09 ***\nAO2020.Mar        -0.168210   0.030511  -5.513 3.52e-08 ***\nAO2020.Apr        -0.495703   0.031731 -15.622  &lt; 2e-16 ***\nAO2020.May        -0.268054   0.030502  -8.788  &lt; 2e-16 ***\nMA-Nonseasonal-01  0.402403   0.051656   7.790 6.70e-15 ***\nMA-Seasonal-12     0.662797   0.043115  15.373  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 319  Transform: log\nAICc:  4596, BIC:  4647  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 42.95 * Shapiro (normality): 0.9965  \nMessages generated by X-13:\nWarnings:\n- At least one visually significant trading day peak has been\n  found in one or more of the estimated spectra.\n\n\nEl modelo identificado es el de las aerolíneas para la transformación logarítmica de Exportaciones. Respecto de la intervención, identifica varios efectos calendario: seis asociados a cada día laborable de lunes a sábado, y otro a la Semana Santa. También se identifican cuatro intervenciones no asociadas a efectos calendario: tres pulsos –intervenciones que afectan un solo mes (AO)– y un cambio permanente –intervención que afectan un rango elevado de meses (LS):\n\nAsociados a la Covid-19 están los pulsos de marzo, abril y mayo de 2020.\nEl cambio permanente empieza en diciembre de 2008 y el coeficiente estimado es negativo. Es decir, la Gran Recesión generó una caída permanente de las exportaciones españolas a la UE27.\n\n\n\nEstimación (y valores extremos)\nVamos a estimar el modelo identificado con seas, incluidas las variables de intervención. A este respecto unas palabras sobre como obtener los días laborables de un mes.\nEntenderemos por días laborables los lunes a viernes de cada mes, menos los días festivos. Un inconveniente de este efecto es que los festivos que afectan a una serie dependen de su naturaleza y ámbito geográfico. Por ejemplo, en Estados Unidos el día del trabajador se celebra el primer lunes de septiembre, en Reino Unido el primer lunes de mayo y en España el 1 de mayo. Además, para la serie de transporte de pasajeros urbanos de Valencia los festivos relevantes serán diferentes que para la misma serie para Madrid.\nPara el ámbito geográfico nacional, los días laborables se puede obtener con la función bizdays. Esta función devuelve el número de días laborables de cada mes para determinados centros financieros (equivalentes a países). Por proximidad geográfica, usaremos el calendario de Londres para España.2\n\nDiasLaborables &lt;- bizdays(exportaciones, FinCenter = \"London\")\nSemanaSanta &lt;- easter(exportaciones)\n\nPara los pulsos, que solo afectan un mes, se crea una variable que vale cero excepto para el mes a intervenir que vale 1.\n\nfechas &lt;- format(seq(as.Date(\"1999-01-01\"), as.Date(\"2025-07-01\"), \"month\"), \"%Y-%m\")\nd0320 &lt;- 1*(fechas == \"2020-03\")\nd0420 &lt;- 1*(fechas == \"2020-04\")\nd0520 &lt;- 1*(fechas == \"2020-05\")\n\nPara el cambio permanente que afectan desde un mes en adelante, se crea una variable que vale cero antes del mes de inicio de la intervención y 1 desde ese mes en adelante.\n\nl1208 &lt;- 1*(fechas &gt; \"2008-11\")\n\nEstimamos el modelo de partida, en el que parece que todos los coeficientes son significativos.\n\nexp.ar1 &lt;- Arima(exportaciones, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 1),\n                 lambda = 0,\n                 xreg = cbind(DiasLaborables, SemanaSanta, \n                              l1208, d0320, d0420, d0520))\nexp.ar1\n\nSeries: exportaciones \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  SemanaSanta    l1208    d0320    d0420\n      -0.4572  -0.6832          0.0314      -0.0219  -0.1958  -0.1802  -0.4991\ns.e.   0.0451   0.0451          0.0018       0.0090   0.0321   0.0327   0.0338\n        d0520\n      -0.2638\ns.e.   0.0330\n\nsigma^2 = 0.001573:  log likelihood = 553.8\nAIC=-1089.6   AICc=-1088.99   BIC=-1056.09\n\n\nVeamos si es necesaria más intervención.\n\nerror &lt;- residuals(exp.ar1)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2,2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1999, 2025, 2))\n\n\n\n\n\n\n\nFigura 13: Error + Intervención\n\n\n\n\n\nSe observan algunos candidatos a valores atípicos por superar el error las 2.5 desviaciones típicas, pero ninguna alcanza las 3 desviaciones típicas. Por tanto, no se van a incluir más variables de intervención.\n\n\nValidación\nCoeficientes significativos\nVeamos si los coeficientes del modelo son significativos. Para ello, aplicamos la prueba z.\n\ncoeftest(exp.ar1)\n\n\nz test of coefficients:\n\n                 Estimate Std. Error  z value  Pr(&gt;|z|)    \nma1            -0.4571716  0.0450707 -10.1434 &lt; 2.2e-16 ***\nsma1           -0.6832458  0.0451119 -15.1456 &lt; 2.2e-16 ***\nDiasLaborables  0.0314312  0.0018308  17.1682 &lt; 2.2e-16 ***\nSemanaSanta    -0.0218979  0.0090384  -2.4228    0.0154 *  \nl1208          -0.1958303  0.0321333  -6.0943 1.099e-09 ***\nd0320          -0.1801776  0.0327318  -5.5047 3.699e-08 ***\nd0420          -0.4991418  0.0337858 -14.7737 &lt; 2.2e-16 ***\nd0520          -0.2638271  0.0329901  -7.9972 1.273e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTodos los coeficientes son significativos.\nMedidas de bondad de ajuste\nEn media nos equivocamos en 530 millones de euros (RMSE) y el error porcentual medio es del 2.9%, muy reducido.\nLa serie no presenta sesgo y el intervalo de confianza para las predicciones es válido.\n\naccuracy(exp.ar1)\n\n\n\n                 ME   RMSE    MAE   MPE MAPE MASE ACF1\nTraining set -33.25 529.85 366.78 -0.35 2.94 0.34 -0.1\n\n\nError de predicción extramuestral según horizonte temporal\nAsumimos que se precisan diez años para hacer una buena estimación, \\(k = 120\\), y fijaremos el horizonte temporal en un año, \\(h = 12\\) meses. Usaremos como criterio de calidad el MedAPE (error mediano) para evitar el efecto sobre los errores de predicción de la no inclusión de las variables ficticias no asociadas a efectos calendario.\n\nk &lt;- 120                   \nh &lt;- 12                    \nT &lt;- length(exportaciones)   \ns &lt;- T - k - h               \n\nmapeArima &lt;- matrix(NA, s + 1, h)\n\nX &lt;- data.frame(cbind(DiasLaborables, SemanaSanta))\n\nfor (i in 0:s) {\n  train.set &lt;- subset(exportaciones, start = i + 1, end = i + k)\n  test.set &lt;-  subset(exportaciones, start = i + k + 1, end = i + k + h) \n  \n  X.train &lt;- as.matrix(X[(i + 1):(i + k),])\n  X.test &lt;- as.matrix(X[(i + k + 1):(i + k + h),])\n  \n  fit &lt;- try(Arima(train.set, \n                   order = c(0, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   lambda = 0,\n                   xreg = X.train))\n  \n  if(!is.element(\"try-error\", class(fit))) {\n    fcast &lt;- forecast(fit, h = h, xreg = X.test) \n    mapeArima[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n\nerrorArima &lt;- apply(mapeArima, MARGIN = 2, FUN = median, na.rm = TRUE)\nerrorArima\n\n [1] 2.652815 2.615755 2.987029 3.479416 3.681393 4.182916 4.902910 5.067910\n [9] 5.060069 4.888785 5.385392 5.521236\n\n\nLa Figura 14 revela que el error de predicción aumenta lentamente según aumenta el horizonte de predicción, pasando del 2.6% a un mes vista hasta el 5.5% a 12 meses vista.\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorArima), colour = \"Blue\") +\n  labs(x = \"Horizonte temporal de predicción\", y = \"\", title = \"\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\n\n\n\nFigura 14: Error de predicción (MedAPE) según horizonte temporal\n\n\n\n\n\nIncorrelación, Homocedasticidad y Normalidad\nVeamos ahora si el residuo es ruido blanco.\n\nBox.test(error, lag = 2,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 1.5401, df = 2, p-value = 0.463\n\nBox.test(error, lag = 24,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 38.304, df = 24, p-value = 0.03226\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 4.5829, df = 2, p-value = 0.1011\n\nBox.test(error^2, lag = 24, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 68.84, df = 24, p-value = 3.272e-06\n\njarque.bera.test(error) \n\n\n    Jarque Bera Test\n\ndata:  error\nX-squared = 0.45772, df = 2, p-value = 0.7954\n\n\nEl error muestra ser incorrelado (al menos al 1%) y seguir una distribución normal. Sin embargo, el error es heterocedástico. Es posible que un análisis más detallado de la intervención corrija este problema. Si no es así, habría que optar por un modelo que no asuma la hipótesis de homocedasticidad. En cualquier caso, la falta de homocedasticidad en el error del modelo no afecta a la consistencia de la predicciones, pero si al cálculo del intervalo de confianza.\n\n\nInterpretación\nLa parte regular del modelo estimado es la misma que la obtenida para la serie Nacimientos y su interpretación es, por tanto, idéntica: en cada mes, la tasa de variación anual de las exportaciones es la misma que la del mes pasado. Además, si algunos de los meses necesarios para predecir fue anómalo, el error hay que tenerlo en cuenta para afinar la previsión.\nVamos por tanto a centrarnos en la interpretación de la intervención:\n\nCada día laborable adicional en un mes aumenta las exportaciones en un 3.1%.\nEl mes en que cae la Semana Santa las exportaciones caen un 2.2%.\nA raíz de la Gran Recesión, las exportaciones se redujeron de forma permanente un 19.6% desde diciembre de 2008. Es decir, sin la Gran Recesión, las exportaciones ahora serían un 19.6% superiores.\nA raíz de la Covid-19, las exportaciones se redujeron de forma temporal en los meses de marzo, abril y mayo de 2020 un 18%, 50% y 26%, respectivamente.\n\n\n\nPredicción de la serie\nUna vez dado por válido el modelo, podemos pasar a realizar predicciones teniendo en cuenta las seis variables de intervención:\n\nDos de ellas son efectos calendario (DiasLaborables y SemanaSanta), para las que debemos indicar qué valores tomarán en el periodo de predicción\nOtra es un cambio permanente y su valor debe ser 1 en el futuro.\nPara los dos pulsos se fijará un valor futuro de 0.\n\nVamos a fijar el horizonte de predicción desde agosto de 2025 hasta diciembre de 2028 (41 meses) y mostrar los resultados numérica (solo para el primer año) y gráficamente (Figura 15).\nRecuerda siempre incluir las variables ficticias en la función forecast en el mismo orden que aparecen en la estimación con Arima.\n\ntmp &lt;- ts(rep(0, 41), start = c(2025, 8), freq = 12)\npdl &lt;- bizdays(tmp, FinCenter = \"London\")\npss &lt;- easter(tmp)\n\npexp.ar1 &lt;- forecast(exp.ar1, \n                     h = 41,\n                     xreg = cbind(pdl, pss, \n                                  rep(1,41), \n                                  rep(0,41), rep(0,41), rep(0,41)), \n                     level = 95)\n\npexp.ar1\n\n\n\n         Point Forecast    Lo 95    Hi 95\nAug 2025       15699.97 14525.79 16969.07\nSep 2025       22101.47 20230.60 24145.35\nOct 2025       22298.97 20217.39 24594.87\nNov 2025       21264.82 19112.80 23659.15\nDec 2025       20452.05 18235.07 22938.57\nJan 2026       20572.85 18205.48 23248.06\nFeb 2026       21137.64 18573.36 24055.94\nMar 2026       24062.72 21002.20 27569.23\nApr 2026       21363.10 18527.10 24633.21\nMay 2026       22037.24 18995.16 25566.50\nJun 2026       23478.34 20118.75 27398.94\nJul 2026       21959.46 18710.98 25771.91\n\n\n\nautoplot(pexp.ar1, \n         xlab = \"\",\n         ylab = \"Millones de euros\",\n         main = \"\",\n         PI = FALSE) +\n  scale_x_continuous(breaks= seq(1999, 2029, 2)) \n\n\n\n\n\n\n\nFigura 15: Exportaciones (1999-2025) y predicción (2025-2028)"
  },
  {
    "objectID": "03-05-Tema5.html#demanda-eléctrica",
    "href": "03-05-Tema5.html#demanda-eléctrica",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "3.3 Demanda eléctrica",
    "text": "3.3 Demanda eléctrica\nConsideremos las serie diaria de demanda eléctrica (GWh) en España durante 2024.\n\nelectricidad &lt;- read.csv(\"./series/Consumo electrico.csv\", \n                         header = TRUE)\n\nelectricidad &lt;- ts(electricidad[, 1],\n                   start = c(1, 1),\n                   frequency = 7)\n\nautoplot(electricidad,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 16: Demanda eléctrica en España en 2024\n\n\n\n\n\n\nTransformación de la serie\nEstrictamente hablando la serie no muestra tendencia, porque solo podemos observar un año. Sin embargo, se observan cambios de nivel en la demanda eléctrica que se pueden confundir con la presencia de tendencia, pero que realmente están asociados a los cambios de temperatura y el uso de los sistemas de climatización. Estos cambios de nivel se repiten cada año y se deberían incorporar en la estructura de la serie como una segunda componente estacional. Ahora bien, dado que solo se está analizando un año de la serie, vamos a asimilar los cambios de nivel a la presencia de tendencia y a considerar, por tanto, la serie como no estacionaria.\n\nggAcf(electricidad, lag = 42, ylim = c(-1, 1))\nggAcf(diff(electricidad), lag = 42, ylim = c(-1, 1))\nggAcf(diff(electricidad, lag = 7), lag = 42, ylim = c(-1, 1))\nggAcf(diff(diff(electricidad, lag = 7)), lag = 42, ylim = c(-1, 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Serie\n\n\n\n\n\n\n\n\n\n\n\n(b) Diferencia regular\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Diferencia estacional\n\n\n\n\n\n\n\n\n\n\n\n(d) Diferencia regular y estacional\n\n\n\n\n\n\n\nFigura 17: FAC para Electricidad\n\n\n\n\n\nndiffs(electricidad)\n\n[1] 0\n\nnsdiffs(electricidad)       \n\n[1] 1\n\n\nLa FAC muestra que para que la serie sea estacionaria y ergódica es necesaria la doble diferenciación regular y estacional, aunque la función ndiffs no indica que la diferenciación regular sea necesaria. Trabajaremos con la siguiente serie transformada \\[\\nabla\\nabla_{12}electricidad_t \\sim I(0)I_{12}(0).\\]\n\n\nIdentificación\nAntes de proceder con la primera autoidentifación, vamos a analizar que intervención puede ser necesaria para estimar y predecir adecuadamente la serie.\nEfectos calendario\nLa Figura 16 muestra que los días festivos hay una marcada caída de la demanda de electricidad (véase el análisis de esta serie realizada en el Tema 2. Por ejemplo, destaca el bajo consumo el 1 de enero (primer dato de la serie), en Navidades o en Semana Santa (semana 13 del año).\nVamos a asumir que la caída en el consumo depende del día de la semana. Es decir, que en un martes festivo la caída del consumo será mayor que la de un domingo festivo porque en el segundo caso el consumo ya es de por si muy bajo.\nPor tanto, para realizar un análisis detallado de la serie vamos crear variables ficticias que identifiquen los días festivos del calendario. Además, vamos a distinguir si el festivo ha sido entre semana (de lunes a viernes), o en fin de semana (sábado ydomingo).3\nEl siguiente código hace todo el trabajo.\n\nLa primer comando crea una serie que identifica cada fecha del año 2024. La función seq devuelve un objeto date donde cada fecha no se guarda como un texto (aunque se muestre así) sino de un formato fecha mas compleja.\nEn el segundo comando el objeto fiestas guarda los 12 días festivos del año: 11 fiestas nacionales y el lunes 1 de abril, festivo en muchas comunidades.\nEn estos dos primeros comandos, la función as.Date se usa para que R identifique una secuencia de caracteres como una fecha tipo “aaaa-mm-dd”.\nEl tercer comando genera una serie ficticia para identificar los festivos entre semana. Primero se identifican que fechas del año aparecen incluidas entre las fiestas (fechas %in% fiestas), generándose un vector booleano (TRUE/FALSE) de longitud 366. Luego se identifican que días del año han caído entre semana (cycle(electricidad) &lt; 6), generándose otro vector TRUE/FALSE de longitud 366. El producto de estos dos vectores solo valdrá 1 (TRUE) si simultáneamente una fecha es festiva y ha caído entre semana.\nEl último comando usa las mismas ideas para generar una serie ficticia que identifica los festivos en fin de semana.\n\n\nfechas &lt;- seq(as.Date(\"2024-01-01\"), as.Date(\"2024-12-31\"), \"day\")\n\nfiestas &lt;- as.Date(c(\"2024-01-01\", \"2024-01-06\", \"2024-03-29\", \"2024-03-31\",\n                     \"2024-04-01\", \"2024-05-01\", \"2024-08-15\", \"2024-10-12\",\n                     \"2024-11-01\", \"2024-12-06\", \"2024-12-08\", \"2024-12-25\"))\n\nfestivosEntreSemana &lt;- (fechas %in% fiestas) * (cycle(electricidad) &lt; 6)\n\nfestivosFinSemana &lt;- (fechas %in% fiestas) * (cycle(electricidad) &gt; 5)\n\nAl final el código nos devuelve dos variables ficticias: festivosEntreSemana valdrá 1 en la posición del año correspondiente a un día festivo entre semana y 0 en otro caso; festivosFinDeSemana valdrá 1 en la posición del año correspondiente a día de fin de semana festivo y 0 en otro caso.\nEfecto temperatura\nEl consumo de electricidad está fuertemente relacionado con la temperatura. La Figura 18 muestra una relación en forma de U entre la temperatura y el consumo de electricidad: cuanto más se aleja la temperatura de la media anual (por exceso de frío o de calor) mayor es el consumo eléctrico.\n\ntemperatura &lt;- read.csv(\"./series/Temperatura.csv\")\ntemperatura &lt;- temperatura[, 1]\n\nggplot() +\n  geom_point(aes(x = temperatura, y = electricidad), size = 2) +\n  xlab(\"Temperatura (ºC)\") + \n  ylab(\"Demanda eléctrica (GWh)\")\n\n\n\n\n\n\n\nFigura 18: Relación entre temperatura y consumo eléctrico\n\n\n\n\n\nVamos a crear una variable ficticia diferenciaTemperatura que valdrá para cada día la diferencia de temperatura respecto de la media anual.\n\ndiferenciaTemperatura &lt;- abs(temperatura - mean(temperatura))\n\nIdentificación\nProbamos con la función auto.arima, indicando la doble diferenciación, los días festivos entre semana, los festivos en domingo y la diferencia de temperatura respecto de la media anual.\n\nauto.arima(electricidad,\n           d = 1,\n           D = 1,\n           xreg = cbind(festivosEntreSemana, festivosFinSemana, \n                        diferenciaTemperatura))\n\nSeries: electricidad \nRegression with ARIMA(0,1,1)(0,1,1)[7] errors \n\nCoefficients:\n         ma1     sma1  festivosEntreSemana  festivosFinSemana\n      0.1734  -0.9166             -71.1134            -6.4956\ns.e.  0.0643   0.0480               4.1155             4.9844\n      diferenciaTemperatura\n                     2.3561\ns.e.                 0.4253\n\nsigma^2 = 244.6:  log likelihood = -1496.32\nAIC=3004.63   AICc=3004.87   BIC=3027.91\n\n\nEl modelo identificado es el de la aerolíneas, ARIMA(0, 1, 1)(0, 1, 1). Respecto de la intervención,\n\nel coeficiente de la intervención festivosFinDeSemana claramente no es significativo,\nsi hay una caída significativa en el consumo cuando el festivo cae entre semana (festivosEntreSemana),\nel aumento del consumo conforme la temperatura se aleja de la media anual también es significativo.\n\n\n\nEstimación (y valores extremos)\nVamos a estimar el modelo de las aerolíneas sin tener en cuenta los festivos en fin de semana.\n\nele.ar1 &lt;- Arima(electricidad, \n                 order = c(0, 1, 1),\n                 seasonal = c(0, 1, 1),\n                 xreg = cbind(festivosEntreSemana, diferenciaTemperatura))\n\nele.ar1\n\nSeries: electricidad \nRegression with ARIMA(0,1,1)(0,1,1)[7] errors \n\nCoefficients:\n         ma1     sma1  festivosEntreSemana  diferenciaTemperatura\n      0.1837  -0.9129             -70.5577                 2.3349\ns.e.  0.0629   0.0490               4.0404                 0.4252\n\nsigma^2 = 245.3:  log likelihood = -1497.18\nAIC=3004.35   AICc=3004.53   BIC=3023.76\n\n\nVeamos si es necesaria más intervención.\n\nerror &lt;- residuals(ele.ar1)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, 3)*sderror, \n             colour = c(\"red\",  \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1, 54, 4)) \n\nfechas[abs(error) &gt; 3 * sderror]\n\n[1] \"2024-01-14\" \"2024-03-28\" \"2024-06-25\" \"2024-08-17\" \"2024-12-23\"\n[6] \"2024-12-24\"\n\n\n\n\n\n\n\n\nFigura 19: Error + Intervención\n\n\n\n\n\nSe observan seis candidatos a valores atípicos por superar el error las 3 desviaciones típicas.\n\nEl consumo es mayor de lo esperado (errores positivos) los días 25 de junio y 17 de agosto. Ambas fechas están cercanas a festivos (24 de junio San Juan, festivo en muchas zonas de España, y 15 de agosto) y esta puede ser la causa, pero no está claro.\nEl consumo es menor de lo esperado (errores negativos) los días 14 de enero, 28 de marzo (Jueves Santo), y 23 y 24 de diciembre (periodo navideño). Para las tres últimas fechas, la causa está clara, son días festivos en gran parte de España o días muy vacacionales, generando una caída en el consumo de electricidad.\n\nDeberíamos intervenir en aquellos días donde la caída del consumo está asociada a procesos festivos, pero a fin de aligerar este ejemplo no vamos a modificar el modelo actual.\n\n\nValidación\nCoeficientes significativos\nVeamos si los coeficientes del modelo son significativos. Para ello, aplicamos la prueba z.\n\ncoeftest(ele.ar1)\n\n\nz test of coefficients:\n\n                        Estimate Std. Error  z value  Pr(&gt;|z|)    \nma1                     0.183658   0.062927   2.9186  0.003516 ** \nsma1                   -0.912882   0.048986 -18.6357 &lt; 2.2e-16 ***\nfestivosEntreSemana   -70.557741   4.040400 -17.4631 &lt; 2.2e-16 ***\ndiferenciaTemperatura   2.334905   0.425190   5.4914 3.987e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTodos los coeficientes son significativos.\nMedidas de bondad de ajuste\nEn media nos equivocamos en 15 GWh (RMSE) y el error porcentual medio es del 1.6%, muy reducido.\nLa serie no presenta sesgo y el intervalo de confianza para las predicciones es válido.\n\naccuracy(ele.ar1)\n\n\n\n                ME RMSE   MAE   MPE MAPE MASE ACF1\nTraining set -1.17 15.4 11.12 -0.18 1.64  0.4    0\n\n\nError de predicción extramuestral según horizonte temporal\nAsumimos que se precisan veinte semanas para hacer una buena estimación, \\(k = 140\\), y fijaremos el horizonte temporal en una semana, \\(h = 7\\) días. Como el modelo apenas presenta valores atípicos, podemos calcular como medida de precisión el valor medio en lugar del mediano.\n\nk &lt;- 140               \nh &lt;- 7               \nT &lt;- length(electricidad)   \ns &lt;- T - k - h               \n\nmapeArima &lt;- matrix(NA, s + 1, h)\n\nX &lt;- data.frame(cbind(festivosEntreSemana, diferenciaTemperatura))\n\nfor (i in 0:s) {\n  train.set &lt;- subset(electricidad, start = i + 1, end = i + k)\n  test.set &lt;-  subset(electricidad, start = i + k + 1, end = i + k + h) \n  \n  X.train &lt;- as.matrix(X[(i + 1):(i + k),])\n  X.test &lt;- as.matrix(X[(i + k + 1):(i + k + h),])\n  \n  fit &lt;- try(Arima(train.set, \n                   order = c(0, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   xreg = X.train))\n  \n  if(!is.element(\"try-error\", class(fit))) {\n    fcast &lt;- forecast(fit, h = h, xreg = X.test) \n    mapeArima[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n\nerrorArima &lt;- colMeans(mapeArima, na.rm = TRUE)\nerrorArima\n\n[1] 1.634380 2.442403 2.897292 3.199880 3.327570 3.465907 3.557186\n\n\nEl error de predicción parte del 1.6% para previsiones a un día vista (algo inferior al error de ajuste) y aumenta progresivamente según aumenta el horizonte de predicción, pasando al 3.6% a 7 días vista.\nIncorrelación, Homocedasticidad y Normalidad\nVeamos ahora si el residuo es ruido blanco.\n\nBox.test(error, lag = 2,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 1.3988, df = 2, p-value = 0.4969\n\nBox.test(error, lag = 14,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 40.578, df = 14, p-value = 0.0002072\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 13.178, df = 2, p-value = 0.001376\n\nBox.test(error^2, lag = 14, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 22.483, df = 14, p-value = 0.06921\n\njarque.bera.test(error) \n\n\n    Jarque Bera Test\n\ndata:  error\nX-squared = 170.83, df = 2, p-value &lt; 2.2e-16\n\nggAcf(error, lag = 35, main = \"\")\n\n\n\n\n\n\n\nFigura 20: FAC del error del modelo\n\n\n\n\n\nClaramente hay autocorrelaciones significativas, además en retardos relevantes (véase Figura 20), el error no muestra ser homocedástico ni seguir una distribución normal.\nUna posible razón para la presencia de autocorrelación y heterocedasticidad puede ser una identificación del modelo incorrecta: igual la diferenciación regular no es precisa o igual hay que dejar el proceso AR(1) regular aunque su coeficiente sea no significativo. También la presencia de días atípicos sin intervención puede ser la causa del incumplimiento de las hipótesis.\n\n\nInterpretación\nEl modelo teórico es \\(electricidad_t \\sim ARIMA_{7}(0, 1, 1)(0, 1, 1) + AI\\), modelo de las aerolíneas con intervención. Como ya hemos visto el desarrollo de este modelo teórico en el apartado 4.5, pasamos directamente al modelo estimado: \\[\n\\begin{aligned}\n\\widehat{electricidad_t}  = & electricidad_{t-1} + (electricidad_{t-7} - electricidad_{t-8}) +\\\\\n&  0.18 \\varepsilon_{t-1} - 0.91 \\varepsilon_{t-7} - 0.17 \\varepsilon_{t-8}   \\\\\n& -70.6 \\cdot festivos_t + 2.3 \\cdot diferenciaTemperatura_t\n\\end{aligned}\n\\]\n\nEl consumo de electricidad de un día es el mismo que el del día anterior más la variación observada entre estos dos días la semana pasada. Por ejemplo, el consumo un miércoles es el del martes pasado más la variación observada entre el martes y el miércoles de la semana previa.\nLos días festivos entre semana el consumo cae en 70.6 GWh respecto de la demanda en un día no festivo entre semana.\nPor cada grado centígrado que la temperatura se aleja de la media anual, el consumo aumenta en 2.3 GWh.\n\n\n\nPredicción de la serie\nUna vez dado por válido el modelo, podemos pasar a realizar predicciones para los próximos siete días y mostrar los resultados numérica y gráficamente (Figura 21).\nTeniendo en cuenta las dos variables de intervención:\n\nPara el efecto calendario debemos indicar qué valores tomará en el periodo de predicción, es decir, que días son festivos entre semana. Como el rango de predicción abarca del 1 al 7 de enero de 2025, se marcarán como festivos el miércoles día 1 (Año nuevo) y el lunes 6 (Reyes).\nPara el efecto de la temperatura deberíamos, primero, obtener la temperatura prevista para el periodo de predicción y, después, calcular el exceso o defecto de temperatura respecto de la media anual. La previsión de la temperatura se puede obtener de AEMET o de cualquier otro servicio de metereología.\n\nRecuerda siempre incluir las variables ficticias en la función forecast en el mismo orden que aparecen en la estimación con Arima.\n\npfestivos &lt;- c(1, 0, 0, 0, 0, 1, 0)\nptemperatura &lt;- c(4.6, 5.0, 3.8, 3.5, 7.8, 8.8, 6.3)\npdiferenciaTemperatura &lt;- abs(ptemperatura - mean(temperatura))\n\npele.ar1 &lt;- forecast(ele.ar1, \n                     h = 7,\n                     xreg = cbind(pfestivos, pdiferenciaTemperatura), \n                     level = 95)\n\npele.ar1\n\n         Point Forecast    Lo 95    Hi 95\n53.28571       593.9934 563.2978 624.6889\n53.42857       664.9575 617.3938 712.5211\n53.57143       658.5071 598.6541 718.3602\n53.71429       585.0029 514.9852 655.0206\n53.85714       541.6158 462.7326 620.4991\n54.00000       566.0608 479.2124 652.9093\n54.14286       652.7118 558.5697 746.8540\n\n\n\nautoplot(pele.ar1, \n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\") +\n         xlim(46, 55)\n\n\n\n\n\n\n\nFigura 21: Demanda eléctrica y predicción\n\n\n\n\n\nLa Figura 21 muestra las últimas semanas de 2024 para la serie Electricidad y su predicción para la primera semana de 2025, junto con el intervalo de confianza. La calidad de ajuste es tan buena que el intervalo de confianza de las predicciones es muy estrecho."
  },
  {
    "objectID": "03-05-Tema5.html#comparación-con-el-método-de-alisado-exponencial",
    "href": "03-05-Tema5.html#comparación-con-el-método-de-alisado-exponencial",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "3.4 Comparación con el método de Alisado exponencial",
    "text": "3.4 Comparación con el método de Alisado exponencial\nVeamos una comparativa, para los dos ejemplos vistos, entre los resultados obtenidos con ARIMA y con Alisado exponencial\n\nNacimientos (log)\n\n\n\n\n\n\n\n\n\n\n\nMétodo\nModelo\nMAPE\n\n\n\n\n\n\n\n\n\nAjuste\nExtra h = 1\nExtra h = 6\nExtra h =12\n\n\nARIMA\n(0,1,1)(0,1,1)+AI\n1.56\n1.42\n1.83\n2.19\n\n\nAlisado\n(A,N,A)\n1.99\n2.41\n3.41\n4.12\n\n\n\nPara Nacimientos, la mejora en los indicadores de calidad con ARIMA respecto de Alisado es muy reducida para predicciones a unos pocos meses vista, pero supera los dos puntos porcentuales para predicciones a 12 meses vista.\n\n\nExportaciones (log)\n\n\n\n\n\n\n\n\n\n\n\nMétodo\nModelo\nMAPE\n\n\n\n\n\n\n\n\n\nAjuste\nExtra h = 1\nExtra h = 6\nExtra h =12\n\n\nARIMA\n(0,1,1)(0,1,1)+AI\n2.94\n2.65\n4.18\n5.52\n\n\nAlisado\n(A,Ad,A)\n4.90\n3.70\n4.78\n5.62\n\n\n\nEn el caso de Exportaciones, la mejora en la calidad con ARIMA respecto de Alisado es de un punto para predicciones a un periodo vista, pero nula para predicciones a 12 meses vista. ARIMA ofrece mejores resultados que Alisado solo para predicciones a corto plazo.\n\n\nElectricidad\n\n\n\n\n\n\n\n\n\n\n\nMétodo\nModelo\nMAPE\n\n\n\n\n\n\n\n\n\nAjuste\nExtra h = 1\nExtra h = 4\nExtra h =7\n\n\nARIMA\n(0,1,1)(0,1,1)+AI\n1.64\n1.63\n3.20\n3.56\n\n\nAlisado\n(A,Ad,A)\n2.17\n2.07\n3.70\n4.18\n\n\n\nEn el caso de Electricidad, la mejora en la calidad de las predicciones con ARIMA respecto de Alisado es inferior al punto porcentual. El uso de los modelos Arima no parece muy justificado, más allá de poder corregir las predicciones en los días festivos."
  },
  {
    "objectID": "03-05-Tema5.html#footnotes",
    "href": "03-05-Tema5.html#footnotes",
    "title": "Series Temporales: Procesos ARIMA con estacionalidad",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara una serie con 306 observaciones, como Nacimientos, por simple azar puede haber 14 errores del modelo que superen el umbral de las dos desviaciones típicas, o 4 observaciones que superen el umbral de 2.5 d.t. pero solo una debería superar el umbral de las 3 d.t.↩︎\nR permite crear tu propio calendario de festivos y existen otras librerías que extienden las opciones de bizdays. En el ejemplo de Pasajeros puedes ver como construir la serie de días laborables del mes para España paso a paso.↩︎\nPara este análisis solo se van a considerar los festivos nacionales en 2024 al que añadiremos el lunes de Semana Santa. Es posible que algunos festivos autonómicos en comunidades muy pobladas tengan un efecto sobre el consumo eléctrico, pero esto es un ejercicio y hay que limitar el alcance del análisis.↩︎"
  },
  {
    "objectID": "03-09-Ejemplo4.html",
    "href": "03-09-Ejemplo4.html",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "",
    "text": "1 Introducción\nConsideremos la serie temporal correspondiente al número de defunciones causadas por enfermedades cerebrovasculares, Esta serie está disponible en el Instituto Nacional de Estadística desde enero de 1980 hasta diciembre de 2024, un total de 45 años o 540 meses.\nLa descomposición revelaba la presencia de varios valores atípicos concentrados al inicio de la serie. Por este motivo, para el análisis por modelos Arima sin estacionalidad, vamos a recortar la serie, que empezará en enero de 1990, y a anualizarla (35 años). La Figura 1 muestra la serie a analizar.\n\nDefEnfCer &lt;- read.csv2(\"./series/Enfermedades cerebrovasculares.csv\", \n                       header = TRUE)\n\nDefEnfCer &lt;- ts(DefEnfCer[,2],\n                start = 1980, \n                freq = 12)\n\nDefEnfCer &lt;- window(DefEnfCer, \n                    start = 1990)\n\nDefEnfCer &lt;- aggregate(DefEnfCer, \n                       FUN = sum)\n\nautoplot(DefEnfCer,\n         xlab = \"\",\n         ylab = \"\",\n         main = \"\") \n\n\n\n\n\n\n\nFigura 1: Defunciones anuales causadas por enfermedades cerebrovasculares\n\n\n\n\n\n\n\n\n\n\n2 Transformación de la serie\nEn el análisis previo se vio que la serie anual hay que diferenciarla una vez para que sea estacionaria y ergódica. Por tanto, se opta por considerar \\(d=1\\) o \\(y_t \\sim I(1)\\).\nPara series anuales la transformación logarítmica no suele ser necesaria.\n\n\n\n\n\n3 Identificación\nVeamos ahora a identificar los valores de \\(p\\) y \\(q\\) a partir de la FAC y la FACP (Figura 2). Difícil decidirse entre un AR(1), un MA(1) o incluso un ARMA(1, 1).\n\nggtsdisplay(diff(DefEnfCer))\n\n\n\n\n\n\n\nFigura 2: Defunciones anuales por enfermedades cerebrovasculares, FAC y FACP de la serie transformada\n\n\n\n\n\n¿Qué recomienda auto.arima? Un ARIMA(1, 1, 0) con deriva.\n\nauto.arima(DefEnfCer, \n           d = 1)\n\nSeries: DefEnfCer \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1      drift\n      -0.6075  -616.3755\ns.e.   0.1366    80.8659\n\nsigma^2 = 596511:  log likelihood = -273.52\nAIC=553.04   AICc=553.84   BIC=557.62\n\n\nLa estimación de este modelo muestra que los dos coeficientes son aparentemente significativos (el coeficiente supera las dos errores estándar) y que no hay valores extremos (Figura 3).\n\narima110 &lt;- Arima(DefEnfCer, \n                  order = c(1, 1, 0),\n                  include.constant = TRUE)\n\nerror &lt;- residuals(arima110)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1990, 2024, 4)) \n\n\n\n\n\n\n\nFigura 3: Error + Intervención\n\n\n\n\n\n\n\n\n\n\n4 Validación\n\nCoeficientes son significativos\nLos dos coeficientes estimados (\\(\\phi_1\\) y \\(\\mu\\)) son significativos.\n\ncoeftest(arima110)\n\n\nz test of coefficients:\n\n        Estimate Std. Error z value  Pr(&gt;|z|)    \nar1     -0.60753    0.13663 -4.4464 8.730e-06 ***\ndrift -616.37546   80.86586 -7.6222 2.494e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nMedidas de error\nEl error medio es 738 defunciones (RMSE) y el error porcentual medio es 1.7% (MAPE). No hay sesgo y la predicción por intervalos será correcta.\n\naccuracy(arima110)\n\n\n\n                ME  RMSE   MAE  MPE MAPE MASE  ACF1\nTraining set 17.22 738.5 560.5 0.04 1.69 0.62 -0.07\n\n\n\n\n\n\nIncorrelación, Homocedasticidad y Normalidad\nVeamos ahora si el residuo es ruido blanco.\n\nBox.test(error, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 1.9896, df = 2, p-value = 0.3698\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\") \n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 1.383, df = 2, p-value = 0.5008\n\nshapiro.test(error)\n\n\n    Shapiro-Wilk normality test\n\ndata:  error\nW = 0.9136, p-value = 0.009367\n\n\nLas hipótesis de incorrelación y homocedasticidad se aceptan (también se aceptan para otros valores de lag razonables). Sin embargo, la hipótesis de normalidad se rechaza al 5%, lo cual no tendrá efecto sobre la calidad de las predicciones, pero puede tenerlo en el cálculo de sus intervalos de confianza.\n\n\n\n\n\n\n5 Interpretación del modelo\nEl modelo teórico es \\(Def_t \\sim ARIMA(1,1,0)\\) + constante: \\[(1 - \\phi_1L)(1 - L) Def_t =  c + \\varepsilon_t.\\]\nDesarrollando queda: \\[Def_t = c + Def_{t-1} +  \\phi_1(Def_{t-1} - Def_{t-2}) + \\varepsilon_t\\]\nFinalmente. el modelo estimado es:\n\\[\\widehat{Def}_t = -990.82 + Def_{t-1} - 0.61(Def_{t-1} - Def_{t-2})\\]\nRecuerda que la constante estimada \\(\\mu\\) no es la constante del modelo \\(c\\) y que esta se obtiene como \\((1 - (-0.6075)) \\cdot (-616.3755) = -990.8236\\).\nCada año el número de defunciones por enfermedades cerebrovasculares es el mismo que el año previo menos 991 casos y menos un 61% de la última variación.\n\n\n\n\n\n6 Predicción\nPodemos usar el modelo estimado para predecir los casos de defunciones por enfermedad cerebrovascular para los próximos 5 años (Figura 4).\n\nparima110 &lt;- forecast(arima110, \n                      h = 5, \n                      level = 95)\nparima110\n\n     Point Forecast    Lo 95    Hi 95\n2025       22185.19 20671.43 23698.95\n2026       21559.36 19933.19 23185.53\n2027       20948.73 18955.38 22942.08\n2028       20328.86 18175.95 22481.78\n2029       19714.61 17332.46 22096.76\n\n\n\nautoplot(parima110, \n     ylab = \"Defunciones\",\n     main = \"\") +\n  scale_x_continuous(breaks= seq(1990, 2030, 4)) \n\n\n\n\n\n\n\nFigura 4: Defunciones por enfermedades cerebrovasculares y predicción\n\n\n\n\n\n\n\n\n\n\n7 Calidad de las previsiones extramuestrales según horizonte temporal\nVamos a aplicar la metodología de origen de predicción móvil a la serie. Asumimos que se precisan veinte años para hacer una buena estimación, \\(k=15\\), y que el horizonte temporal es cinco años, \\(h = 5\\).\n\nk &lt;- 15                  \nh &lt;- 5                    \nTT &lt;- length(DefEnfCer)   \ns &lt;- TT-k-h               \n\nmapeArima &lt;- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set &lt;- subset(DefEnfCer, start = i + 1, end = i + k)\n  test.set &lt;-  subset(DefEnfCer, start = i + k + 1, end = i + k + h)\n  \n  fit &lt;- Arima(train.set, \n               order = c(1, 1, 0),\n               include.constant = TRUE)\n  \n  fcast&lt;-forecast(fit, h = h)\n  \n  mapeArima[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n}\n\nerrorArima &lt;- colMeans(mapeArima)\nerrorArima\n\n[1] 1.634259 1.844931 2.670749 3.078858 3.597547\n\n\nSe observa que el error de ajuste (1.7%) y el error de predicción extramuestral a un periodo vista (1.6%) son similares. Además, conforme aumenta el horizonte temporal de previsión el error aumenta, aunque a cinco años vista se mantiene aun bajo.\n\n\n\n\n\n8 Comparación con un modelo de Alisado\nSi estimamos la serie anual de Defunciones por enfermedades cerebrovasculares aplicando técnicas de Alisado exponencial nos encontramos con un modelo con error y pendiente aditivas ETS(A,A,N).\n\nsummary(alisado &lt;- ets(DefEnfCer))\n\nETS(M,A,N) \n\nCall:\nets(y = DefEnfCer)\n\n  Smoothing parameters:\n    alpha = 0.4328 \n    beta  = 1e-04 \n\n  Initial states:\n    l = 44336.8327 \n    b = -639.7901 \n\n  sigma:  0.0244\n\n     AIC     AICc      BIC \n596.7792 598.8482 604.5560 \n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE       ACF1\nTraining set 62.08904 751.5102 598.8054 0.2250512 1.823688 0.6593278 -0.2028046\n\n\nLa calidad del ajuste del modelo de Alisado es algo inferior a la del modelo Arima: MAPE de 1.8% para el primero frente a 1.7% para el segundo; RMSE de 738 para el primero frente a 752 para el segundo. Además, si se aplica la metodología de origen de predicción móvil al modelo de Alisado, este ofrece predicciones extramuestrales menos precisas que Arima. Por ejemplo, a un periodo vista el error por Alisado es del 2.5% y con Arima del 1.7% y a cinco periodos vista estos errores pasan al 5.8% y 3.6%.\nFinalmente, las predicciones realizadas con ambos modelos son muy similares.\n\npalisado &lt;- forecast(alisado, h = 5)\n\nautoplot(DefEnfCer,\n         series = \"Defunciones\",\n         main=\"\",\n         xlab=\"\", \n         ylab=\"Defunciones\") +\n  autolayer(parima110$mean, series = \"Previsión con Arima\") +\n  autolayer(palisado$mean, series = \"Previsión con Alisado\") +\n  labs(colour = \"Series\") + \n  theme(legend.position=c(0.8,0.8))\n\n\n\n\n\n\n\nFigura 5: Defunciones Enf. Cerebrovasculares y previsión\n\n\n\n\n\nEn definitiva, para esta serie, que es corta y aparentemente sencilla, la aproximación por la metodología Arima ofrece predicciones similares a las del modelo de Alisado. La precisión de las predicciones con Arima es mayor que con Alisado, pero la diferencia es mínima (menor a 1 p.p.) para horizontes temporales hasta 3 años."
  },
  {
    "objectID": "02-Logistica.html",
    "href": "02-Logistica.html",
    "title": "Logística",
    "section": "",
    "text": "El grueso del material del curso lo podéis encontrar en esta página web. Sin embargo, hay muuuucho material adicional disponible en formato papel y online. A continuación os describo brevemente dónde podéis encontrar parte de este material.\n\n\n\n\n\n\nContenidos del curso\n\nPágina web del curso dónde encontrarás todo el material teórico y práctico necesario para seguir el curso y realizar las prácticas de evaluación.\nAula Virtual donde encontrarás una copia de todo el material del curso en un único fichero comprimido. Además, Aulavirtual será el medio de comunicación oficial entre nosotros y dónde subiréis las respuestas a las pruebas tipo test y las prácticas de evaluación.\n\n\n\n\n\n\n\n\nMateriales\n\nContenidos teóricos: los encontrarás en la sección Diapos. Las primeras entradas corresponden a los contenidos teóricos del curso, las siguientes a un ejemplo práctico de aplicación para cada tema. Este ejemplo no necesariamente lo veremos durante el curso, pero puede serte de ayuda para terminar de comprender los conceptos teóricos vistos y su aplicación práctica con R. La última entrada es un extenso ejemplo que va más allá de una simple aplicación de los conceptos visto durante el curso.\nEn Píldoras encontrarás extensiones a conceptos del curso, ninguno de los cuales veremos en clase.\nRecursos de R: contiene los ficheros de datos de los ejemplos utilizados durante el curso (ficheros .csv) así como el código de R que usaremos en las clases (ficheros .R).\nPráctica: contiene el fichero de datos que cada uno de vosotros debéis usar para realizar las prácticas. Además, se indica una descripción provisional e incompleta del contenido de las prácticas de evaluación. El contenido final de cada una de ellas se comentará en clase al finalizar cada tema y las prácticas deberán ajustarse a este contenido.\n\n\n\n\n\n\nMás recursos\nLa última sección de la página web Más es un pequeño baúl de recursos que te pueden ser útiles:\n\nR: enlaces a páginas con recursos sobre R y RStudio\nMarkdown: enlaces a páginas con recursos sobre (R)Markdown y Quatro\nOtros: enlace a R-bloggers, un blog sobre R que te puede ser de gran ayuda y a libros online sobre R, análisis de datos, inferencia…\n\n\n\n\n\n\n\n\nBibliografía\nEl material de este curso debe mucho a\n\nForecasting: Principles and Practice: un libro online (realizado con bookdown) de Rob J. Hyndman y George Athanasopoulos. Durante el curso usaremos fundamentalmente la librería forecast de Hyndman, Athanasopoulos y otros (aquí).\n\n\n\nHay un montón de bibliografía sobre Predicción con Datos Temporales, tanto en formato libro como online. Es imposible ser exhaustivos, así que aquí va una muestra.\nSentíos libres de hurgar en la web en busca de tutoriales, videos, libros… que os puedan ser útiles. Si algún material que localizáis creéis que es realmente útil, por favor, compartirlo con vuestros compañeros y conmigo.\n\n\nGrandes clásicos de Series Temporales\n\nAbraham, B. y Ledolter, J. (1983) Statistical methods for forecasting. Wiley\nBox, G.E.P. y Jenkins, G.M. (1976) Time series analysis, forecasting and control. Holden-Day\nBox, G.E.P., Jenkins, G.M. y Reinsel, G.C. (1994) Time series análisis. Prentice-Hall\nBrockwell, P.J. y Davis, R.A. (1996) Introduction to time series and forecasting. Springer-Verlag\nChatfield, C. (1989) The analysis of time series. An introduction. Chapman & Hall\nGreene, W. (1998) Análisis econométrico. Prentice Hall\nHolden, K., Peel D.A. y Thompson, J. L. (1990) Economic Forecasting: an introduction. Cambridge University Preess: Cambridge\nHolton, J. y Keating, B. (1996) Previsión en los negocios. IRWIN\nMarkridakis, S., Weelwright, S.C. y Hyndman, R.J. (1998) Forecasting: Methods and Applications. Willey\nPeña, D. (1999) Estadística: modelos y métodos 2 (Modelos lineales y Series Temporales.) Alianza Universidad Textos.\nPulido, A. y López, A.M. (1999) Predicción y simulación aplicada al a economía y gestión de empresas. Pirámide.\nUriel, E. (2005) Introducción al análisis de series temporales. Paraninfo.\n\n\n\nBibliografía de Series Temporales con R\n\nCowpertwait, P. S. P. y Metcalfe, A. V. (2009) Introductory Time Series with R. Springer (Collection Use R!)\nPfaff, B. (2008) Analysis of Integrated and Cointegrated Time Series with R. Springer (Collection Use R!)\nCryer, J. D., Chan, Kung-Sik. (2008) Time Series Analysis. With Applications in R. Springer\n\n\n\nDirecciones de internet\n\nQuick-R: Time Series and Forecasting: un vistazo muy pero que muy rápido al análisis de series temporales con R.\nTres manuales online de series temporales al estilo del comentado al inicio de la bibliografía se pueden encontrar aquí, aquí y aquí\nJournal of Time Series Analysis: revista científica sobre series temporales\n\nInternational Journal of Forecasting: revista científica sobre predicción\n\n\n\nSoftware gratuito\n\nR-project: creo que ya lo conocemos todos.\nJDemetra+: ajuste automático de series con modelos ARIMA aplicando dos métodos: TRAMO-SEATS+ y X-12ARIMA/X-13ARIMA-SEATS. Existe un interfaz del programa para R con la librería RJDemetra.\nGretl: programa de modelización gratuito: regresión múltiple, modelos de elección binaria, series temporales, etc."
  },
  {
    "objectID": "01-Guia-curso.html#introducción",
    "href": "01-Guia-curso.html#introducción",
    "title": "Guía del Curso",
    "section": "Introducción",
    "text": "Introducción\nEl módulo de Series Temporales pertenece al Máster de Bioestadística y se configura como un módulo impartido en el segundo cuatrimestre con 10 sesiones de cinco horas.\nEl objetivo general es aprender a manejarse con datos de corte temporal, Series Temporales. Es decir, ser capaces de aplicar los métodos para describir, analizar, modelizar y predecir series de datos que evolucionan en el tiempo. Además, hay que saber evaluar la calidad de las predicciones en el contexto de la estadística tradicional y con técnicas de Machine Learning.\nPara ello, veremos la teoría necesaria y, sobre todo, practicaremos utilizando el programa estadístico R y el lenguaje Markdown (o Quarto) a través de RStudio. A estas alturas del grado ya debes estar familiarizado con este programa."
  },
  {
    "objectID": "01-Guia-curso.html#contenidos",
    "href": "01-Guia-curso.html#contenidos",
    "title": "Guía del Curso",
    "section": "Contenidos",
    "text": "Contenidos\nEl programa de la asignatura contiene 5 temas distribuidos en 10 sesiones de 5 horas:\n\nSesión 1 y 2: Tema 1. Definición y Componentes\nSesión 3 y 4: Tema 2. Métodos de Alisado Exponencial\nSesión 5: Tema 3. Procesos estocásticos\nSesión 6 y 7: Tema 4. Modelos ARIMA\nSesión 8 y 9: Tema 5. Modelos ARIMA con estacionalidad\nSesión 10: Repaso general\n\nPasar de un tema al siguiente supone incrementar la complejidad en las técnicas empleadas para el análisis y predicción de una serie temporal, de forma que podréis valorar con datos reales la relación entre complejidad–tiempo–resultados y optar por la metodología más adecuada."
  },
  {
    "objectID": "01-Guia-curso.html#metodología",
    "href": "01-Guia-curso.html#metodología",
    "title": "Guía del Curso",
    "section": "Metodología",
    "text": "Metodología\nDado el carácter eminentemente práctico del módulo, todas las sesiones se impartirán con el ordenador a mano y se alternará entre teoría y práctica según convenga:\n\nTiempo de teoría: os plantearé los conceptos nucleares de cada tema, de forma precisa y rigurosa, en lenguaje natural, gráfico y formal. El material teórico lo podéis encontrar en la sección Diapos.\nTiempo de práctica: a partir del código y de los ficheros de datos que podéis encontrar en la sección Recursos de R practicaréis los conceptos teóricos y aprenderéis el manejo de R para el análisis de series temporales.\nTiempo de evaluación: tras cada tema realizaréis una breve prueba tipo test, con tiempo limitado, para valorar si habéis adquirido los conocimientos teóricos y las habilidades practicas básicas.\nTrabajo práctico de evaluación: también en clase os daré tiempo para realizar parte de los trabajos prácticos que servirán de evaluación del módulo.\n\n\n\nTodo el material necesario para este curso lo tenéis disponible en esta página web en la sección Recursos de la asignatura así como en Aula Virtual. También podéis más detalles en la sección Logística de la web."
  },
  {
    "objectID": "01-Guia-curso.html#evaluación",
    "href": "01-Guia-curso.html#evaluación",
    "title": "Guía del Curso",
    "section": "Evaluación",
    "text": "Evaluación\nTanto la parte teórica como la práctica se evaluarán exclusivamente a través de breves pruebas y trabajos prácticos que se irán realizando a lo largo del curso. No habrá, por tanto, examen de evaluación. En caso de suspender en primera convocatoria, deberéis hablar conmigo para coordinar el trabajo de recuperación.\nTras cada una de las cinco unidades temáticas se realizará una breve prueba tipo test. La prueba contendrá preguntas de respuesta múltiple, numérica, verdadero/falso, etc. Este conjunto de pruebas supondrá un 20% de la nota de la asignatura y no es recuperable.\nAdemás, tras cada unidad temática, os daré una práctica de evaluación que deberéis entregar en el plazo establecido. La valoración de estos trabajos supondrá el restante 80% de la nota de la asignatura, que sí es recuperable.\nLas prácticas de evaluación deben realizarse utilizando el programa estadístico R y recomiendo que uséis para la escritura del trabajo el lenguaje Markdown o Quatro."
  },
  {
    "objectID": "04-07-Combinando_predicciones.html#footnotes",
    "href": "04-07-Combinando_predicciones.html#footnotes",
    "title": "Combinación de predicciones",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPara ser rigurosos, solo en un caso un método supera a la media simple, el método Ingenuo I en previsiones a 4 años vista, pero por una décima.↩︎"
  },
  {
    "objectID": "03-02-Tema2.html#notación-y-definiciones",
    "href": "03-02-Tema2.html#notación-y-definiciones",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "2.1 Notación y definiciones",
    "text": "2.1 Notación y definiciones\nDada una serie temporal \\(\\{y_t\\}_{t=1}^T\\), se define:\n\nPrevisión \\(h\\) periodos adelante, como la previsión de la serie para el periodo \\(t+h\\) disponiendo de información hasta el periodo \\(t\\), y se denota por \\(\\hat{y}_{t+h|t}\\). Por simplicidad lo escribiremos también como \\(\\hat{y}_{t+h}\\).\nAsí, \\(\\hat{y}_{t+1|t}\\) es la previsión un periodo adelante o a un periodo vista. Es decir, la previsión de la serie en el periodo \\(t+1\\) desde el periodo \\(t\\).\nPor simplicidad denotaremos a \\(\\hat{y}_{t+1|t}\\) como \\(\\hat{y}_{t+1}\\). Por tanto, \\(\\hat{y}_{t}\\) será la previsión en \\(t\\) con datos hasta el periodo \\(t-1\\) (\\(\\hat{y}_{t} = \\hat{y}_{t|t-1}\\)).\n\nSi para un periodo \\(t\\) se tiene la observación \\(y_t\\) y una previsión \\(\\hat{y}_t\\), se define como error a un periodo vista a \\[\\hat{e}_t=y_t-\\hat{y}_t,\\] de forma que la serie \\(\\{\\hat{e}_t\\}_{t=1}^T\\) nos permitirá definir varios criterios de calidad.\n\n¿Calidad de ajuste o de previsión?\nLos mismos criterios que se van a definir a continuación pueden ser criterios de calidad de ajuste o de calidad de las previsiones. Esta diferencia depende de cómo se ha obtenido \\(\\hat{y}_t\\).\nSi en el proceso de predicción hay parámetros cuyo ajuste o estimación se ha realizado usando toda la serie temporal, entonces hablaremos de calidad de ajuste: para obtener \\(\\hat{y}_t\\) se habrá usado el dato \\(y_t\\) y los siguientes, es decir, datos posteriores al periodo \\(t\\) y entonces \\(\\hat{e}_t\\) es un error de ajuste.\nPor el contrario, si en el proceso de predicción no hay parámetros o habiéndolos su estimación se ha realizado usando la serie temporal hasta el periodo \\(t-1\\), entonces hablaremos de calidad de predicción: para obtener \\(\\hat{y}_t\\) solo se habrán usado datos hasta \\(y_{t-1}\\), es decir, datos anteriores al periodo \\(t\\), y entonces \\(\\hat{e}_t\\) es un error de predicción.\nA veces al error de ajuste se le denomina error de previsión intramuestral y hablaremos de criterios de calidad intramuestral. De la misma forma al error de previsión se le denomina error de previsión extramuestral y hablaremos de criterios de calidad extramuestral o precisión de las predicciones."
  },
  {
    "objectID": "03-02-Tema2.html#criterios-de-calidad-1",
    "href": "03-02-Tema2.html#criterios-de-calidad-1",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "2.2 Criterios de calidad",
    "text": "2.2 Criterios de calidad\nDada una serie \\(\\{y_t\\}_{t=1}^T\\), un método de predicción y su vector de errores asociado \\(\\{\\hat{e}_t\\}_{t=1}^T\\), podemos definir múltiples criterios de calidad de ajuste o predicción del método que hacen referencia a la presencia de sesgo en las predicciones, la magnitud del error cometido y la calidad del intervalo de confianza de las predicciones. Las más habituales son (siglas en inglés):\n\nError medio (ME): \\(\\frac{1}{T}\\sum_{t=1}^T \\hat{e}_t\\)\n\n\nRaíz del error cuadrático medio (RMSE): \\(\\sqrt{\\frac{1}{T}\\sum_{t=1}^T \\hat{e}^2_t}\\)\n\n\nError absoluto medio (MAE): \\(\\frac{1}{T}\\sum_{t=1}^T |\\hat{e}_t|\\)\n\n\nError porcentual medio (MPE): \\(\\frac{100}{T}\\sum_{t=1}^T \\frac{\\hat{e}_t}{y_t}\\)\n\n\nError porcentual absoluto medio (MAPE): \\(\\frac{100}{T}\\sum_{t=1}^T \\big|\\frac{\\hat{e}_t}{y_t}\\big|\\)\n\n\nError escalado absoluto medio (MASE): \\(\\big(\\frac{1}{T}\\sum_{t=1}^T |\\hat{e}_t|\\big)/q\\), donde \\(q\\) es el error absoluto medio para un método ingenuo de predicción: el método ingenuo I para series sin estacionalidad y el método ingenuo con estacionalidad para series con estacionalidad.\n\n\nCorrelación entre \\(\\hat{e}_t\\) y \\(\\hat{e}_{t-1}\\) (ACF1).\n\n\n\nME y MPE permiten valorar el sesgo de las predicciones (que estén sistemáticamente por encima o por debajo de los valores reales).\n\nLo esperado es un valor cercano a cero (en el caso de ME con relación al valor medio de la serie). Valores muy alejados de cero son indicadores de sesgo de predicción.\nLa más cómoda de interpretar es MPE. Asumiremos que si MPE es menor que 1% en valor absoluto, no hay sesgo.\n\nRMSE y MAE son medidas de calidad de ajuste/precisión. Indican el error medio cometido, independientemente del signo, medido en las mismas unidades que la serie temporal.\n\nEstán acotadas inferiormente por el valor óptimo de 0, pero no hay cota superior.\n\nMAPE es una medida de calidad de ajuste/precisión alternativa que indica el error porcentual medio cometido.\n\nEstá acotado inferiormente por el valor óptimo de 0%, y la cota superior natural es 100%, aunque podría sobrepasarse.\nSi \\(y_t\\) puede valer 0, entonces MAPE no se puede calcular. Además, MAPE penaliza más los errores negativos frente a los errores positivos. Se puede definir el error porcentual absoluto medio simétrico (sMAPE) \\(\\frac{200}{T}\\sum_{t=1}^T \\Big|\\frac{\\hat{e}_t}{y_t + \\hat{y}_t}\\Big|\\) a fin de corregir estos problemas.\n\nMASE es la ratio entre el MAE del método usado y el MAE de un método ingenuo de predicción. Permite saber cuánto ganamos en calidad de ajuste/predicción al pasar de un método ingenuo a otro más complicado.\n\nUn valor cercano a 1 indica que el método usado no es mejor que el método ingenuo\nCuanto más cercano a 0, mejor es el método usado respecto del método ingenuo\nSu complementario a 1 se puede interpretar como la tasa de mejora\n\nACF1 permite saber si el método empleado ha extraído toda la información disponible en los datos de la serie para hacer las predicciones. Si no es así, la fórmula usada para estimar el intervalo de confianza de las predicciones no será válida:\n\nUn valor muy cercano a 0 (menor que 0.1 en valor absoluto) indica que se ha extraído toda la información y la fórmula usada para estimar el intervalo de confianza de las predicciones es válida.\nValores minimamente alejados de 0 indican que la fórmula no es válida.\n\n\n\n\n\n\n\nCálculo del intervalo de confianza de las predicciones\n\n\n\nVe a la Píldora Bootstrapping para intervalos de predicción para saber más sobre las fórmulas usadas para estimar el intervalo de confianza de las predicciones y alternativas de cálculo cuando estas fórmulas no son válidas.\n\n\n\n\nEn todas las fórmulas de criterios de calidad las medias se pueden sustituir por medianas. Esto es especialmente útil cuando hay observaciones atípicas que generan errores muy altos.\nLos indicadores de calidad de ajuste que se basan en predicciones intramuestrales a un periodo vista, presentan dos problemas. Primero, evalúan el error a un periodo vista, cuando en muchas situaciones reales las predicciones se realizan sobre un horizonte temporal más amplio. Segundo, son errores intramuestrales, resultantes de predecir los mismos datos que ha usado el método para calcular la predicción y, por tanto, sobrestiman la capacidad predictiva del modelo.\nVeremos en el epígrafe 4 de este tema métodos de evaluación de la precisión de las predicciones que superan estas limitaciones."
  },
  {
    "objectID": "03-02-Tema2.html#métodos-sencillos-de-predicción-1",
    "href": "03-02-Tema2.html#métodos-sencillos-de-predicción-1",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "3.1 Métodos sencillos de predicción",
    "text": "3.1 Métodos sencillos de predicción\n\nSeries sin tendencia y sin estacionalidad\nMétodo de la Media: \\(\\hat{y}_{T+h}=(y_1+\\ldots,y_T)/T\\).\n\nLa predicción para cualquier periodo futuro es la media de las observaciones disponibles previas.\nFunción de R: meanf(y, h)\n\nMétodo ingenuo I: \\(\\hat{y}_{T+h}=y_T\\).\n\nLa predicción para cualquier periodo futuro es la última observación disponible.\nFunción de R: naive(y, h) o rwf(y, h) (rw de random walk)\nPara series sin estacionalidad este es el método ingenuo de comparación del MASE.\n\n\n\nSeries con tendencia y sin estacionalidad\nMétodo ingenuo II: \\(\\hat{y}_{T+h}=y_T + h(y_T-y_{T-1})\\).\n\nLa predicción \\(h\\) periodos adelante es la última observación disponible más \\(h\\) veces el último incremento observado.\nNo tiene función en R, pero se podría emular mediante la función ets (véase epígrafe de 5.5, Alisado exponencial de Holt).\n\nMétodo de la deriva: \\(\\hat{y}_{T+h}=y_T+h\\frac{y_T - y_1}{T-1}\\).\n\nLa predicción \\(h\\) periodos adelante es la última observación disponible más \\(h\\) veces el incremento medio observado.\nFunción de R: rwf(y, h, drift = TRUE)\n\n\n\nSeries sin tendencia y con estacionalidad\nMétodo ingenuo con estacionalidad: \\(\\hat{y}_{T+h}=y_{T-m(k+1)}\\).\n\nLa predicción para un periodo es la última observación disponible de la misma estación que la fecha que se desea predecir.\n\\(k\\) es la parte entera de \\((h-1)/m\\), es decir, el número de estaciones completas en el periodo de predicción previo al periodo \\(T+h\\).\nFunción de R: snaive(y, h)\nPara series con estacionalidad este es el método ingenuo de comparación del MASE\n\nNo hay métodos sencillos cuando la serie tiene tendencia y estacionalidad, así que se suele usar el método ingenuo con estacionalidad."
  },
  {
    "objectID": "03-02-Tema2.html#ejemplo-de-aplicación",
    "href": "03-02-Tema2.html#ejemplo-de-aplicación",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "3.2 Ejemplo de aplicación",
    "text": "3.2 Ejemplo de aplicación\n\nSerie Residuos\nAnalizaremos Residuos, una serie anual de 1995 a 2023 (fuente Instituto Nacional de Estadística) que muestra los residuos recogidos por o en nombre de las autoridades municipales y eliminados a través del sistema de gestión de residuos. La Figura 1 muestra que es una serie con tendencia que ha cambiado con el tiempo.\n\nresiduos &lt;- read.csv2(\"./series/Residuos.csv\", \n                      header = TRUE)\n\nresiduos &lt;- ts(residuos[, 2],\n               start = 1995, \n               frequency  = 1)\n\nautoplot(residuos,\n         main = \"\",\n         xlab = \"\",\n         ylab = \"Kg per cápita\")\n\n\n\n\n\n\n\nFigura 1: Residuos per cápita recogidos\n\n\n\n\n\nLas siguientes salidas muestran el resultado de la aplicación de los métodos sencillos a la serie. Se ha fijado un horizonte de previsión de cinco años (h = 5). Los dos primeros métodos realizan una predicción constante, el método de la media da la media de la recogida de residuos per cápita en el periodo de análisis (530.52) y el Ingenuo I da el último dato observado (465). El método de la deriva da una previsión que decrece de año en año un valor igual a \\((y_T - y_1)/(T-1)\\), que en este caso vale \\(-1.43\\).\n\n(mediaResiduos &lt;- meanf(residuos, h = 5))\n\n     Point Forecast   Lo 80    Hi 80    Lo 95    Hi 95\n2024       530.5172 439.883 621.1514 389.0681 671.9663\n2025       530.5172 439.883 621.1514 389.0681 671.9663\n2026       530.5172 439.883 621.1514 389.0681 671.9663\n2027       530.5172 439.883 621.1514 389.0681 671.9663\n2028       530.5172 439.883 621.1514 389.0681 671.9663\n\n(naiveResiduos &lt;- naive(residuos, h = 5))\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2024            465 437.9526 492.0474 423.6346 506.3654\n2025            465 426.7492 503.2508 406.5005 523.4995\n2026            465 418.1526 511.8474 393.3530 536.6470\n2027            465 410.9053 519.0947 382.2692 547.7308\n2028            465 404.5202 525.4798 372.5042 557.4958\n\n(derivaResiduos &lt;- rwf(residuos, drift = TRUE , h = 5))\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2024       463.5714 435.6045 491.5384 420.7997 506.3432\n2025       462.1429 421.9155 502.3702 400.6204 523.6653\n2026       460.7143 410.6316 510.7970 384.1194 537.3091\n2027       459.2857 400.5299 518.0416 369.4264 549.1450\n2028       457.8571 391.1476 524.5667 355.8337 559.8806\n\nderivaResiduos$model$par$drift # Permite ver la pendiente del modelo.\n\n[1] -1.428571\n\n#summary(derivaResiduos) Con summary se puede obtener una salida más completa\n\nLa Figura 2 muestra el resultado gráfico de la aplicación de estos métodos. El argumento PI = FALSE hace que no se impriman los intervalos de confianza de las predicciones.\n\nautoplot(residuos, \n         series = \"Residuos\",\n         xlab = \"\",\n         ylab = \"Kg per cápita\",\n         main = \"\") +\n  autolayer(mediaResiduos, series=\"Media\", PI = FALSE) +\n  autolayer(naiveResiduos, series=\"Ingenuo\", PI = FALSE) +\n  autolayer(derivaResiduos, series=\"Deriva\", PI = FALSE) +\n  scale_colour_discrete(limits=c(\"Residuos\", \"Media\", \n                                 \"Ingenuo\", \"Deriva\")) +\n  labs(colour=\"Métodos\") + \n  theme(legend.position.inside =c(0.2,0.3))\n\n\n\n\n\n\n\nFigura 2: Recogida de residuos y predicción por métodos sencillos\n\n\n\n\n\nCon la función accuracy se puede obtener el error de ajuste a un periodo vista de cada método:\n\naccuracy(mediaResiduos)\naccuracy(naiveResiduos)\naccuracy(derivaResiduos)\n\n\n\n             ME  RMSE   MAE   MPE  MAPE MASE ACF1\nMedia      0.00 66.71 59.12 -1.53 11.11 3.68 0.93\nIngenuo I -1.43 21.11 16.07 -0.37  2.96 1.00 0.43\nDeriva     0.00 21.06 16.00 -0.09  2.94 1.00 0.43\n\n\nPodemos destacar que:\n\nEl método de la deriva presenta la mejor calidad de ajuste con un error de 21.06 kg per cápita (RMSE) o del 2.94% (MAPE). El método Ingenuo I tienen una calidad de ajuste casi igual, con un error medio de 21.11 kg per cápita (RMSE) y un error porcentual del 2.96% (MAPE).\nSi se usa como criterio de calidad de ajuste el error absoluto medio (MAE), vuelve a ser mejor el método de la deriva.\nNingún método genera predicciones por intervalo fiables (ACF1 &gt; 0.1).\nLos métodos Ingenuo I y de la Deriva no presentan sesgo (|MPE| &lt; 1%), pero si lo hace el de la Media. Además, el error medio (ME) siempre será nulo para el método de la Media y de la Deriva, lo que indica que en término medio nos equivocamos lo mismo por exceso como por defecto. Esta es una buena propiedad, que el método Ingenuo I no verifica.\nPara series sin estacionalidad el método sencillo de comparación usado en el cálculo del MASE es el Ingenuo I. Es por ello que este indicador vale 1 para este método. Como el método de Media tiene un MAE superior al Ingenuo I, su MASE es mayor que 1. En concreto, MASE = 3.68 = 59.12 / 16.07.\n\nConcluimos que, con independencia del criterio usado, los métodos que mejor ajustan los datos son el el Ingenuo I y el de la Deriva.\n\n\nSerie Nacimientos\nPodemos usar el método ingenuo con estacionalidad con la serie Nacimientos para obtener una previsión a dos años vista.\n\nnacimientos &lt;- read.csv2(\"./series/Nacimientos.csv\", \n                         header = TRUE)\n\nnacimientos &lt;- ts(nacimientos[, 2],\n                  start = c(1975, 1),\n                  frequency = 12)\n\n(snaive.nacimientos &lt;- snaive(nacimientos, \n                              h = 24, \n                              level = 95))\n\n         Point Forecast    Lo 95    Hi 95\nJul 2025          27439 24075.46 30802.54\nAug 2025          27644 24280.46 31007.54\nSep 2025          27196 23832.46 30559.54\nOct 2025          28348 24984.46 31711.54\nNov 2025          26767 23403.46 30130.54\nDec 2025          26665 23301.46 30028.54\nJan 2026          26792 23428.46 30155.54\nFeb 2026          24288 20924.46 27651.54\nMar 2026          26338 22974.46 29701.54\nApr 2026          26011 22647.46 29374.54\nMay 2026          26140 22776.46 29503.54\nJun 2026          26066 22702.46 29429.54\nJul 2026          27439 22682.24 32195.76\nAug 2026          27644 22887.24 32400.76\nSep 2026          27196 22439.24 31952.76\nOct 2026          28348 23591.24 33104.76\nNov 2026          26767 22010.24 31523.76\nDec 2026          26665 21908.24 31421.76\nJan 2027          26792 22035.24 31548.76\nFeb 2027          24288 19531.24 29044.76\nMar 2027          26338 21581.24 31094.76\nApr 2027          26011 21254.24 30767.76\nMay 2027          26140 21383.24 30896.76\nJun 2027          26066 21309.24 30822.76\n\naccuracy(snaive.nacimientos)\n\n                    ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set -588.6936 1716.123 1355.481 -1.59859 3.692636    1 0.7134456\n\n\nEl error absoluto porcentual medio es del 3.7% (que corresponde a unos 1700 bebés según RMSE). Es decir, aplicando algo tan simple como predecir el número de nacimientos para un mes como los nacimientos del mismo mes del año previo, tenemos ya un error de ajuste muy bajo. Sin embargo, este método en general sobreestima algo el número de nacimientos (|MPE| mayor que 1%) y sus predicciones por intervalo no son fiables.\nLa Figura 3 muestra la serie y la predicción que, debido al método usado, no incorpora la tendencia decreciente de los últimos años.\n\nautoplot(snaive.nacimientos,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\",\n         PI = FALSE,\n         xlim = c(2000, 2028))\n\n\n\n\n\n\n\nFigura 3: Nacimientos y predicción por el método Ingenuo con estacionalidad\n\n\n\n\n\n\n\nSerie Demanda eléctrica\nPodemos usar el método ingenuo con estacionalidad con la serie Demanda eléctrica, que tiene una estacionalidad de orden 7, pero no parece presentar tendencia. El error absoluto porcentual medio es del 4.1% o 41 GWh (RMSE), un error razonablemente reducido. Sin embargo, el ACF1 indica que la fórmula usada para el cálculo del intervalo de confianza de las predicciones no es válida.\n\nelectricidad &lt;- read.csv(\"./series/Consumo electrico.csv\", \n                         header = TRUE)\n\nelectricidad &lt;- ts(electricidad[, 1],\n                   start = c(1, 1),\n                   frequency = 7)\n\nsnaive.electricidad &lt;- snaive(electricidad, \n                              h = 28, \n                              level = 95)\n\naccuracy(snaive.electricidad)\n\n                     ME     RMSE      MAE        MPE     MAPE MASE      ACF1\nTraining set -0.4689415 41.42265 27.81256 -0.2537472 4.085628    1 0.7181066\n\n\nLa Figura 4 muestra la serie y la predicción a cuatro semanas vista. Debido a que la semana de referencia para predecir es la semana de Navidad, donde el consumo eléctrico es inferior al usual, las predicciones resultan ser claramente incorrectas. Este es un buen ejemplo de la diferencia entre calidad de ajuste y precisión de las predicciones.\n\nautoplot(snaive.electricidad,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 4: Demanda eléctrica y predicción por el método Ingenuo con estacionalidad"
  },
  {
    "objectID": "03-02-Tema2.html#validación-por-conjunto-de-datos-de-entrenamiento-y-prueba",
    "href": "03-02-Tema2.html#validación-por-conjunto-de-datos-de-entrenamiento-y-prueba",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "4.1 Validación por conjunto de datos de entrenamiento y prueba",
    "text": "4.1 Validación por conjunto de datos de entrenamiento y prueba\nVamos a estimar la calidad de las predicciones obteniendo medidas de error para previsiones extramuestrales a varios periodos vista usando la filosofía del conjunto de datos de entrenamiento y prueba.\nDividimos la serie temporal \\(\\{y_t\\}_{t=1}^T\\) en dos subseries. Los primeros datos \\(\\{y_t\\}_{t=1}^{T_0}\\), \\(T_0 &lt; T\\), se usarán para estimar el modelo (conjunto de entrenamiento); y los últimos datos \\(\\{y_t\\}_{t={T_0+1}}^{T}\\) para calcular la precisión de las predicciones (conjunto de prueba).\nEsta metodología, muy efectiva para datos de corte transversal, genera dos problemas cuando se aplica a series temporales: i) el error obtenido es una mezcla de errores de predicción a diferentes horizontes temporales, difícil de interpretar; ii) los resultados dependen tremendamente del punto de corte temporal seleccionado \\(T_0\\).\n\nSerie Residuos\nVamos a reservar, por ejemplo, las últimas 7 observaciones de la serie Residuos (años 2017 a 2023) y ajustar el modelo con las restantes. Después usaremos este modelo para calcular las predicciones a 7 periodos vista y compararlas con los valores reales de la serie.\n\n# Definimos las observaciones intra- y extramuestrales\nresiduosIntra &lt;- subset(residuos, end = length(residuos) - 7)\nresiduosExtra &lt;- subset(residuos, start = length(residuos) - 6)\n\n# Estimamos el modelo con todos los datos menos los 7 ultimos y\n# predecimos los 7 años que hemos quitado de la serie \nresiduosExtraPre &lt;- rwf(residuosIntra,  h = 7, drift = TRUE)\n\n# Vemos la calidad del ajuste. Primero la predicción y luego los datos reales\naccuracy(residuosExtraPre, residuosExtra)\n\n\n\n                ME  RMSE   MAE   MPE MAPE MASE ACF1 Theil's U\nTraining set  0.00 23.45 18.38 -0.13 3.30 0.99 0.46        NA\nTest set     18.14 19.81 18.14  3.81 3.81 0.98 0.12      1.86\n\n\nAtendiendo al MAPE se tiene que el error de previsión a un periodo vista en el periodo intramuestral de 1995 a 2016 es del 3.3%, medio p.p. inferior al error de previsión a largo plazo en el periodo extramuestral de 2017 a 2023 igual a 3.8%. Además, para el periodo extramuestral el error medio (ME) es positivo y muy elevado, un indicativo de que las previsiones están segadas (subestiman la realidad).\nLa Figura 5 puede ayudar a entender este proceso de validación:\n\nLa línea de puntos vertical separa el periodo muestral (1995-2016) usado para estimar el modelo, del periodo extramuestral (2017-2023) usado sólo para hacer las previsiones.\nLa serie Residuos aparece como una línea sólida en negro, desde 1995 hasta 2023.\nLa previsión intramuestral (a un periodo vista) de la serie Residuos aparece como una línea azul.\nLa línea en rojo es la previsión extramuestral a largo plazo. Observa que todas las previsiones están por debajo del valor real de la serie.\nAl lado de cada previsión (intra y extramuestral) se ha indicado el error estimado (MAPE).\n\nClaramente estos resultados dependen del punto de corte seleccionado.\n\n\n\n\n\n\n\n\nFigura 5: Residuos, predicción intra- y extramuestral\n\n\n\n\n\n\n\n\n\n\n\nImportamcia del punto de corte\n\n\n\nPrueba a reservar las últimas 6 observaciones de la serie Residuos y repite el análisis.\n\n\n\n\nSerie Nacimientos\nCalculamos de nuevo los diferentes criterios de bondad de ajuste para valorar la calidad de las previsiones extramuestrales a largo plazo. En este caso vamos a reservar los últimos 36 meses como periodo extramuestral.\n\nnacimientosIntra &lt;- subset(nacimientos, end = length(nacimientos) - 36)\nnacimientosExtra &lt;- subset(nacimientos, start = length(nacimientos) - 35)\n\nnacimientosExtraPre &lt;- snaive(nacimientosIntra, h = 36)\n\naccuracy(nacimientosExtraPre, nacimientosExtra)\n\n\n\n                   ME    RMSE     MAE   MPE MAPE MASE ACF1 Theil's U\nTraining set  -595.50 1755.83 1397.25 -1.59 3.76 1.00 0.72        NA\nTest set     -1291.56 1562.25 1348.83 -4.77 4.99 0.97 0.34      1.29\n\n\n\n\n\n\n\n\n\n\nFigura 6: Nacimientos, predicción intra- y extramuestral\n\n\n\n\n\nEl método ingenuo con estacionalidad no captura el decrecimiento en el número de nacimientos y sus previsiones extramuestrales se mantienen constantes en el tiempo. Conforme se avanza en el horizonte temporal las previsiones se van alejando de la realidad y el error extramuestral es del 5.0%, más de un p.p. por encima del error de estimación intramuestral de 3.8%.\n\n\nSerie Demanda eléctrica\nPara la serie de consumo eléctrico vamos a reservar las 8 últimas semanas (56 días) como periodo extramuestral.\n\nelectricidadIntra &lt;- subset(electricidad, end = length(electricidad) - 56)\nelectricidadExtra &lt;- subset(electricidad, start = length(electricidad) - 55)\n\nelectricidadExtraPre &lt;- snaive(electricidadIntra, h = 56)\n\naccuracy(electricidadExtraPre, electricidadExtra)\n\n\n\n                ME  RMSE   MAE   MPE  MAPE MASE ACF1 Theil's U\nTraining set -0.91 37.31 25.83 -0.29  3.79 1.00 0.68        NA\nTest set     62.80 83.66 70.17  8.74 10.01 2.72 0.59      1.37\n\n\nEl error intramuestral obtenido es del 3.8%, que aumenta más de 6 p.p. al obtener el error de previsión extramuestral (10%). El elevado valor positivo del error medio indica que las previsiones extramuestrales subestiman el consumo real de electricidad debido a que no capturan el aumento del consumo asociado a un mayor gasto en calefacción.\n\n\n\n\n\n\n\n\nFigura 7: Consumo eléctrico, predicción intra- y extramuestral"
  },
  {
    "objectID": "03-02-Tema2.html#origen-de-predicción-móvil",
    "href": "03-02-Tema2.html#origen-de-predicción-móvil",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "4.2 Origen de predicción móvil",
    "text": "4.2 Origen de predicción móvil\nVeamos ahora una técnica, basada en el concepto de validación cruzada que permite obtener de forma individualizada los errores de previsión extramuestral a un periodo vista, a dos periodos vista, etc.\nSupongamos que para estimar el modelo se necesita un mínimo de \\(k\\) observaciones y que se desea predecir hasta un horizonte temporal \\(h\\).\n\nSeleccionamos las observaciones \\(1,2,...,k\\) para estimar el modelo y predecimos las observaciones desde \\(k+1\\) hasta \\(k+h\\). Tenemos, por tanto, \\(h\\) predicciones.\nCalculamos el error de predicción para las predicciones desde \\(k+1\\) hasta \\(k+h\\). Tenemos \\(h\\) errores, el primero a un periodo vista, el segundo a dos periodos vista y el último a \\(h\\) perriodos vista.\nRepetimos este proceso desplazando el número de observaciones seleccionadas para la estimación un periodo adelante. Es decir, ahora usamos las observaciones \\(2,3,...,k+1\\) para estimar el modelo, predecimos las observaciones desde \\(k+2\\) hasta \\(k+1+h\\) y calculamos el error de predicción.\nIteramos el proceso, desplazando cada vez las observaciones de la estimación un periodo adelante.\nEn general para \\(i=0,1,...,T-k-h\\), donde \\(T\\) es el número total de observaciones:\n\nSeleccionamos las observaciones \\(i+1,i+2,...,i+k\\) para estimar el modelo.\nPredecimos las observaciones desde \\(i+k+1\\) hasta \\(i+k+h\\).\nCalculamos el error de predicción para las observaciones desde \\(i+k+1\\) hasta \\(i+k+h\\).\nPara cada horizonte temporal de predicción se calcula la medida de error deseada.\n\n\n\n\n\n\n\n\n\n\nFigura 8: Descripción del proceso de origen de predicción móvil\n\n\n\nEste procedimiento se denomina origen de predicción móvil, en inglés rolling forecast origin o rolling windows.\nCuando se aplica esta metodología hay que tener en cuenta que los resultados pueden depender del número \\(k\\) de datos usados para la estimación del modelo.\n\nEjemplo de aplicación con Nacimientos\nVamos a aplicar la metodología previa a la serie anual de Nacimientos. Asumimos que se precisan veinte años para hacer una buena estimación, \\(k=20\\), y que el horizonte temporal es de cinco años, \\(h = 5\\). Como la serie es anual, usaremos el método de la deriva para predecir. La siguiente rutina permite obtener el MAPE para previsiones con un horizonte temporal desde uno a cinco años.\n\nnacAnual &lt;- aggregate(nacimientos, FUN = sum)\n\nk &lt;- 20                   #Minimo numero de datos para estimar\nh &lt;- 5                    #Horizonte de las prediciciones\nTT &lt;- length(nacAnual)    #Longitud serie\ns &lt;- TT - k - h           #Total de estimaciones\n\nmapeRwf &lt;- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set &lt;- subset(nacAnual, start = i + 1, end = i + k)\n  test.set &lt;-  subset(nacAnual, start = i + k + 1, end = i + k + h)\n  \n  fcast &lt;- rwf(train.set, h = h, drift = TRUE)\n  mapeRwf[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n}\n\nLa matriz mapeRwf contiene los errores de previsión extramuestral, donde cada columna corresponde a un horizonte temporal de previsión diferente. Ahora vamos a calcular el error medio por columna.\n\nmapeRwfMedia &lt;- colMeans(mapeRwf)\nround(mapeRwfMedia, 2)\n\n[1]  4.20  8.21 12.29 16.39 20.45\n\n\nPara evitar el posible efecto datos atípicos, se puede calcular el error mediano en lugar del error medio. En este caso no hay una función directa en R y usaremos apply.\n\nmapeRwfMediana &lt;- apply(mapeRwf, MARGIN = 2, FUN = median)\nround(mapeRwfMediana, 2)\n\n[1]  4.21  8.73 12.83 18.03 21.59\n\n\nEl error de previsión extramuestral (medio o mediano) crece gradualmente con el horizonte de previsión. Para el primer año el error de predicción se mantiene en un moderado 4.2%, para el segundo año de predicción el MAPE salta al 9% y para los restantes años sigue creciendo. Como la serie de Nacimientos no presenta años especialmente atípicos, para todos los horizontes de previsión el error medio y mediano es muy similar.\n\n\nEjemplo de aplicación con Demanda eléctrica\nAhora aplicaremos la metodología origen de predicción móvil la serie de Demanda eléctrica. En este caso se asumirá que se precisan veinte semanas para hacer una buena estimación, \\(k = 140\\), y que el horizonte temporal es de 4 semanas, \\(h = 28\\). Como la serie tiene estacionalidad, usaremos el método ingenuo con estacionalidad para predecir. La siguiente rutina permite obtener el RMSE para previsiones con un horizonte temporal desde uno a 28 días.\n\nk &lt;- 140                  #Minimo numero de datos para estimar\nh &lt;- 28                   #Horizonte de las prediciciones\nTT &lt;- length(electricidad)#Longitud serie\ns &lt;- TT - k - h           #Total de estimaciones\n\nrmseRwf &lt;- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set &lt;- subset(electricidad, start = i + 1, end = i + k)\n  test.set &lt;-  subset(electricidad, start = i + k + 1, end = i + k + h)\n  \n  fcast &lt;- snaive(train.set, h = h)\n  rmseRwf[i + 1,] &lt;- (test.set - fcast$mean)^2\n}\n\nrmseRwfMedia &lt;- sqrt(colMeans(rmseRwf))\nround(rmseRwfMedia, 2)\n\n [1] 32.35 32.34 32.87 32.87 32.89 32.92 33.18 42.63 42.95 43.21 43.46 43.71\n[13] 44.02 44.29 48.64 48.71 48.74 48.75 48.72 48.56 48.81 57.07 57.39 57.46\n[25] 57.46 57.47 57.45 57.58\n\nrmseRwfMediana &lt;- sqrt(apply(rmseRwf, MARGIN = 2, FUN = median))\nround(rmseRwfMediana, 2)\n\n [1] 18.93 18.93 18.97 18.97 18.97 19.23 19.33 24.40 24.58 24.70 24.97 25.44\n[13] 25.68 26.62 30.02 30.50 30.50 30.50 30.50 30.02 30.02 37.66 37.87 38.39\n[25] 38.39 38.39 38.39 38.96\n\n\nEn este caso, la presencia de muchos días atípicos (todos los festivos entre semana el consumo de electricidad es notablemente más bajo de lo esperado) y de una segunda componente estacional dentro del año hacen que el error medio sea más elevado que el mediano –aproximadamente, el primero es 18 GWh mayor que el segundo.\nRespecto del error de previsión mediano, este crece semana tras semana con el horizonte de previsión. Para predicciones de uno a siete periodos vista el error de predicción está alrededor de los 13-14 GWh; desde 8 a 21 días vista sube hasta los 18 GWh; y de 22 a 28 días vista supera los 19 GWh.\n\n\n\n\n\n\n\n\nCódigo para cálculo del error deseado\n\n\n\nPara Nacimientos hemos visto el código necesario para calcular el MAPE por origen de predicción móvil. Las dos líneas clave del código son:\n\n\nmapeRwf[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\nmapeRwfMedia &lt;- colMeans(mapeRwf)\n\n\nLa primera línea obtiene el error porcentual absoluto de cada observación y la segunda calcula la media de estos errores.\n\n\nPara calcular el RMSE en el ejemplo de Demanda eléctrica ha sido necesario adaptar estás dos líneas de código adecuadamente. Ahora la primera línea calcula el error cuadrático de cada observación y la segunda la raíz de la media de estos errores:\n\n\nrmseRwf[i + 1,] &lt;- (test.set - fcast$mean)^2\nrmseRwfMedia &lt;- sqrt(colMeans(rmseRwf))\n\n\nEn general, según el error que desees obtener, deberás modificar estas dos líneas de código como corresponda.\n\n\nAdemás, si hay valores atípicos, puede ser conveniente calcular el error mediano en lugar del error medio."
  },
  {
    "objectID": "03-02-Tema2.html#introducción-1",
    "href": "03-02-Tema2.html#introducción-1",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.1 Introducción",
    "text": "5.1 Introducción\nLos métodos de alisado exponencial aparecen en los años 50 del siglo pasado de la mano de Brown, Holt y Winters (véase Brown 1959; Holt 2004; Winters 1960) y han sido la raíz de uno de los métodos de predicción más sencillos y eficaces. La idea básica es predecir usando una media ponderada de los datos pasados, donde los más recientes tienen un peso mayor y este decae exponencialmente conforme usamos observaciones más antiguas.\nEl alisado exponencial es una familia de métodos de ajuste y previsión que ofrece muy buenos resultados para predicciones a corto plazo o para predecir series con pocos datos o sencillas (sin mucho ruido).\nSuponen un grado de modelización mayor que los métodos sencillos vistos previamente, pero sin alcanzar la complejidad de otras metodologías (modelos ARIMA).\nEn origen, son métodos descriptivos con el único objetivo de producir predicciones puntuales. Sin embargo, su enfoque como modelos de espacio de estados posibilita un marco teórico para obtener intervalos de confianza de las predicciones."
  },
  {
    "objectID": "03-02-Tema2.html#componentes-de-una-serie-en-el-contexto-del-alisado-exponencial",
    "href": "03-02-Tema2.html#componentes-de-una-serie-en-el-contexto-del-alisado-exponencial",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.2 Componentes de una serie en el contexto del alisado exponencial",
    "text": "5.2 Componentes de una serie en el contexto del alisado exponencial\nPara obtener una predicción en el periodo \\(t+1\\) con datos hasta el periodo \\(t\\) necesitamos tres componentes:\n\nLa estimación del nivel de la serie en el periodo \\(t\\): \\(l_t\\)\nLa estimación de la pendiente de la serie en el periodo \\(t\\): \\(b_t\\)\nLa estimación de la estacionalidad en el mes correspondiente al periodo \\(t+1\\) con datos hasta \\(t\\): \\(s_{t + 1 - m}\\) (recuerda, \\(m\\) es el orden estacional\n\nA partir de estas componentes obtenidas en el periodo \\(t\\) y para un esquema aditivo, se tendría que la predicción en el periodo \\(t+1\\) es:\n\\[\\widehat{y}_{t+1} = l_t+b_t+s_{t+1-m}.\\]\nEn general, las componentes pueden existir o no y se pueden combinar entre ellas aditiva o multiplicativamente. Veamos algunos casos:\n\nExisten todas y son multiplicativas: \\(\\widehat{y}_{t+1}=l_t \\cdot b_t \\cdot s_{t + 1 - m}\\)\nExisten todas, nivel y pendiente aditivas, y estacionalidad multiplicativa: \\(\\widehat{y}_{t+1}=(l_t+b_t)s_{t + 1 - m}\\)\nNo hay pendiente y la estacionalidad es aditiva: \\(\\widehat{y}_{t+1}=l_t+s_{t + 1 - m}\\)\n\n\n\n¿Como obtenemos los valores de \\(l_t\\), \\(b_t\\) y \\(s_{t + 1 - m}\\)? Mediante expresiones recursivas, donde cada componente se calcula a partir de los valores hasta \\(t\\) de la serie y de las componentes:\n\\[\n\\begin{aligned}\nl_t& = f_l(y_t,y_{t-1}\\ldots, l_{t-1},l_{t-2}\\ldots,b_{t-1},b_{t-2}\\ldots,s_{t-1},s_{t-2}\\ldots) \\\\\nb_t& = f_b(y_t,y_{t-1}\\ldots, l_{t},l_{t-1}\\ldots,b_{t-1},b_{t-2}\\ldots,s_{t-1},s_{t-2}\\ldots) \\\\\ns_t& = f_s(y_t,y_{t-1}\\ldots, l_{t},l_{t-1}\\ldots,b_{t},b_{t-1}\\ldots,s_{t-1},s_{t-2}\\ldots)\n\\end{aligned}\n\\]\nPor ejemplo, el método ingenuo I se puede interpretar dentro de este contexto como un método de alisado donde \\(l_t = y_t\\) y no hay ni pendiente ni estacionalidad. Por tanto, \\(\\widehat{y}_{t+1} = l_{t} = y_{t}\\).\nDe la misma forma, el método ingenuo II se puede interpretar como un método de alisado donde \\(l_t = y_t\\), \\(b_t = y_t - y_{t-1}\\) y no hay estacionalidad. Entonces, \\(\\widehat{y}_{t+1}=l_t + b_t = y_t + (y_t - y_{t-1})\\).\nEn las expresiones previas hemos supuesto que se quería obtener una predicción a un periodo vista (\\(\\widehat{y}_{t+1}\\)). Si el objetivo es estimar una previsión \\(h\\) periodos hacia delante desde el periodo \\(t\\), \\(\\widehat{y}_{t+h}\\), hay que modificar la ecuación de predicción adecuadamente. Por ejemplo, para el caso aditivo se tendría que\n\\[\\widehat{y}_{t+h} = l_t+hb_t+s_{t+h-m(k+1)}\\] donde \\(k = \\lfloor (h-1)/m\\rfloor\\).\nEl concepto de componentes aquí visto no coincide con el definido en el Tema 1. Sin embargo, podemos asimilar la tendencia de una serie como la suma (o multiplicación) del nivel y la pendiente \\(T_{t+1} = l_t + b_t\\) (o \\(T_{t+1} = l_t \\cdot b_t\\)) y de esta forma ambas definiciones de componentes de una serie se hacen compatibles."
  },
  {
    "objectID": "03-02-Tema2.html#casos-posibles",
    "href": "03-02-Tema2.html#casos-posibles",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.3 Casos posibles",
    "text": "5.3 Casos posibles\nTodas las series tiene nivel, pero dependiendo del tipo de pendiente y estacionalidad hay 15 casos posibles, mostrados en la Tabla 1.\n\n\n\nTabla 1: Casos de alisado según el tipo de tendencia y estacionalidad\n\n\n\n\n\n\n\n\n\n\n\nTendencia\n\nEstacionalidad\n\n\n\n\n\n\nNinguna (N)\nAditiva (A)\nMultiplicativa (M)\n\n\nNinguna (N)\nN, N\nN, A\nN, M\n\n\nAditiva (A)\nA, N\nA, A\nA, M\n\n\nAditiva Amortiguada (Ad)\nAd, N\nAd, A\nAd, M\n\n\nMultiplicativa (M)\nM, N\nM, A\nM, M\n\n\nMultiplicativa Amortiguada (Md)\nMd, N\nMd, A\nMd, M\n\n\n\n\n\n\nCada caso difiere en las componentes que se observan y su esquema, dando lugar a un conjunto diferente de ecuaciones recursivas de actualización.\n\n\nResiduo aditivo versus residuo multiplicativo\nTodo modelo estimado tiene asociado un residuo. En los modelos usuales este error se define como, \\(\\widehat{\\varepsilon}_t = y_t - \\widehat{y}_t\\), pero no tiene por que ser así,. En los modelos de Alisado el residuo estimado puede ser aditivo o multiplicativo.\nSi el residuo es aditivo, entonces el modelo es \\(y_t = \\widehat{y}_t + \\widehat{\\varepsilon}_t\\) y el residuo se define de la forma usual \\[\\widehat{\\varepsilon}_t = y_t - \\widehat{y}_t.\\]\nAhora bien, si el residuo es multiplicativo, entonces el modelo es \\(y_t = \\widehat{y}_t \\cdot (1 + \\widehat{\\varepsilon}_t)\\), y no \\(y_t = \\widehat{y}_t \\cdot \\widehat{\\varepsilon}_t\\) como se podría esperar. Por tanto, el residuo multiplicativo se define como \\[\\widehat{\\varepsilon}_t = (y_t - \\widehat{y}_t)/\\widehat{y}_t.\\]\nDe esta forma en ambos casos –aditivo y multiplicativo– el residuo evoluciona alrededor del valor 0 y se le pueden imponer las hipótesis usuales de ruido blanco. Observa que el error multiplicativo tampoco es el error porcentual tal y como se define para el calculo del MPE o del MAPE.\nEn cualquiera de los casos y para cualquier modelo estimado con R, podemos obtener el residuo con la función residuals.\n\n\nCasos más comunes\nSi a la Tabla 1 se añade que el error puede ser aditivo (A) o multiplicativo (M), tenemos 30 posibilidades. El tipo de error (aditivo o multiplicativo) es, sobre todo, relevante en el cálculo del intervalo de confianza de las predicciones.\nLos modelos más usuales son (error, tendencia, estacionalidad):\n\n(A, N, N): Alisado exponencial simple\n(A, A, N): Alisado de Holt\n(A, Ad, N): Alisado con tendencia amortiguada (d de damped)\n(A, A, A): Alisado de Holt-Winters aditivo\n(M, A, M): Alisado de Holt-Winters multiplicativo1\n\nAcude al artículo de Hyndman and Khandakar (2008) para saber más de cada modelo, o al libro de Hyndman et al. (2008).\n\nLas funciones ets y forecast\nPodemos estimar cualquiera de los treinta modelos usando la función ets del paquete forecast.\n\nEl tipo de modelo en ets se especifica con el argumento model, un código de tres letras indicando el tipo de Error, Tendencia y eStacionalidad (ETS). Por ejemplo, model = \"ANN\" indica un modelo con error aditivo, sin tendencia ni estacionalidad, es decir, el alisado exponencial simple; model = \"AAN\" indica un modelo con error aditivo, pendiente aditiva, pero sin estacionalidad, el alisado exponencial de Holt. El alisado exponencial de Holt-Winters multiplicativo sería model = \"MAM\".\nSi se desea incluir amortiguamiento, hay que añadir el argumento damped = TRUE.\nPor defecto ets no considera modelos con tendencia multiplicativa (últimas dos líneas de la Tabla 1). Debes fijar el parámetro allow.multiplicative.trend=TRUE para contemplar esta opción.\n\nA diferencia de las funciones vistas en el epígrafe 3.1 (naive, meanf, rwf y snaive), la función ets solo estima los modelos, pero no produce predicciones. Para ello habrá que usar la función forecast sobre un modelo estimado con ets. El argumento h de esta función especifica el horizonte temporal de predicción. También puedes usar level para fijar el nivel de confianza del intervalo de predicción.\nMira la ayuda de R para ver una explicación detallada de los argumentos de estas las funciones ets y forecast."
  },
  {
    "objectID": "03-02-Tema2.html#alisado-exponencial-simple-a-n-n",
    "href": "03-02-Tema2.html#alisado-exponencial-simple-a-n-n",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.4 Alisado exponencial simple (A, N, N)",
    "text": "5.4 Alisado exponencial simple (A, N, N)\n\n\n\nDefinición\nEl alisado exponencial simple es adecuado para una serie estacionaria y sin estacionalidad. Es decir, una serie que se mueve alrededor de un nivel medio desconocido: \\(y_t = \\mu + \\varepsilon_t\\). Por tanto, para obtener una predicción en el periodo \\(t+1\\) necesitamos la estimación del nivel de la serie con la información disponible hasta el periodo \\(t\\). Denominaremos a este nivel \\(l_t\\), de esta forma se tendrá que: \\[\\widehat{y}_{t+1} = l_t.\\]\nEs decir, \\(l_t\\) es la estimación del nivel desconocido \\(\\mu\\) con información hasta el periodo \\(t\\).\n¿Pero cómo obtenemos \\(l_t\\)? Mediante una expresión recursiva donde el nivel \\(l_t\\) se calcula a partir de los valores hasta \\(t\\) de la serie y los valores pasados estimados para el nivel. En concreto, para el Alisado exponencial simple la ecuación recursiva de suavizado es \\[l_t=\\alpha y_t + (1-\\alpha)l_{t-1}.\\] Dos estimaciones razonables de \\(l_t\\), el nivel de la serie en el periodo \\(t\\), son el valor observado para la serie en ese periodo \\(y_t\\) y el nivel del periodo previo \\(l_{t-1}\\). La estimación final de \\(l_t\\) es una media ponderada de ambas según el parámetro \\(\\alpha\\), y esta estimación final es la previsión de la serie para el periodo siguiente.\nEl parámetro \\(\\alpha\\) se denomina parámetro de suavizado y verifica que \\(0 \\leq \\alpha \\leq 1\\).\nYa hemos comentado que la ecuación de predicción intramuestral es \\(\\widehat{y}_{t+1} = l_t\\). La ecuación de predicción extramuestral es \\[\\widehat{y}_{T+h} = \\widehat{y}_{T+1} = l_T.\\] El método de Alisado también ofrece predicciones constantes para series sin tendencia ni estacionalidad.\n\n\nEstimación de los parámetros del modelo\nDado el proceso iterativo para el cálculo de \\(l_t\\) se necesita un valor inicial de arranque \\(l_0\\). Cada programa estadístico usa su propio método para obtener \\(l_0\\).\nRespecto de \\(\\alpha\\), usualmente se estima el valor optimo según un criterio de calidad de ajuste. El parámetro \\(\\alpha\\) se puede interpretar:\n\nSi \\(\\alpha = 1\\), la ecuación recursiva queda \\(l_t = y_t\\), es decir, el método ingenuo I (\\(\\widehat{y}_{t+1}=y_t\\)). Este caso es óptimo cuando el nivel de la serie varía constantemente en el tiempo.\nSi \\(\\alpha = 0\\), la ecuación recursiva queda \\(l_t=l_{t-1}=\\ldots = l_0\\), es decir, \\(\\hat y_{t+1}= l_0\\). Esto es óptimo cuando el nivel permanece constante en el tiempo.\n\nEn concreto, ets por defecto estima los parámetros \\(\\alpha\\) y \\(l_0\\) maximizando la función de verosimilitud, pero el argumento opt.crit permite cambiar de criterio. Esta búsqueda está restringida a \\(0 &lt; \\alpha &lt; 1\\). Es decir el parámetro \\(\\alpha\\) nunca puede ser 0 o 1 y en la práctica sus valores limite son 0.0001 y 0.9999.\n\n\nEjemplo de aplicacion a la serie consumo de alimentos en hogar\nAnalizaremos el consumo alimentario en hogar per cápita en España. Esta serie está construida a partir de la serie de consumo alimentario en hogar (disponible en el Ministerio de Agricultura, Alimentación y Medio Ambiente), y la serie de población (disponible en el Instituto Nacional de Estadística). Es una serie anual de 1990 a 2024 (35 datos) y la unidad es el Kg per cápita. La Figura 9 muestra que es una serie estacionaria.\n\nalimentospc &lt;- read.csv2(\"./series/Alimentacionpc.csv\",\n                         header = TRUE)\n\nalimentospc &lt;- ts(alimentospc,\n                  start = 1990, \n                  freq = 1)\n    \nautoplot(alimentospc, \n         xlab = \"\", \n         ylab = \"Kg per cápita\",\n         main = \"\",\n         ylim = c(0, 700))\n\n\n\n\n\n\n\nFigura 9: Consumo alimentario en hogar\n\n\n\n\n\nEl pico en el año 2020 se debe al aumento del consumo de alimentos en el hogar causado por el periodo de confinamiento por la Covid-19 (marzo a junio) y el aumento del trabajo en casa. La caída de consumo en los años posteriores se debe a un aumento del gasto en ocio, que incluye la restauración.\nVamos a usar el método de Alisado exponencial simple para predecir la serie Consumo de alimentos en el hogar. Usaremos para ello la función ets con model = \"ZNN\".\n\netsAlimentospc &lt;- ets(alimentospc, \n                      model = \"ZNN\")\n\nsummary(etsAlimentospc)\n\nETS(A,N,N) \n\nCall:\nets(y = alimentospc, model = \"ZNN\")\n\n  Smoothing parameters:\n    alpha = 0.9578 \n\n  Initial states:\n    l = 625.3094 \n\n  sigma:  21.9533\n\n     AIC     AICc      BIC \n344.6019 345.3761 349.2679 \n\nTraining set error measures:\n                    ME    RMSE      MAE        MPE     MAPE      MASE\nTraining set -2.250486 21.3168 13.81233 -0.4446267 2.211916 0.9818223\n                     ACF1\nTraining set 0.0007501672\n\n\nVeamos la salida en detalle:\n\nEl modelo óptimo tiene error aditivo: ETS(A,N,N).\nEl valor de \\(\\alpha\\) óptimo (minimiza la verosimilitud) es \\(\\alpha =\\) 0.96, un valor muy cercano a 1. Es decir, el nivel de Alimentos varía constantemente en el tiempo.\nEl valor de arranque \\(l_0\\) óptimo es 625.31. Es decir, el año 1989 (año anterior al primero de la serie) se estima un nivel de consumo de 625.31 kg per cápita.\nsigma es la desviación típica del error (aditivo) de predicción. Se diferencia de RMSE en el denominador. Para calcular sigma en lugar de dividir por \\(T\\) se divide por \\(T-k\\). En este caso \\(k=3\\): \\(l_0\\), \\(\\alpha\\) y sigma. Sí, sigma se considerará siempre otro parámetro estimado.\nSe obtiene un AICc \\(= 345.38\\). Si estimamos un modelo con error multiplicativo, obtendremos un AICc mayor.\nLa calidad del ajuste es buena, como evidencia el error porcentual medio del 2.2%. Además, no hay sesgo (|MPE| = 0.44% &lt; 1%) y el cálculo de la predicciones por intervalo es correcto (|ACF1| = 0.00075 &lt; 0.1).\nComo el valor de \\(\\alpha\\) es próximo a 1, el modelo de Alisado se aproxima mucho al Ingenuo I. Por este motivo el MASE es casi igual a 1.\n\nEn el objeto etsAlimentospc la matriz etsAlimentospc$states guarda todos los valores del nivel obtenidos con la ecuación recursiva, incluido el valor de arranque, así que es una matriz con \\(T+1\\) filas (36 en el ejemplo). Puedes ver en su última fila que el valor de \\(l_{2024}\\), el nivel del último año, vale 549.87.\n\ntail(etsAlimentospc$states, 1)\n\nTime Series:\nStart = 2024 \nEnd = 2024 \nFrequency = 1 \n            l\n[1,] 549.8672\n\n\nPor tratarse de un modelo sin pendiente ni estacionalidad, la predicción es constante en el tiempo. Recuerda que \\(\\widehat{y}_{T+h} = l_T\\). Así, la predicción para 2025 es \\(\\widehat{y}_{2025}=l_{2024}=\\) 549.87. Igualmente \\(\\widehat{y}_{2026}=l_{2024}=\\) 549.87. Todas las previsiones son iguales a \\(l_{2024}\\).\nMediante la función forecast podemos predecir el consumo de alimentos per cápita para los próximos cinco años y obteenr el intervalo de confianza de las predicciones.\n\netsAlimentospcf &lt;- forecast(etsAlimentospc,\n                            h = 5, \n                            level = 95)\n\netsAlimentospcf\n\n     Point Forecast    Lo 95    Hi 95\n2025       549.8672 506.8396 592.8948\n2026       549.8672 490.2874 609.4470\n2027       549.8672 477.4232 622.3112\n2028       549.8672 466.5214 633.2129\n2029       549.8672 456.8893 642.8451\n\n\nLa Figura 10 muestra la serie Consumo de alimentos, las previsiones extramuestrales que son constantes y el intervalo de confianza. Conforme aumentamos el horizonte de predicción, el intervalo de confianza es más amplio como reflejo de la mayor incertidumbre en la predicción.\n\nautoplot(etsAlimentospcf,\n         xlab = \"\",\n         ylab = \"Kg per cápita\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 10: Consumo de alimentos per cápita (1990 - 2024) y predicción con Alisado exponencial simple"
  },
  {
    "objectID": "03-02-Tema2.html#alisado-exponencial-de-holt-a-a-n",
    "href": "03-02-Tema2.html#alisado-exponencial-de-holt-a-a-n",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.5 Alisado exponencial de Holt (A, A, N)",
    "text": "5.5 Alisado exponencial de Holt (A, A, N)\nEl alisado exponencial de Holt es adecuado para una serie no estacionaria y sin estacionalidad.\n\n\n\nFormulas interactivas de sus componentes\nLas ecuaciones recursivas son\n\\[\n\\begin{aligned}\nl_t & =\\alpha y_t + (1-\\alpha)(l_{t-1}+b_{t-1}) \\\\\nb_t & =\\beta (l_t - l_{t-1}) + (1-\\beta)b_{t-1}\n\\end{aligned}\n\\]\nLa ecuación de la predicción intramuestral a un periodo vista es\n\\[\\widehat{y}_{t+1} = l_t + b_t,\\] de forma que la ecuación de predicción extramuestral es \\[\\widehat{y}_{T+h}=l_T + h b_T.\\]\nDos estimaciones razonables del nivel de la serie en el periodo \\(t\\) son el valor observado para la serie en ese periodo \\(y_t\\), y una estimación del nivel del periodo \\(t\\) realizada desde el periodo \\(t-1\\): \\(l_{t-1} + b_{t-1}\\). Por otro lado, dos estimaciones razonables de la pendiente de la serie en el periodo \\(t\\) son el cambio de nivel de \\(t-1\\) a \\(t\\) (el último observado) \\(l_t-l_{t-1}\\), y el valor de la pendiente en el periodo previo, \\(b_{t-1}\\). En ambos casos, nivel y pendiente, la estimación final es una media ponderada, parametrizada por \\(0 &lt; \\alpha, \\beta &lt; 1\\).\nObserva que el método ingenuo II es un caso concreto de Alisado de Holt. Si hacemos \\(\\alpha=\\beta = 1,\\) queda \\(l_t=y_t\\) y \\(b_t=y_t-y_{t-1}\\), por tanto \\[\\widehat{y}_{t+1}=l_t + b_t = y_t + (y_t - y_{t-1})\\] y \\[\\widehat{y}_{T+h}=l_T + h \\cdot b_T = y_T + h(y_T - y_{T-1}).\\]\n\n\n\n\n\n\n¿Sabrías responder a estas preguntas?\n\n\n\nHemos visto que en modelo de Alisado con tendencia, si \\(\\alpha = \\beta = 1\\), la ecuación de predicción que queda es la del método Ingenuo II.\n¿Cómo quedaría la ecuación de predicción si \\(\\alpha = \\beta = 0\\)?\n¿Y si \\(\\alpha = 1\\) y \\(\\beta = 0\\)? ¿Y si \\(\\alpha = 0\\) y \\(\\beta = 1\\)?\n\n\n\n\nEstimación de los parámetros del modelo\nPara aplicar este método es necesario estimar unos valores iniciales \\(l_0\\) y \\(b_0\\) de las ecuaciones recursivas e identificar los valores más adecuados de los parámetros \\(\\alpha\\) y \\(\\beta\\).\nLa función ets por defecto estima los parámetros \\(\\alpha\\), \\(\\beta\\), \\(l_0\\) y \\(b_0\\) maximizando la función de verosimilitud. En este caso la búsqueda está restringida a \\(0 &lt; \\beta &lt; \\alpha &lt; 1\\). Por tanto, \\(\\alpha\\) y \\(\\beta\\) nunca pueden ser 0 o 1 y en la práctica sus valores limite son 0.0001 y 0.9999.\nLa interpretación del parámetro \\(\\alpha\\) es similar al caso del alisado exponencial simple.\nInterpretación del parámetro \\(\\beta\\):\n\nSi \\(\\beta = 1\\), \\(b_t = l_t - l_{t-1}\\), la pendiente se actualiza constantemente porque varía periodo a periodo Puede ser un indicador de mal ajuste (tendencia no lineal o pendiente no aditiva).\nSi \\(\\beta = 0\\), \\(b_t = b_{t-1}= \\ldots = b_0\\), la pendiente se mantiene constante en el tiempo.\n\n\n\nEjemplo de aplicación a la serie Residuos\nVamos a usar el método de alisado de Holt para predecir la serie Residuos. Usaremos para ello la función ets con el argumento model = \"AAN\" (error y tendencia aditivas sin estacionalidad). Además, es necesario añadir el argumento damped = FALSE para prevenir el uso de tendencia amortiguada, que veremos en el siguiente epígrafe.\n\netsResiduos &lt;- ets(residuos, \n                   model = \"AAN\",\n                   damped = FALSE)\n\nsummary(etsResiduos)\n\nETS(A,A,N) \n\nCall:\nets(y = residuos, model = \"AAN\", damped = FALSE)\n\n  Smoothing parameters:\n    alpha = 0.9998 \n    beta  = 0.4929 \n\n  Initial states:\n    l = 479.3655 \n    b = 51.8543 \n\n  sigma:  21.4529\n\n     AIC     AICc      BIC \n281.1673 283.7760 288.0038 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -4.083149 19.91854 16.04773 -0.6775317 2.945818 0.9985256\n                   ACF1\nTraining set 0.01027896\n\n\nLos valores óptimos de los cuatro parámetros estimados son \\(\\alpha=\\) 1, \\(\\beta=\\) 0.49, \\(l_0 =\\) 479.37 y \\(b_0 =\\) 51.85.\nObserva que \\(\\alpha\\) es prácticamente 1, indicando que el nivel de la serie varía de forma constante; por otro lado, \\(\\beta\\) es aproximadamente 0.5, indicando que la pendiente varía moderadamente con el paso de los años. La calidad de las predicciones es razonable, con un error porcentual medio del 2.9%.\nPor otro lado, el valor de \\(l_0\\) indica que el nivel estimado para el volumen de residuos de 1994 es de 479.37. Además, el incremento entre 1994 y 1995 se estima en \\(b_0 =\\) 51.85.\n\n\n\n\n\n\nParámetros estimados\n\n\n\n¿Cuántos parámetros se han estimado (y la respuesta no es 4)? ¿Cuál es el denominador en el cálculo de RMSE y de sigma?\n\n\nEn el objeto etsResiduos la matriz etsResiduos$states guarda todos los valores obtenidos con las ecuaciones recursivas, en este caso el nivel y la pendiente, incluidos los valores de arranque. Puedes ver los valores de \\(l_{2023}\\) y \\(b_{2023}\\) en su última fila, que valen respectivamente\n\ntail(etsResiduos$states, 1)\n\nTime Series:\nStart = 2023 \nEnd = 2023 \nFrequency = 1 \n            l         b\n2023 465.0041 -6.509018\n\n\nAsí, la predicción para \\(2024\\) es \\(\\widehat{y}_{2024}=l_{2023} + b_{2023}=\\) 465.00 \\(+\\) -6.51 \\(=\\) 458.50. Igualmente \\(\\widehat{y}_{2025}=l_{2023} + 2\\cdot b_{2023}=\\) 451.99. Es decir, el variación entre previsiones es constante e igual a \\(b_{2023}\\).\n\netsResiduosf &lt;- forecast(etsResiduos,\n                         h = 5, \n                         level = 95)\n\netsResiduosf\n\n     Point Forecast    Lo 95    Hi 95\n2024       458.4951 416.4481 500.5421\n2025       451.9861 376.4405 527.5317\n2026       445.4771 332.8836 558.0705\n2027       438.9680 285.5491 592.3869\n2028       432.4590 234.6046 630.3135\n\n\nLa Figura 11 muestra la serie Residuos y las previsiones extramuestrales, que muestran una ligera tendencia decreciente.\n\nautoplot(etsResiduosf,\n         xlab = \"\",\n         ylab = \"Kg per cápita\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 11: Residuos y predicción con alisado de Holt"
  },
  {
    "objectID": "03-02-Tema2.html#alisado-exponencial-con-pendiente-amortiguada-a-ad-n",
    "href": "03-02-Tema2.html#alisado-exponencial-con-pendiente-amortiguada-a-ad-n",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.6 Alisado exponencial con pendiente amortiguada (A, Ad, N)",
    "text": "5.6 Alisado exponencial con pendiente amortiguada (A, Ad, N)\nLas previsiones con el método de Holt presentan siempre una pendiente constante. En previsiones a corto plazo esto no es un problema, pero para previsiones a largo plazo la experiencia indica que suele aparecer un sesgo de previsión. El alisado exponencial con pendiente amortiguada trata de corregir esta limitación. El mecanismo, propuesto por Gardner and Mckenzie (1985), es introducir un nuevo parámetro \\(0 \\leq \\phi \\leq 1\\) que amortigua la tendencia hasta hacerla plana en el largo plazo.\n\nFormulas interactivas de sus componentes\nLas ecuaciones recursivas son\n\\[\n\\begin{aligned}\nl_t & =\\alpha y_t + (1-\\alpha)(l_{t-1}+\\phi b_{t-1}) \\\\\nb_t & =\\beta (l_t - l_{t-1}) + (1-\\beta)\\phi b_{t-1}\n\\end{aligned}\n\\]\nLa ecuación de la predicción intramuestral a un periodo vista es \\[\\widehat{y}_{t+1} = l_t + \\phi b_t,\\] de forma que la ecuación de predicción extramuestral es \\[\\widehat{y}_{T+h}=l_T + (\\phi + \\phi^2 + \\ldots + \\phi^h) b_T.\\]\nSe ha añadido un nuevo parámetro \\(\\phi\\in [0, 1]\\) que acompaña siempre a la pendiente \\(b_t\\). Si \\(\\phi = 1\\), se tiene el alisado de Holt y si \\(\\phi = 0\\), se tiene el alisado simple. Se puede comprobar que en el largo plazo las predicciones se hacen constantes e iguales a \\(l_T + \\phi b_T/(1 - \\phi)\\).\nPor razones prácticas el rango de búsqueda de \\(\\phi\\) queda en el intervalo \\([0.8, 0.98]\\). Si el valor óptimo de \\(\\phi\\) fuera su valor máximo de \\(0.98\\) o muy cercano a este, cabría plantearse si no sería más adecuado un modelo sin amortiguamiento.\n\n\nEjemplo de aplicación a la serie Residuos\nVamos a usar el método de Alisado con amortiguamiento para predecir, una vez más, la serie Residuos añadiendo a la función ets el argumento damped = TRUE. En este caso, para ver el efecto del amortiguamiento vamos a pedir un horizonte temporal de previsión más largo.\n\netsDResiduos &lt;- ets(residuos, \n                    model = \"AAN\", \n                    damped = TRUE)\n\nsummary(etsDResiduos)\n\nETS(A,Ad,N) \n\nCall:\nets(y = residuos, model = \"AAN\", damped = TRUE)\n\n  Smoothing parameters:\n    alpha = 0.9766 \n    beta  = 0.3826 \n    phi   = 0.98 \n\n  Initial states:\n    l = 512.9683 \n    b = 28.5828 \n\n  sigma:  22.0143\n\n     AIC     AICc      BIC \n283.4818 287.2999 291.6855 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -2.914249 20.02682 15.48305 -0.4657129 2.852787 0.9633898\n                  ACF1\nTraining set 0.0363016\n\n\nEl valor óptimo del parámetro \\(\\phi\\) es \\(0.98\\) y el error porcentual 2.85%, básicamente el mismo que al obtenido con el alisado de Holt sin amortiguamiento. Sin embargo, el modelo de Alisado con amortiguamiento genera intervalos de predicción correctos, cosa que no estaba tan clara con el modelo de Alisado de Holt.\nAhora bien, ante un valor de amortiguamiento igual a \\(\\phi = 0.98\\), tan próximo a \\(1\\), sería razonable descartar este modelo y volver al modelo de Alisado sin amortiguamiento.\n\n\n\n\n\n\nParámetros estimados\n\n\n\n¿Cuántos parámetros se han estimado en este caso? ¿Cuáles?\n\n\nLa Figura 12 muestra la serie Residuos, su estimación intramuestral y las predicciones a 15 años vista. Observa que la pendiente de las previsiones se amortigua con el paso del tiempo: al principio las previsiones decrecen más rápidamente que en los últimos años.\n\netsDResiduosf &lt;- forecast(etsDResiduos, \n                          h = 15,\n                          level = 95)\n\netsDResiduosf\n\n     Point Forecast        Lo 95     Hi 95\n2024       461.1032  417.9559291  504.2505\n2025       456.8149  384.2729204  529.3569\n2026       452.6124  348.8660108  556.3588\n2027       448.4939  311.3433660  585.6445\n2028       444.4578  271.7711682  617.1445\n2029       440.5024  230.2999688  650.7049\n2030       436.6262  187.0897946  686.1625\n2031       432.8274  142.2914159  723.3634\n2032       429.1046   96.0420917  762.1672\n2033       425.4563   48.4655515  802.4471\n2034       421.8810   -0.3268045  844.0887\n2035       418.3771  -50.2344575  886.9887\n2036       414.9434 -101.1663947  931.0531\n2037       411.5783 -153.0398942  976.1964\n2038       408.2805 -205.7794835 1022.3404\n\nautoplot(etsDResiduosf,\n         xlab = \"\",\n         ylab = \"kg per cápita\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\n\n\n\nFigura 12: Residuos y predicción con alisado exponencial con amortiguamiento"
  },
  {
    "objectID": "03-02-Tema2.html#alisado-de-holt-winters-aditivo-a-a-a-y-multiplicativo-m-a-m",
    "href": "03-02-Tema2.html#alisado-de-holt-winters-aditivo-a-a-a-y-multiplicativo-m-a-m",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.7 Alisado de Holt-Winters aditivo (A, A, A) y multiplicativo (M, A, M)",
    "text": "5.7 Alisado de Holt-Winters aditivo (A, A, A) y multiplicativo (M, A, M)\nEl método de alisado exponencial de Holt-Winters es adecuado para una serie con tendencia y estacionalidad. Existen dos versiones según que el esquema sea aditivo o multiplicativo.\n\n5.7.1 Alisado de Holt-Winters aditivo (A, A, A)\nLas ecuaciones recursivas de actualización son:\n\\[\n\\begin{aligned}\nl_t & =\\alpha (y_t - s_{t-m} ) + (1-\\alpha)(l_{t-1}+b_{t-1}) \\\\\nb_t & =\\beta (l_t - l_{t-1}) + (1-\\beta)b_{t-1} \\\\\ns_t & =\\gamma (y_t - l_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m}\n\\end{aligned}\n\\] con \\(0 \\leq \\alpha, \\beta, \\gamma \\leq 1\\).\nLa ecuación de la predicción intramuestral a un periodo vista es \\[\\widehat{y}_{t+1}  = l_t + b_t + s_{t+1-m},\\] de forma que la ecuación de predicción extramuestral es: \\[\\widehat{y}_{T+h}=l_T + h b_T + s_{T+h - m(k+1)},\\] con \\(k = \\lfloor(h-1)/m\\rfloor\\).\nObserva que las ecuaciones para el nivel y la pendiente son similares a las ya vistas para el método de Holt. Respecto de la ecuación de actualización de la estacionalidad, dos estimaciones razonables de esta componente en el periodo \\(t\\) son su valor estimado a partir de la serie menos su tendencia \\(y_t-l_{t-1} - b_{t-1}\\), y la estimación que ya teníamos de la estacionalidad previamente, \\(s_{t-m}\\). La estimación final es una media ponderada, parametrizada por \\(0 \\leq \\gamma &lt; 1 - \\alpha\\).\nInterpretación del parámetro \\(\\gamma\\):\n\nSi \\(\\gamma = 1\\), \\(s_t = y_t - l_{t-1} - b_{t-1}\\), la estacionalidad se actualiza constantemente porque varía periodo a periodo\nSi \\(\\gamma = 0\\), \\(s_t = s_{t-m}= \\ldots = s_0\\), la estacionalidad se mantiene constante en el tiempo.\n\n\n\n\n\n5.7.2 Alisado de Holt-Winters multiplicativo (M, A, M)\nLas ecuaciones recursivas de actualización son:\n\\[\n\\begin{aligned}\nl_t & =\\alpha \\frac{y_t}{s_{t-m}} + (1-\\alpha)(l_{t-1}+b_{t-1}) \\\\\nb_t & =\\beta (l_t - l_{t-1}) + (1-\\beta)b_{t-1} \\\\\ns_t & =\\gamma \\frac{y_t}{l_{t-1} + b_{t-1}} + (1 - \\gamma)s_{t-m}\n\\end{aligned}\n\\]\nLa ecuación de la predicción intramuestral a un periodo vista es \\[\\widehat{y}_{t+1}  = (l_t + b_t)s_{t+1-m},\\] de forma que la ecuación de predicción extramuestral es: \\[\\widehat{y}_{T+h}=(l_T + h b_T)s_{T+h - m(k+1)}.\\]\n\n\nEjemplo con Demanda electrica\nVamos a usar el método de Holt-Winters Aditivo para predecir la serie Demanda eléctrica, que presentaba un esquema aditivo. Para ello usaremos la función ets con el argumento model = \"AAA\" (y damped = FALSE). Vamos a considerar la serie desde el 5 de febrero (lunes) hasta el 2 de junio (domingo), 17 semanas, y pedir una previsión a dos semanas vista.\nEn el proceso de estimación, el parámetro \\(\\gamma\\) que gobierna la componente estacional está restringido a \\(0 &lt; \\gamma &lt; 1 - \\alpha\\).\n\n#Nos quedamos con los meses de febrero a mayo.\nelectricidadr &lt;- window(electricidad, \n                       start = c(6, 1), \n                       end = c(22, 7)) \n\nelectricidadEts &lt;- ets(electricidadr, \n                       model = \"AAA\", \n                       damped = FALSE)\n\nsummary(electricidadEts)\n\nETS(A,A,A) \n\nCall:\nets(y = electricidadr, model = \"AAA\", damped = FALSE)\n\n  Smoothing parameters:\n    alpha = 0.6529 \n    beta  = 1e-04 \n    gamma = 2e-04 \n\n  Initial states:\n    l = 730.3177 \n    b = -0.7317 \n    s = -82.5663 -42.14 24.6768 31.6077 24.791 28.1604\n           15.4703\n\n  sigma:  19.9488\n\n     AIC     AICc      BIC \n1293.548 1296.492 1326.898 \n\nTraining set error measures:\n                     ME     RMSE     MAE         MPE    MAPE      MASE\nTraining set 0.02358221 19.00449 11.5782 -0.04038823 1.79318 0.5237777\n                  ACF1\nTraining set 0.1161587\n\n\nLos valores óptimos de los parámetros son \\(\\alpha=\\) 0.65, \\(\\beta=\\) 0 y \\(\\gamma=\\) 0. Los valores nulos para \\(\\beta\\) y \\(\\gamma\\) indican que ambas, la pendiente y la estacionalidad, permanecen constantes en el tiempo (véase Figura 13).\n\nautoplot(electricidadEts)\n\n\n\n\n\n\n\nFigura 13: Componentes del modelo de Holt-Winters aditivo para Demanda eléctrica\n\n\n\n\n\nLa calidad de las predicciones es notable, con un error porcentual medio del 1.8%.\nLos últimos valores de las componentes son,\n\nTT &lt;- nrow(electricidadEts$states)\nelectricidadEts$states[TT,]\n\n\n\n      l       b      s1      s2      s3      s4      s5      s6      s7 \n644.537  -0.731 -82.564 -42.136  24.674  31.602  24.796  28.159  15.469 \n\n\nComo el último dato de la serie es domingo 2 de junio, los valores del nivel \\(l\\) y la pendiente \\(b\\) mostrados corresponden a ese día. Sin embargo, los valores de la componente estacional están ordenados al revés partiendo de domingo: s1 es el valor estacional para domingo (día del último dato, 2 de junio), s2 el de sábado, s3 de viernes, hasta s7 que sería lunes. Podemos reproducir las predicciones para los próximos 7 días, 3 a 9 de junio (ojo, el etiquetado de la salida no tiene sentido):\n\nelectricidadEts$states[TT, 1] + (1:7)*electricidadEts$states[TT, 2] + \n  electricidadEts$states[TT, 9:3]\n\n      s7       s6       s5       s4       s3       s2       s1 \n659.2750 671.2336 667.1390 673.2131 665.5539 598.0128 556.8534 \n\n\nO mejor usar la función forecast para obtener las predicciones y sus intervalos a dos semanas vista.\n\nelectricidadf &lt;- forecast(electricidadEts,\n                          h = 14, \n                          level = 95)\n\nelectricidadf\n\n         Point Forecast    Lo 95    Hi 95\n23.00000       659.2750 620.1760 698.3740\n23.14286       671.2336 624.5360 717.9312\n23.28571       667.1390 613.9148 720.3632\n23.42857       673.2131 614.1777 732.2485\n23.57143       665.5539 601.2286 729.8793\n23.71429       598.0128 528.7992 667.2265\n23.85714       556.8534 483.0732 630.6337\n24.00000       654.1553 576.0716 732.2390\n24.14286       666.1139 583.9526 748.2753\n24.28571       662.0193 575.9721 748.0665\n24.42857       668.0935 578.3272 757.8597\n24.57143       660.4343 567.0960 753.7726\n24.71429       592.8932 496.1134 689.6729\n24.85714       551.7338 451.6298 651.8377\n\n\nLa Figura 14 muestra la serie Demanda eléctrica y las previsiones extramuestrales.\n\nautoplot(electricidadf,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\n\n\n\nFigura 14: Demanda eléctrica y predicción con alisado de Holt-Winters aditivo\n\n\n\n\n\n\n\nEjemplo con Nacimientos\nVamos a usar el método de Holt-Winters multiplicativo para predecir la serie Nacimientos, que presentaba un esquema multiplicativo. En este caso usaremos el argumento model = \"MAM\". Vamos a considerar la serie Nacimientos desde enero de 2000 y pedir una previsión a dos años vista.\n\nnacimientosb &lt;- window(nacimientos, start = 2000)\n\nnacimientosbEts &lt;- ets(nacimientosb, \n                       model = \"MAM\", \n                       damped = FALSE)\n\nsummary(nacimientosbEts)\n\nETS(M,A,M) \n\nCall:\nets(y = nacimientosb, model = \"MAM\", damped = FALSE)\n\n  Smoothing parameters:\n    alpha = 0.6035 \n    beta  = 0.069 \n    gamma = 2e-04 \n\n  Initial states:\n    l = 33048.2136 \n    b = 133.9574 \n    s = 0.9959 1.0004 1.0561 1.0382 1.0313 1.0383\n           0.9735 1.0043 0.9592 0.996 0.908 0.9988\n\n  sigma:  0.0278\n\n     AIC     AICc      BIC \n5967.022 5969.147 6030.323 \n\nTraining set error measures:\n                    ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set -6.188543 891.1403 668.3472 -0.05242082 1.962985 0.5067597\n                   ACF1\nTraining set 0.01019654\n\n\nLos valores óptimos de los parámetros son \\(\\alpha=\\) 0.6, \\(\\beta=\\) 0.07 y \\(\\gamma=\\) 0. Los valores tan bajos para \\(\\beta\\) y \\(\\gamma\\) indican que ambas, la pendiente y la estacionalidad, modifican su valor muy lentamente (véase Figura 15).\n\nautoplot(nacimientosbEts)\n\n\n\n\n\n\n\nFigura 15: Componentes del modelo de Holt-Winters multiplicativo para Nacimientos\n\n\n\n\n\nLa calidad de las predicciones es notable, con un error porcentual medio inferior al 2%.\nLos últimos valores de las componentes son,\n\nTT &lt;- nrow(nacimientosbEts$states)\nnacimientosbEts$states[TT,]\n\n\n\n        l         b        s1        s2        s3        s4        s5        s6 \n26607.274    -5.338     0.973     1.004     0.959     0.996     0.908     0.999 \n       s7        s8        s9       s10       s11       s12 \n    0.996     1.000     1.056     1.038     1.031     1.038 \n\n\nComo el último dato de la serie es junio de 2025, los valores del nivel \\(l\\) y la pendiente \\(b\\) mostrados corresponden a ese mes. Sin embargo, recuerda que la componente estacional sigue un orden inverso: s1 es el valor estacional para junio (mes del último dato), s2 el de mayo, s3 de abril, hasta s11 que sería agosto y s12 que es julio.\nPodemos reproducir las predicciones para los primeros 12 meses de julio de 2025 a junio de 2026 (recuerda, el etiquetado de la salida no es correcto) para ver que coinciden con las obtenidas con la función forecast (solo se muestran los primeros meses).\n\n(nacimientosbEts$states[TT, 1] + (1:12)*nacimientosbEts$states[TT, 2]) * \n  nacimientosbEts$states[TT, 14:3]\n\n     s12      s11      s10       s9       s8       s7       s6       s5 \n27621.59 27430.07 27608.68 28077.16 26591.51 26465.81 26537.31 24119.46 \n      s4       s3       s2       s1 \n26452.03 25471.08 26662.84 25839.21 \n\n\n\nnacimientosbf &lt;- forecast(nacimientosbEts, \n                              h = 24, \n                              level = 95)\n\nnacimientosbf\n\n\n\n         Point Forecast    Lo 95    Hi 95\nJul 2025       27621.59 26115.40 29127.78\nAug 2025       27430.07 25627.16 29232.99\nSep 2025       27608.68 25477.52 29739.84\nOct 2025       28077.16 25578.84 30575.48\nNov 2025       26591.51 23902.47 29280.55\n\n\nLa Figura 16 muestra la serie Nacimientos y las previsiones extramuestrales.\n\nautoplot(nacimientosbf,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\",\n         PI = FALSE)\n\n\n\n\n\n\n\nFigura 16: Nacimientos y predicción con alisado de Holt-Winters multiplicativo"
  },
  {
    "objectID": "03-02-Tema2.html#ejemplo-con-transformación-logarítmica",
    "href": "03-02-Tema2.html#ejemplo-con-transformación-logarítmica",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.8 Ejemplo con transformación logarítmica",
    "text": "5.8 Ejemplo con transformación logarítmica\nUna alternativa para predecir cualquier serie es predecir su transformación logarítmica. Después, se aplica la transformación inversa y se obtienen las predicciones de la serie original. No siempre, pero este procedimiento puede mejorar la calidad de las predicciones. Además, este proceso asegura que las predicciones y sus intervalos sean siempre positivas (ve a la píldora Series acotadas para aprender más sobre cómo garantizar que las predicciones sean positivas o que permanezcan dentro de cierto intervalo).\nEl uso de la transformación logarítmica en la estimación de modelos y predicción se puede realizar de forma sencilla y transparente con cualquiera de las funciones de que hemos visto a partir de los argumentos lambda y biasadj.\n\nlambda = 0 indica que se ha de realizar la transformación logarítmica de la serie previamente a su modelización. Es un parámetro de la transformación Box-Cox que no veremos en detalle en el tema 3.\nbiasadj = TRUE es necesario si tras una transformación de la serie original queremos que las predicciones sean insesgadas. Es decir, queremos obtener la predicción media.\nSea \\(y_t\\) la serie original y \\(z_t=log(y_t)\\) su transformación logarítmica. Si obtenemos una predicción \\(\\widehat{y}_t\\) de la serie original, esta será insesgada \\(E[\\widehat{y}_t]=y_t\\). Ahora bien, si obtenemos una predicción \\(\\widehat{z}_t\\) de la serie transformada, podemos pensar que \\(e^{\\widehat{z}_t}\\) es una predicción insesgada de la serie original, pero resulta que \\(E[e^{\\widehat{z}_t}] \\neq y_t\\). Es decir, la exponencial de la predicción de la serie con transformada logarítmica no es insesgada.\nSi el argumento biasadj es fijado a FALSE, las predicciones se calcularán de forma directa deshaciendo la transformación y serán sesgadas. En concreto, lo que se obtiene es una predicción mediana. Si, por el contrario, es fijado a TRUE, las predicciones se calcularán por medio de una fórmula alternativa y serán insesgadas.\nEn ambos casos, para series largas no debería observarse mucha diferencia entre las dos alternativas.\n\nVamos a practicar el uso de estos argumentos con la serie Nacimientos. Como se va a predecir el logaritmo de la serie, se debe indicar a la función ets model = \"AAA\" que estima el modelo Holt-Winters aditivo. Además, vamos a pedir que las predicciones sean insesgadas con biasadj = TRUE.\n\nnacimientosbEtsl &lt;- ets(nacimientosb, \n                     model = \"AAA\",\n                     damped = FALSE,\n                     lambda = 0, \n                     biasadj = TRUE)\n\nsummary(nacimientosbEtsl)\n\nETS(A,A,A) \n\nCall:\nets(y = nacimientosb, model = \"AAA\", damped = FALSE, lambda = 0, \n    biasadj = TRUE)\n\n  Box-Cox transformation: lambda= 0 \n\n  Smoothing parameters:\n    alpha = 0.1432 \n    beta  = 0.0101 \n    gamma = 0.249 \n\n  Initial states:\n    l = 10.411 \n    b = 9e-04 \n    s = 0.0045 0.0156 0.0603 0.063 0.0307 0.0133\n           -0.0383 0.0205 -0.0304 -0.0033 -0.09 -0.046\n\n  sigma:  0.0314\n\n      AIC      AICc       BIC \n-348.5507 -346.4257 -285.2498 \n\nTraining set error measures:\n                    ME     RMSE     MAE        MPE     MAPE      MASE      ACF1\nTraining set -40.89966 1011.194 772.584 -0.1295977 2.272043 0.5857951 0.4303813\n\nnacimientosbfl &lt;- forecast(nacimientosbEtsl,\n                           h = 24,\n                           level = 95,\n                           biasadj = TRUE)\n\nnacimientosbfl\n\n         Point Forecast    Lo 95    Hi 95\nJul 2025       27877.76 26199.38 29634.38\nAug 2025       27981.87 26278.00 29766.11\nSep 2025       27759.83 26048.10 29553.37\nOct 2025       28496.48 26714.94 30364.41\nNov 2025       27059.11 25341.90 28860.91\nDec 2025       26506.84 24797.32 28302.02\nJan 2026       26513.34 24773.64 28341.79\nFeb 2026       24306.74 22682.37 26015.61\nMar 2026       26334.48 24540.28 28223.91\nApr 2026       25188.27 23437.10 27034.41\nMay 2026       26101.51 24248.18 28057.61\nJun 2026       25982.92 24097.23 27975.60\nJul 2026       27839.46 25650.55 30163.59\nAug 2026       27944.14 25701.26 30328.64\nSep 2026       27723.14 25450.34 30142.72\nOct 2026       28459.61 26075.20 31001.62\nNov 2026       27024.89 24709.89 29496.56\nDec 2026       26474.12 24154.49 28954.57\nJan 2027       26481.44 24107.35 29024.22\nFeb 2027       24278.30 22050.59 26668.28\nMar 2027       26304.55 23833.65 28960.03\nApr 2027       25160.53 22740.58 27765.93\nMay 2027       26073.71 23505.60 28843.73\nJun 2027       25956.21 23337.93 28785.72\n\n\nEn este caso la calidad de las predicciones (MAPE = 2.3%) es algo superior a la obtenida con la serie sin transformar.\nLa Figura 17 muestra la serie Nacimientos y las previsiones extramuestrales obtenidas con y sin la transformación logarítmica. En este caso, las previsiones con la serie sin transformar son mayores que las obtenidas con la serie transformada en los meses de primavera e inferiores el resto del año.\n\n\n\n\n\n\n\n\nFigura 17: Nacimientos y dos predicciones con alisado de Holt-Winters\n\n\n\n\n\nLa Tabla 2 muestra las predicciones de Nacimientos obtenidas sin transformar la serie, con transformación logarítmica y predicciones insesgadas (biasadj = TRUE), y con transformación logarítmica y predicciones sesgadas (biasadj = FALSE).\n\n\n\n\nTabla 2: Diferencias en la predicción según transformación logarítmica y corrección por sesgo\n\n\n\n\n\n\nSin transformar\nlog(Nac) insesgadas\nlog(Nac) sesgadas\n\n\n\n\n27621.59\n27877.76\n27863.99\n\n\n27430.07\n27981.87\n27967.73\n\n\n27608.68\n27759.83\n27745.44\n\n\n28077.16\n28496.48\n28481.28\n\n\n26591.51\n27059.11\n27044.23\n\n\n26465.81\n26506.84\n26491.78\n\n\n26537.31\n26513.34\n26497.73\n\n\n24119.46\n24306.74\n24291.88\n\n\n26452.03\n26334.48\n26317.73\n\n\n25471.08\n25188.27\n25171.57\n\n\n26662.84\n26101.51\n26083.44\n\n\n25839.21\n25982.92\n25964.10\n\n\n\n\n\n\n\n\nLas predicciones sesgadas son menores que las insesgadas. Esto siempre es así. La diferencia depende fundamentalmente de la desviación típica del error, sigma en la salida de los métodos de alisado exponencial. Cuanto mayor es sigma, mayores son las diferencias.\nPor otro lado, las predicciones obtenidas sin y con la transformación logarítmica no guardan ninguna relación.\nNi la transformación logarítmica ni el uso de predicciones insesgadas aseguran mejores predicciones respecto de otras opciones, como puede ser trabajar con predicciones sesgadas o no realizar la transformación logarítmica. Hay que usar Origen de predicción móvil para determinar que transformación es la mejor."
  },
  {
    "objectID": "03-02-Tema2.html#casos-generales-de-alisado-exponencial-la-función-ets-de-nuevo",
    "href": "03-02-Tema2.html#casos-generales-de-alisado-exponencial-la-función-ets-de-nuevo",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "5.9 Casos generales de alisado exponencial: la función ets (de nuevo)",
    "text": "5.9 Casos generales de alisado exponencial: la función ets (de nuevo)\nEn los epígrafes previos hemos visto cinco de los casos expuestos en la taxonomía de la Tabla 1, fijados a partir de los argumentos model y damped de la función ets. Veamos ahora como estimar cualquiera de los treinta modelos que surgen según las diferentes posibilidades de la tendencia (N, A, Ad, M y Md), la estacionalidad (N, A y M) y el error (A, M).\nRecordemos que el tipo de error no influye en el cálculo de las previsiones, solo influye en el cálculo del intervalo de confianza de estas.\nPodemos estimar cualquiera de los treinta modelos usando la función ets del paquete forecast.\nLo más habitual es no saber cual es el mejor modelo, entendiendo como tal, el que mejor se ajusta a la serie temporal. De hecho, si lo que buscamos es predecir bien, el mejor modelo será el que mejor prediga.\nSi en una de las tres letras del código del modelo se indica “Z”, la función ets selecciona de entre los modelos posibles el que mejor se ajusta. Por ejemplo, model = \"AAZ\" indica un modelo con error y pendiente aditivos y dejaría a ets la búsqueda de la mejor opción para la estacionalidad (aditiva o multiplicativa). Si se especifica model = \"ZZZ junto con damped = NULL (opciones por defecto) se dejaría a la función total libertad para buscar entre todos los modelos (excepto aquellos con pendiente multiplicativa). Si se desea restringir la búsqueda a modelos sin amortiguamiento basta indicar damped = FALSE y si se desea restringir la búsqueda solo a modelos aditivos se puede usar el argumento additive.only = TRUE.\n\nCriterios de optimización\nFijado un modelo, ets estima por defecto sus parámetros maximizando la función de verosimilitud. Esta búsqueda esta restringida a \\(0 &lt; \\beta &lt; \\alpha &lt; 1\\), \\(0 &lt; \\gamma &lt; 1 - \\alpha\\) y \\(0.8 &lt; \\phi &lt; 0.98\\). Es decir, los tres primeros parámetros nunca pueden ser 0 o 1, y en la práctica sus valores límite son 0.0001 y 0.9999.\nPuedes cambiar el criterio de optimización con el argumento opt.crit. Por defecto vale “lik” (de likelihood o verosimilitud), pero si lo fijas a opt.crit = \"mse\" se estiman los parámetros que minimizan el error cuadrático medio. Otra opción interesante es opt.crit = \"amse\" que minimiza la media de los errores cuadráticos medios obtenido sobre las previsiones hasta nmse periodos vista. En este caso usa el argumento nmse para fijar el valor numérico del horizonte temporal.\n\n\nCriterios de selección de modelos\nQueda pendiente saber que criterio se usa para seleccionar el modelo cuando se ofrece esta opción. Esto se hace a partir de un criterio de información entre Akaike (aic), Akaike corregido para pequeñas muestras (aicc) y el Bayesiano (bic). Sus fórmulas son: \\[aic = -2log(L) + 2k\\] \\[aicc = aic + \\frac{k(k+1)}{T-k-1}\\] \\[bic=aic + k(log(T) - 2)\\] donde \\(L\\) es la verosimilitud, \\(T\\) el número de datos y \\(k\\) el de parámetros estimados (incluidos los puntos iniciales de arranque y la desviación típica del error).\nCuanto menor es el criterio de información, mejor modelo. Por defecto se usa Akaike corregido para pequeñas muestras (aicc), pero el argumento ic permite cambiar de criterio.\n\n\n\n\n\n\nUna reflexión sobre los métodos automáticos de selección de modelos\n\n\n\n\n\nCon el comando forecast(ets(nacimientos),h=24) obtenemos una predicción mensual a dos años vista del número de nacimientos en España. Así de simple, solo 31 caracteres. Todo esto gracias a que un algoritmo interno ha estimado los parámetros de múltiples modelos, elegido el mejor modelo de todos y lo ha usado para obtener las predicciones. Podemos afirmar que tenemos las mejores predicciones. Un momento, ¿podemos?\nParémonos a reflexionar sobre lo que hemos hecho o, más bien, lo que el algoritmo ha hecho y a contrastarlo con lo que nosotros queríamos. Por un lado, el algoritmo estima los parámetros de un menú fijo de modelos y para ello usa un criterio de optimización, que por defecto es maximizar la función de verosimilitud; cuando ya tiene estimados todos los modelos, elije el mejor usando el criterio de información de Akaike corregido para muestras pequeñas; y finalmente, nosotros medimos la capacidad predictiva del modelo seleccionado usando el error absoluto porcentual medio. Vaya, resulta que en los procesos de identificación y estimación del mejor modelo se usan dos criterios diferentes, que además no coinciden con nuestro criterio de calidad de las predicciones.\nSi consideramos que la calidad de un modelo viene dada por el error absoluto porcentual medio en las predicciones intramuestrales a un periodo vista (lo que hemos decidido llamar MAPE), ¿no deberíamos estimar los parámetros del modelo usando como criterio la minimización del MAPE?, ¿no deberíamos elegir entre varios modelos aquel que presenta un MAPE menor? De esta forma, en todos los pasos del proceso se usa el mismo criterio, que es, además, el criterio que hemos considerado adecuado para valorar la calidad de las predicciones.\nPero no es esto lo que hacemos.\nNada nos garantiza que el modelo estimado y seleccionado por el algoritmo estime las mejores predicciones posibles. Y por mejores quiero decir que de entre todos los posibles modelos del menú y todos los posibles valores de sus parámetros, el seleccionado sea el que minimiza nuestro criterio de calidad de las predicciones.\nAhora ya podemos dar respuesta a la pregunta del primer párrafo: no, no podemos afirmar que nuestras predicciones sean las mejores.\nAlguien dirá que casi seguro entre las predicciones subóptimas obtenidas por el algoritmo con su extraña mezcla de criterios y las predicciones óptimas de verdad no habrá mucha diferencia. Total, que más da una función de verosimilitud que un criterio de información que una medida del error medio. Pero lo cierto es que no lo sabemos, no tenemos ni idea de la distancia que hay entre lo óptimo y lo subóptimo, y si el coste de equivocarme en las predicciones es alto, puede que incluso una pequeña diferencia sea relevante.\nEsta reflexión realizada en el contexto de series temporales y para la función ets es aplicable a todos los casos donde dejamos que un algoritmo ya programado elija el mejor modelo, y se basa en el hecho de que rara vez los criterios de estimación y elección que usan los algoritmos coinciden con el concepto de calidad de ajuste que estamos interesados.\nA pesar de lo aquí expuesto, como es más cómodo (y rápido) tirar de rutinas ya programadas que escribir nuestro propio código, seguiremos trabajando con modelos subóptimos y obteniendo estimaciones subóptimas, pero diciendo que son las mejores."
  },
  {
    "objectID": "03-02-Tema2.html#recogida-de-residuos",
    "href": "03-02-Tema2.html#recogida-de-residuos",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "6.1 Recogida de residuos",
    "text": "6.1 Recogida de residuos\n\nIdentificación y estimación del mejor modelo\nSi estimamos el mejor modelo de alisado exponencial para la serie Residuos sin ningún tipo de restricción, nos encontramos:\n\nresiduosEts &lt;- ets(residuos)\n\nsummary(residuosEts) \n\nETS(M,A,N) \n\nCall:\nets(y = residuos)\n\n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.4498 \n\n  Initial states:\n    l = 490.9216 \n    b = 6.8509 \n\n  sigma:  0.0358\n\n     AIC     AICc      BIC \n273.6061 276.2148 280.4425 \n\nTraining set error measures:\n                     ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set -0.9595632 19.22314 15.12049 -0.08559712 2.768678 0.9408304\n                   ACF1\nTraining set 0.03440538\n\n\nEl modelo estimado es ETS(M,A,N) o “MAN”, un modelo con pendiente aditiva, sin estacionalidad y con error multiplicativo. Es decir, \\(y_{t+1} = (l_t + b_t) \\cdot (1 + \\varepsilon_{t+1})\\).\nEl valor de \\(\\alpha\\) técnicamente es 1, indicando que el nivel de la serie varia constantemente en el tiempo. El valor de \\(\\beta = 0.45\\) indica que también la pendiente cambia en el tiempo.\nRespecto de la calidad del modelo, el valor de MAPE= \\(2.8\\)% evidencia que estamos ante un modelo que se ajusta bien a los datos. MASE= \\(0.94\\) indica que el modelo de alisado exponencial simple reduce en solo un \\(6\\)% el error del método ingenuo I. Además, el modelo estimado no presenta sesgo.\n\n\nPredicción\nMediante la función forecast podemos predecir la serie Residuos. La última pendiente estimada por Alisado es negativa (\\(b_{2023}\\) = -5.67), por tanto, la predicción decrece en el tiempo (véase Figura 18).\n\nresiduosEtsPre &lt;- forecast(residuosEts, \n                         h = 5,\n                         level = 95)\n\nresiduosEtsPre\n\n     Point Forecast    Lo 95    Hi 95\n2024       459.3359 427.1431 491.5288\n2025       453.6698 397.1732 510.1664\n2026       448.0037 365.1582 530.8492\n2027       442.3376 330.7965 553.8787\n2028       436.6715 294.1520 579.1909\n\nautoplot(residuosEtsPre,\n         xlab = \"\",\n         ylab = \"Kg per cápita\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 18: Residuos y predicción a 5 años vista\n\n\n\n\n\n\n\nAnálisis del residuo\nEl error de un modelo de alisado contiene la componente de Intervención y el propio término de Error. Ver numérica o gráficamente el error permite identificar fácilmente la presencia de valores atípicos (intervención). Obtenemos el error con la función residuals.\n\nerror &lt;- residuals(residuosEts)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"Periodo\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(1995, 2023, 2)) \n\n\n\n\n\n\n\nFigura 19: Error + Intervención\n\n\n\n\n\nLa Figura 19 muestra que aunque algún error supera las dos desviaciones típicas, ninguno puede ser considerado claramente como atípico.\n\n\nValidación: error extramuestral a varios periodos vista\nVamos a mejorar la estimación de la calidad de las predicciones obteniendo el MAPE para previsiones extramuestrales a varios periodos vista. Para ello vamos a reservar, por ejemplo, las últimas 6 observaciones de la serie Residuos y ajustar el modelo con las restantes. Después usaremos este modelo para calcular las predicciones a 6 periodos vista y compararlas con los valores reales de la serie Residuos.\nRecuerda, este método para valorar la calidad de las predicciones usa la filosofía del método Conjunto de entrenamiento/Conjuto de prueba: el periodo de datos usado en la estimación no se usa como periodo de datos para la validación. Sin embargo, tiene el problema de que el error obtenido es una mezcla de errores de predicción a corto, medio y largo plazo difícil de valorar. Además, los resultados dependen tremendamente del punto de corte temporal seleccionado.\n\n# Definimos las observaciones intra y extramuestrales\nresiduoIntra &lt;- subset(residuos, end = length(residuos) - 6)\nresiduoExtra &lt;- subset(residuos, start = length(residuos) - 5)\n\n# Estimamos el modelo con todos los datos menos los 6 ultimos\nresiduoIntraEts &lt;- ets(residuoIntra, model = \"MAN\")\n\n# Predecimos los 6 años que hemos quitado de la serie y \n# vemos la calidad del ajuste.\nresiduoExtraPre &lt;- forecast(residuoIntraEts, h = 6)\naccuracy(residuoExtraPre, residuoExtra)\n\n                    ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set -3.567170 26.775671 19.987941 -0.5625129 3.637451 1.0993368\nTest set      3.771456  8.501916  6.681918  0.7726736 1.400719 0.3675055\n                    ACF1 Theil's U\nTraining set  0.05629426        NA\nTest set     -0.08588046 0.7548456\n\n\nAtendiendo al MAPE se tiene que el error de previsión a un periodo vista en el periodo intramuestral de 1995 a 2017 es del 3.6%; y el error de previsión a largo plazo en el periodo extramuestral de 2018 a 2023 es del 1.4%. Para el punto de corte elegido, la calidad de las previsiones no se deteriora cuanto nos salimos de las condiciones óptimas.\nUn gráfico puede ayudar a entender este proceso de validación. En la Figura 20:\n\nLa línea de puntos vertical separa el periodo muestral (1995-2017) usado para estimar el modelo, del periodo extramuestral (2018-2023) usado sólo para hacer las previsiones.\nLa serie Residuos aparece como una línea sólida en negro, desde 1995 hasta 2023.\nLa previsión intra-muestral (a un periodo vista) de la serie Residuos aparece como una línea azul. Observa la previsión puede ser mayor o menor que la serie, no evidenciándose sesgo.\nLa línea en rojo es la previsión extra-muestral a largo plazo: \\(\\hat{y}_{T+h}=l_T\\), donde \\(T=2017\\). Observa que casi todas las previsiones están por debajo del valor real de la serie.\nAl lado de cada previsión se ha indicado el error estimado (MAPE). Para la previsión extramuestral, el error es la media de errores muy bajos (primeras previsiones) y errores muy elevados (últimas previsiones).\n\nClaramente estos resultados dependen del punto de corte seleccionado.\n\n\n\n\n\n\n\n\nFigura 20: Residuos, predicción intra y extramuestral"
  },
  {
    "objectID": "03-02-Tema2.html#nacimientos",
    "href": "03-02-Tema2.html#nacimientos",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "6.2 Nacimientos",
    "text": "6.2 Nacimientos\nVeamos un segundo ejemplo con la serie Nacimientos (desde el año 2000).\n\nIdentificación y estimación del mejor modelo\nSi usamos a la función ets sin ningún tipo de restricción para que localice el mejor modelo para la serie Nacimientos, aparece un modelo con tendencia amortiguada y un valor de \\(\\phi = 0.98\\), cercano al máximo valor permitido. Por este motivo, vamos a solicitar la identificación y estimación del mejor modelo excluyendo aquellos con tendencia amortiguada.\n\n#nacimientosEts &lt;- ets(nacimientosb)\nnacimientosEts &lt;- ets(nacimientosb, \n                      damped = FALSE)\n\nsummary(nacimientosEts)\n\nETS(A,A,A) \n\nCall:\nets(y = nacimientosb, damped = FALSE)\n\n  Smoothing parameters:\n    alpha = 0.4838 \n    beta  = 0.0106 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 33045.5226 \n    b = 146.0719 \n    s = -0.3204 37.4571 1953.69 1402.23 1021.944 1226.764\n           -957.5673 179.2031 -1379.473 -181.91 -3234.13 -67.8866\n\n  sigma:  899.1074\n\n     AIC     AICc      BIC \n5931.442 5933.567 5994.743 \n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -57.6172 875.2858 671.0568 -0.1823204 1.995274 0.5088143\n                   ACF1\nTraining set 0.05823082\n\n\nEl mejor modelo (A, A, A) tiene error, tendencia y estacionalidad aditivas: \\(y_t = l_{t-1} + b_{t-1} + s_{t + 1 - m}\\).\nEl bajo valor de \\(\\beta\\) y \\(\\gamma\\) indican que ambas, la pendiente y la estacionalidad, varían muy lentamente en el tiempo (véase la Figura 21).\n\nautoplot(nacimientosEts,\n         xlab = \"Periodo\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 21: Componentes del modelo óptimo para Nacimientos\n\n\n\n\n\nRespecto de la calidad del modelo, el MAPE de 2% indica que estamos ante un modelo que se ajusta muy bien a los datos; y el valor de MASE igual a 0.51 indica que este modelo reduce en un 49% el error del método ingenuo con estacionalidad, el más sencillo posible. El modelo no tiene sesgo y el valor de ACF1 de 0.06, inferior a 0.1, indica que el intervalo de confianza de las predicciones está bien calculado.\nPodemos ver los últimos valores estimados del nivel, la pendiente y la estacionalidad para interpretarlos. Como el último dato de la serie es junio de 2025, los valores del nivel \\(l\\) y la pendiente \\(b\\) mostrados corresponden a ese mes. Pero recuerda que los valores de la componente estacional están ordenados al revés partiendo de junio: s1 es el valor estacional para junio, s2 el de mayo, s3 el se abril, y así hasta s12 que sería julio.\n\nTT &lt;- nrow(nacimientosEts$states)\nnacimientosEts$states[TT,]\n\n            l             b            s1            s2            s3 \n26731.4368760   -41.0779679  -957.7263395   178.9110540 -1379.6229104 \n           s4            s5            s6            s7            s8 \n -182.1189222 -3234.1645446   -67.9123558    -0.4663169    37.3091214 \n           s9           s10           s11           s12 \n 1953.5428305  1402.0792378  1021.7948111  1226.6106455 \n\n\nFebrero (s5) es el mes con menor número de nacimientos: 3234 nacimientos menos que la media anual. Por contra, octubre (s9) es el mes con mayor número de nacimientos: 1954 más que la media anual.\nPodemos usar estos valores para predecir los próximos 12 meses, julio de 2025 hasta junio de 2026 (ojo, el etiquetado de la salida no tiene sentido):\n\nnacimientosEts$states[TT, 1] + (1:12) * nacimientosEts$states[TT, 2] + \n  nacimientosEts$states[TT, 14:3]\n\n     s12      s11      s10       s9       s8       s7       s6       s5 \n27916.97 27671.08 28010.28 28520.67 26563.36 26484.50 26375.98 23168.65 \n      s4       s3       s2       s1 \n26179.62 24941.03 26458.49 25280.77 \n\n\nNuestra predicción para julio de 2025 es de 27916.97 bebés y para junio de 2026 de 25280.77 bebés.\n\n\nPredicción\nSi pedimos los valores de predicción tenemos (sólo se muestran los primeros meses):\n\nnacimientosEtsPre &lt;- forecast(nacimientosEts, \n                              h = 24, \n                              level = 95)\n\nnacimientosEtsPre\n\n\n\n         Point Forecast    Lo 95    Hi 95\nJul 2025       27916.97 26154.75 29679.19\nAug 2025       27671.08 25705.24 29636.91\nSep 2025       28010.28 25852.38 30168.19\nOct 2025       28520.67 26179.25 30862.09\nNov 2025       26563.36 24044.97 29081.74\n\n\nLa Figura 22 muestra la serie Nacimientos, su predicción a dos años vista y el intervalo de confianza.\n\nautoplot(nacimientosEtsPre,\n         xlab = \"\",\n         ylab = \"Nacimientos\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 22: Nacimientos y predicción\n\n\n\n\n\n\n\nAnálisis del error\nSe identifica varios valores claramente atípicos –superan las 3 desviaciones típicas– que corresponden a enero de 2011 y, aproximadamente, nueves después del confinamiento por la pandemia (diciembre de 2020, y febrero y marzo de 2021). Abril de 2008 y diciembre de 2010 son otros candidatos a intervención por superar las 2.5 desviaciones típicas.2\n\nerror &lt;- residuals(nacimientosEts)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"Periodo\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(2000, 2026, 2)) \n\nfechas &lt;- format(seq(as.Date(\"2000-01-01\"), as.Date(\"2025-06-01\"), \"month\"), \"%Y-%m\")\nfechas[abs(error) &gt; 3 * sderror]\n\n[1] \"2011-01\" \"2020-12\" \"2021-02\" \"2021-03\"\n\n\n\n\n\n\n\n\nFigura 23: Error + Intervención\n\n\n\n\n\nUn método alternativo para obtener valores atípicos es la prueba de Tukey (véase la píldora Valores perdidos y valores atípicos).\n\natipicos &lt;- tsoutliers(error)\nfechas[atipicos$index]\n\n[1] \"2020-12\"\n\n\nEn este caso solo se identifica como atípico el valor de diciembre de 2020.\n\n\nValidación: error extramuestral según horizonte temporal\nEn este ejemplo calcularemos el error extramuestral según el horizonte temporal de previsión, una metodología que ya hemos visto anteriormente.\n\nk &lt;- 120                 \nh &lt;- 12                  \nTT &lt;- length(nacimientosb)\ns &lt;- TT - k - h \n\nmapeAlisado &lt;- matrix(NA, s + 1, h)\nfor (i in 0:s) {\n  train.set &lt;- subset(nacimientosb, start = i + 1, end = i + k)\n  test.set &lt;-  subset(nacimientosb, start = i + k + 1, end = i + k + h)\n  \n  fit &lt;- ets(train.set, model = \"AAA\", damped = FALSE)\n  fcast&lt;-forecast(fit, h = h)\n  mapeAlisado[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n}\n\nerrorAlisado &lt;- colMeans(mapeAlisado)\nerrorAlisado\n\n [1] 2.202293 2.511743 2.689947 2.947580 2.888890 2.965959 3.162538 3.196361\n [9] 3.219150 3.278137 3.133652 3.265335\n\n\n\nggplot() +\n  geom_line(aes(x = 1:12, y = errorAlisado)) +\n  labs(x = \"Horizonte temporal de predicción\", y = \"MAPE\", title = \"\") +\n  scale_x_continuous(breaks= 1:12)\n\n\n\n\n\n\n\nFigura 24: Error de predicción según horizonte temporal\n\n\n\n\n\nLa Figura 24 muestra el error de previsión extramuestral según el horizonte temporal. El error extramuestral a un periodo vista es comparable al error intramuestral (2.2% frente a 2.0%). Aunque el error de previsión aumenta conforme lo hace el horizonte temporal, siempre se mantiene muy bajo. Por ejemplo, en las previsiones a 12 meses vista el error es del 3.3%."
  },
  {
    "objectID": "03-02-Tema2.html#demanda-eléctrica",
    "href": "03-02-Tema2.html#demanda-eléctrica",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "6.3 Demanda eléctrica",
    "text": "6.3 Demanda eléctrica\nConsideremos la serie de consumo eléctrico diario durante el año 2024.\n\n\n\nIdentificación y estimación del mejor modelo\nEl modelo óptimo sin restricciones es (AAdA), es decir, \\(y_{t+1} = l_t + \\phi b_t + s_{t+1-m} + \\varepsilon_{t+1}\\), con un elevado nivel de amortiguamiento \\(\\phi = 0.89\\).\n\nelectricidadEts &lt;- ets(electricidad)\n\nsummary(electricidadEts) \n\nETS(A,Ad,A) \n\nCall:\nets(y = electricidad)\n\n  Smoothing parameters:\n    alpha = 0.9239 \n    beta  = 1e-04 \n    gamma = 1e-04 \n    phi   = 0.8857 \n\n  Initial states:\n    l = 644.3443 \n    b = 14.8303 \n    s = -84.5061 -47.067 22.048 32.1189 32.3575 31.6764\n           13.3724\n\n  sigma:  23.2475\n\n     AIC     AICc      BIC \n4477.179 4478.213 4527.913 \n\nTraining set error measures:\n                     ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set -0.3561611 22.86319 14.43216 -0.1206147 2.171595 0.518908\n                   ACF1\nTraining set 0.02118618\n\n\nEl valor \\(\\beta=0\\) y \\(\\gamma = 0\\) indican que la pendiente y la estacionalidad se mantienen constante en el tiempo, mientras que el elevado valor de \\(\\alpha\\) indica que el nivel de la serie cambia de forma constante. Este cambio de nivel está relacionado con las variaciones en el consumo eléctrico debido a los cambios en la temperatura y el uso de aparatos de climatización.\nRespecto de la calidad del modelo, el MAPE de 2.2% indica que estamos ante un modelo que se ajusta muy bien a los datos; no hay sesgo (MPE es inferior al 1%); y el valor de ACF1, muy bajo, indica que la fórmula usada para el cálculo del intervalo de confianza de las predicciones es válida.\nPodemos ver los últimos valores estimados del nivel y la estacionalidad para interpretarlos. Recuerda que los valores de la componente estacional están ordenados al revés. En este caso, como el último dato (31 de diciembre de 2024) fue martes, s1 es martes, s2 es lunes…y s7 es miércoles.\n\nTT &lt;- nrow(electricidadEts$states)\nelectricidadEts$states[TT,]\n\n            l             b            s1            s2            s3 \n638.703913172  -0.005458752  31.670093723  13.378175541 -84.511145443 \n           s4            s5            s6            s7 \n-47.074612082  22.050610389  32.121999595  32.351842678 \n\n\nEl domingo la demanda eléctrica cae 84.5 GWh respecto de la media semanal. Por el contrario, el miércoles es el día de mayor incremento de demanda respecto de la media semanal, 32.4 GWh.\n\n\nPredicción\nSi pedimos los valores de predicción para las cuatro semanas siguientes, tenemos (sólo se muestran la primera):\n\nelectricidadEtsPre &lt;- forecast(electricidadEts, \n                               h = 28, \n                               level = 95)\n\nelectricidadEtsPre\n\n\n\n         Point Forecast    Lo 95    Hi 95\n53.28571       671.0509 625.4867 716.6151\n53.42857       670.8168 608.7810 732.8526\n53.57143       660.7416 585.7674 735.7159\n53.71429       591.6130 505.6238 677.6023\n53.85714       554.1735 458.4268 649.9202\n54.00000       652.0602 547.4612 756.6593\n54.14286       670.3498 557.5903 783.1093\n\n\nLa Figura 25 muestra la serie Demanda eléctrica, su predicción a cuatro semanas vista y el intervalo de confianza.\n\nautoplot(electricidadEtsPre,\n         xlab = \"\",\n         ylab = \"GWh\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 25: Demanda eléctrica y predicción\n\n\n\n\n\n\n\nAnálisis del error\n\nerror &lt;- residuals(electricidadEts)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"Semana\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"blue\", \"blue\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(1, 52, 4)) \n\nfechas &lt;- format(seq(as.Date(\"2024-1-1\"), as.Date(\"2024-12-31\"), \"day\"), \"%Y-%m-%d\")\nfechas[abs(error) &gt; 3 * sderror]\n\n [1] \"2024-01-01\" \"2024-01-02\" \"2024-03-28\" \"2024-03-30\" \"2024-05-01\"\n [6] \"2024-08-15\" \"2024-11-01\" \"2024-12-06\" \"2024-12-24\" \"2024-12-25\"\n[11] \"2024-12-26\"\n\n\n\n\n\n\n\n\nFigura 26: Error + Intervención\n\n\n\n\n\nEn la Figura 26 se identifican múltiples días atípicos asociados con un consumo inferior al esperado debido a festividades: Año nuevo, Semana Santa (el Viernes Santo fue el 29 de abril), Día del trabajador, Virgen de agosto, Todos los Santos, Día de la Constitución y Navidad. El Día de Reyes (6 de enero), el Día de la Hispanidad (12 de octubre) y el Día de la Inmaculada (8 de diciembre) no aparecen porque cayeron en sábado o domingo, días para los que el consumo de electricidad ya es bajo.\nTambién se observan tres días con un consumo mayor de lo esperado justo después de un festivo, el 2 de enero (tras Reyes), el 30 de abril (Sábado Santo) y el 26 de diciembre (tras Navidad). En este caso, la causa no es un incremento inesperado del consumo, sino la dinámica del propio método de estimación. Al llegar un día festivo, el método de Alisado falla ofreciendo una predicción más alta de la real y dando lugar a un error negativo. Al día siguiente, el método de Alisado ajusta su predicción a la baja, pero por no ser festivo vuelve a fallar, esta vez ofreciendo una predicción más baja de la real y dando lugar a un error positivo. Por este motivo en la Figura 26 después de un error negativo muy elevado se observa un error positivo también muy elevado.\nVeamos como en este caso la prueba de Tukey identifica las mismas fechas y añade el 17 de agosto.\n\natipicos &lt;- tsoutliers(error)\nfechas[atipicos$index]\n\n [1] \"2024-01-01\" \"2024-01-02\" \"2024-03-28\" \"2024-03-30\" \"2024-05-01\"\n [6] \"2024-08-15\" \"2024-08-17\" \"2024-11-01\" \"2024-12-06\" \"2024-12-24\"\n[11] \"2024-12-25\" \"2024-12-26\""
  },
  {
    "objectID": "03-02-Tema2.html#footnotes",
    "href": "03-02-Tema2.html#footnotes",
    "title": "Series Temporales: Alisado Exponencial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFormalmente en el modelo de Holt-Winters multiplicativo el error es aditivo. Es decir, debería ser (A, A, M). Sin embargo, debido a que desde un punto de vista teórico este modelo, junto con otros, puede tener varianza infinita, la función ets no permite por defecto su estimación. Para poder estimar con la función ets el modelo (A, A, M) es necesario añadir el argumento restrict = FALSE. Los modelos que para poder ser estimados requieren este argumento son: ANM, AAM, AAdM, MMA, MMdA, AMN, AMdN, AMA, AMdA, AMM y AMdM.↩︎\nRecuerda que el valor de 3 es uno de los posibles y debe ajustarse a las características de la serie y el análisis.↩︎"
  },
  {
    "objectID": "03-04-Tema4.html#sobre-el-proceso-estocástico",
    "href": "03-04-Tema4.html#sobre-el-proceso-estocástico",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "3.1 Sobre el proceso estocástico",
    "text": "3.1 Sobre el proceso estocástico\nA lo largo de este tema asumiremos que:\n\n\\(\\{y_t\\}_{t=1}^T\\) es una realización de un proceso estocástico desconocido.\nEl proceso estocástico es estacionario en sentido amplio: \\[E[y_t]  = \\mu &lt; \\infty \\;\\;\\; \\forall t,\\] \\[Cov[y_t, y_{t-k}]  = \\gamma_k  \\;\\;\\; \\forall k.\\]\nEl proceso estocástico es ergódico, o su condición suficiente: \\[\\lim_{k \\rightarrow \\infty} \\gamma_k  = 0.\\]"
  },
  {
    "objectID": "03-04-Tema4.html#sobre-el-vector-de-residuos",
    "href": "03-04-Tema4.html#sobre-el-vector-de-residuos",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "3.2 Sobre el vector de residuos",
    "text": "3.2 Sobre el vector de residuos\nTambién asumiremos que los residuos del modelo \\(\\{\\varepsilon_t\\}_{t=1}^T\\) son ruido blanco:\n\nMedia cero: \\(E[\\varepsilon_t]=0\\)\n\n\nVarianza constante (homocedástico): \\(E[\\varepsilon_t^2]=\\sigma^2\\)\n\n\nIncorrelación: \\(E[\\varepsilon_t \\cdot \\varepsilon_{s}]=0 \\;\\;\\; t \\neq s\\)\n\n\nDistribución Normal: \\(\\varepsilon_t \\sim N\\)\n\nEs decir, \\(\\varepsilon_t \\sim N(0,\\sigma^2)\\) i.i.d."
  },
  {
    "objectID": "03-04-Tema4.html#procesos-autorregresivos-arp",
    "href": "03-04-Tema4.html#procesos-autorregresivos-arp",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "4.1 Procesos autorregresivos AR(p)",
    "text": "4.1 Procesos autorregresivos AR(p)\n\nDefinición\nEl modelo general autorregresivo de orden p, \\(y_t \\sim AR(p)\\) viene definido por \\[y_t=c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\varepsilon_t,\\] que usando el operador retardo queda \\[(1 - \\phi_1 L - \\phi_2 L^2 - \\ldots - \\phi_p L^p)y_t = c + \\varepsilon_t\\]\n\n\nPropiedades\nEl proceso es estacionario si quedan fuera del círculo de radio la unidad todas las raíces del polinomio autorregresivo \\[\\Phi_p(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\ldots - \\phi_p z^p.\\]\nEs invertible siempre.\n\nPodemos transformar el proceso AR(p) en un proceso donde \\(y_t\\) depende de la suma infinita de errores pasados, MA(\\(\\infty\\)).\nSi conocemos las p primeras autocorrelaciones, podemos estimar los p parámetros del modelo. Por ejemplo, para un proceso AR(2) se verifica que: \\[\\rho_1 = \\phi_1 + \\phi_2 \\rho_1\\] \\[\\rho_2 = \\phi_1 \\rho_1 + \\phi_2\\]\nEstas ecuaciones se denominan Ecuaciones de Yule-Walker.\nObserva que si tenemos una estimación de las dos primeras autocorrelaciones, estas ecuaciones nos permiten obtener una estimación de los coeficientes del proceso AR(2) como una aplicación del método de los momentos.\n\nSobre todo,\n\nLa FAC del proceso decae exponencialmente a partir del orden p\nLa FACP verifica que los p primeros valores son no nulos y todos los demás valen cero.\n\n\n\nEjemplos\n\n\\(y_t \\sim AR(1): \\;\\;y_t = c + \\phi_1 y_{t-1} + \\varepsilon_t\\) o \\((1 - \\phi_1 L)y_t = c + \\varepsilon_t\\)\n\\(y_t \\sim AR(2): \\;\\;y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\varepsilon_t\\) o \\((1 - \\phi_1 L - \\phi_2 L^2)y_t = c + \\varepsilon_t\\)\n\n\n\nSimulación de procesos autorregresivos\nLa Figura 1 muestra dos simulaciones del proceso AR(1) \\(y_t = 0.8y_{t-1} + \\varepsilon_t\\), el panel superior con 20 datos y el inferior con 100 datos. En ambos casos \\(\\varepsilon_t\\) se distribuye como una normal con media cero y varianza la unidad. (Todas las simulaciones se han realizado con la función arima.sim de la librería stats.)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) n = 20\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) n = 100\n\n\n\n\n\n\n\nFigura 1: Simulación de dos procesos AR(1) con diferente tamaño muestral"
  },
  {
    "objectID": "03-04-Tema4.html#procesos-en-medias-móviles-maq",
    "href": "03-04-Tema4.html#procesos-en-medias-móviles-maq",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "4.2 Procesos en medias móviles MA(q)",
    "text": "4.2 Procesos en medias móviles MA(q)\n\nDefinición\nEl modelo general en medias móviles de orden q, \\(y_t \\sim MA(q)\\) viene definido por \\[y_t=c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\ldots + \\theta_q \\varepsilon_{t-q},\\] que usando el operador retardo queda \\[y_t = c + (1 + \\theta_1 L + \\theta_2 L^2 + \\ldots + \\theta_q L^q) \\varepsilon_t\\]\n\n\nPropiedades\nEl proceso es invertible si quedan fuera del círculo de radio la unidad todas las raíces del polinomio en medias móviles \\[\\Theta_q(z) = 1 + \\theta_1 z + \\theta_2 z^2 + \\ldots + \\theta_q z^q.\\]\n\nPodemos transformar el proceso MA(q) en un proceso AR(\\(\\infty\\)).\nSi conocemos las q primeras autocorrelaciones, podemos estimar los q parámetros del modelo. Por ejemplo, para un proceso MA(2) se verifica que: \\[\\rho_1 = \\frac{\\theta_1 + \\theta_1\\theta_2}{1 + \\theta_1^2 + \\theta_2^2}\\] \\[\\rho_2 = \\frac{\\theta_2}{1 + \\theta_1^2 + \\theta_2^2}\\]\n\nEs estacionario siempre.\nSobre todo,\n\nLa FAC verifica que los q primeros valores son no nulos y todos los demás valen cero.\nLa FACP decae exponencialmente a partir del orden q.\n\n\n\nEjemplos\n\n\\(y_t \\sim MA(1): \\;\\;y_t = c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}\\) o \\(y_t = c + (1 + \\theta_1 L)\\varepsilon_t\\)\n\\(y_t \\sim MA(2): \\;\\;y_t=c + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2}\\) o \\(y_t = c + (1 + \\theta_1 L + \\theta_2 L^2)\\varepsilon_t\\)\n\n\n\nSimulación de procesos en medias móviles\nLa Figura 2 ofrece dos simulaciones del proceso MA(1) \\(y_t = 0.8\\varepsilon_{t-1} + \\varepsilon_t\\), la primera con 20 datos y la segunda con 100 datos. En ambos casos \\(\\varepsilon_t\\) se distribuye como una normal con media cero y varianza la unidad.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) n = 20\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) n = 100\n\n\n\n\n\n\n\nFigura 2: Simulación de dos procesos MA(1) con diferente tamaño muestral"
  },
  {
    "objectID": "03-04-Tema4.html#procesos-armapq",
    "href": "03-04-Tema4.html#procesos-armapq",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "4.3 Procesos ARMA(p,q)",
    "text": "4.3 Procesos ARMA(p,q)\n\nDefinición\nEl modelo general \\(y_t \\sim ARMA(p,q)\\) viene dado por \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p}  +\n        \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\ldots +\n        \\theta_q \\varepsilon_{t-q}+ \\varepsilon_t,\\] que usando el operador retardo queda \\[(1 - \\phi_1 L - \\ldots - \\phi_p L^p)y_t = c + (1 + \\theta_1 L + \\ldots + \\theta_q L^q) \\varepsilon_t.\\]\nEl proceso más simple es el ARMA(1,1), \\(y_t = c  + \\phi_1 y_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\varepsilon_{t}\\).\n\n\nPropiedades\nEl proceso es estacionario si quedan fuera del círculo de radio la unidad todas las raíces del polinomio \\[\\Phi_p(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\ldots - \\phi_p z^p.\\] El proceso es invertible si quedan fuera del círculo de radio la unidad todas las raíces del polinomio \\[\\Theta_q(z) = 1 + \\theta_1 z + \\theta_2 z^2 + \\ldots + \\theta_q z^q.\\] Sobre todo,\n\nLa FAC decae exponencialmente a partir del orden p.\nLa FACP decae exponencialmente a partir del orden q.\n\n\n\nEjemplos\n\n\\(y_t \\sim ARMA(1, 1): \\;\\;y_t = c  + \\phi_1 y_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\varepsilon_{t}\\) o \\((1 - \\phi_1 L)y_t = c + (1 + \\theta_1 L)\\varepsilon_t\\).\n\\(y_t \\sim ARMA(0, 0): \\;\\;y_t = c + \\varepsilon_{t}\\). Si \\(c = 0\\), a este proceso se le denomina ruido blanco.\n\n\n\nSimulación de procesos ARMA\nLa Figura 3 muestra las FAC y FACP de una simulación de tamaño 200 para un proceso ARMA(1,1), donde \\(\\varepsilon_t\\) se distribuye como una normal con media cero y varianza la unidad.\n\\(y_t = 0.7y_{t-1} + 0.6\\varepsilon_{t-1} + \\varepsilon_t\\)\n\n\n\n\n\n\n\n\nFigura 3: Simulación de un proceso ARMA(1, 1)"
  },
  {
    "objectID": "03-04-Tema4.html#proceso-arimapdq",
    "href": "03-04-Tema4.html#proceso-arimapdq",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "4.4 Proceso ARIMA(p,d,q)",
    "text": "4.4 Proceso ARIMA(p,d,q)\nSi la serie \\(y_t\\) no es estacionaria pero tras diferenciarla \\(d\\) veces se hace estacionaria, diremos que la serie es integrada de orden \\(d\\): \\(y_t \\sim I(d)\\). Por tanto,\n\nuna serie estacionaria se indicará como \\(y_t \\sim I(0)\\)\n\\(y_t \\sim I(d)\\) es equivalente a \\(\\nabla^d y_t = (1 - L)^d y_t \\sim I(0)\\)\n\nUna serie \\(y_t\\) sigue un proceso \\(ARIMA(p,d,q)\\) si:\n\nhay que diferenciar la serie \\(d\\) veces para hacerla estacionaria, \\(y_t \\sim I(d)\\); y\nla serie diferenciada sigue un proceso ARMA(p,q), \\(\\nabla^d y_t \\sim ARMA(p,q)\\).\n\nEntonces, podemos escribir: \\[\\begin{equation*}\n\\begin{array}{c@{\\qquad}c@{\\quad}ccc}\n  y_t \\sim  ARIMA(p,d,q): & (1 - \\phi_1 L - \\ldots - \\phi_p L^p) & (1- L)^d y_t & = & c + (1 + \\theta_1 L + \\ldots + \\theta_q L^q) \\varepsilon_t \\\\\n                          & \\uparrow                            & \\uparrow      &   & \\uparrow \\\\\n                          & AR(p)                               & I(d)          &   & MA(q)\n\\end{array}\n\\end{equation*}\\]\n\nEjemplos\n\n\\(y_t \\sim ARIMA(0, 1, 1)\\): \\[\n\\begin{aligned}\n(1- L) y_t & = c + (1 + \\theta_1 L) \\varepsilon_t \\\\\ny_t & = c + y_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\varepsilon_t\n\\end{aligned}\n\\]\n\\(\\log(y_t) \\sim ARIMA(1, 1, 0)\\): \\[\n\\begin{aligned}\n(1 - \\phi_1 L)(1- L) \\log(y_t) & = (1 - \\phi_1 L)TVy_t = c + \\varepsilon_t \\\\\nTVy_t & = c + \\phi_1 TVy_{t-1} + \\varepsilon_t\n\\end{aligned}\n\\]\n\\(y_t \\sim ARIMA(0, 1, 0)\\): \\[\n\\begin{aligned}\n(1- L) y_t & = c + \\varepsilon_t \\\\\ny_t & = c + y_{t-1} + \\varepsilon_t.\n\\end{aligned}\n\\] Si \\(c=0\\), tenemos un paseo aleatorio; si \\(c \\neq 0\\), tenemos un paseo aleatorio con deriva.\n\n\n\nSimulación de procesos no estacionarios\nVeamos la FAC de tres series simuladas no estacionarias (Figura 4): un paseo aleatorio, un paseo aleatorio con deriva y un modelo lineal. Sus FAC son indistinguibles, pero los tres casos revelan claramente su carácter no estacionario.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) FAC de un paseo aleatorio\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) FAC de un paseo aleatorio con deriva\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) FAC de in proceso determinista lineal\n\n\n\n\n\n\n\nFigura 4: FAC para tres procesos no estacionarios simulados"
  },
  {
    "objectID": "03-04-Tema4.html#recogida-de-residuos",
    "href": "03-04-Tema4.html#recogida-de-residuos",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "6.1 Recogida de Residuos",
    "text": "6.1 Recogida de Residuos\nVamos a aplicar la metodología de Box-Jenkins a la serie Residuos, una serie anual de 1995 a 2023 (fuente Instituto Nacional de Estadística) que muestra los residuos recogidos por o en nombre de las autoridades municipales y eliminados a través del sistema de gestión de residuos. La unidad es el kg per cápita.\n\nresiduos &lt;- read.csv2(\"./series/Residuos.csv\", \n                      header = TRUE)\n\nresiduos &lt;- ts(residuos[, 2], \n               start = 1995, \n               frequency  = 1)\n\nautoplot(residuos,\n         xlab = \"\", \n         ylab = \"Kg per cápita\", \n         main = \"\")\n\n\n\n\n\n\n\nFigura 6: Recogida de residuos\n\n\n\n\n\n\nTransformación de la serie\nEl primer paso es transformar la serie original para que sea estacionaria. La Figura 7 muestra la gráfica temporal y la FAC para la serie original y su primera diferencia, y la función ndiffs usa un contraste de raíces unitarias para determinar el número de diferencias necesarias para que la serie sea estacionaria. Tras su análisis, podemos concluir que es necesario diferenciar la serie una vez. Es decir, \\(d=1\\) o \\(residuos_t \\sim I(1)\\).\n\nautoplot(residuos, xlab = \"\", ylab = \"\", main = \"\")\nautoplot(diff(residuos), xlab = \"\", ylab = \"\", main = \"\")\nggAcf(residuos, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(residuos), xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n\n\n\n\n\n\n(b) Dif. de la serie\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) FAC serie original\n\n\n\n\n\n\n\n\n\n\n\n(d) FAC dif. de la serie\n\n\n\n\n\n\n\nFigura 7: Gráfica y FAC para Residuos\n\n\n\n\n\nndiffs(residuos)\n\n[1] 1\n\n\n\n\nIdentificación\nTras diferenciar la serie, vamos a identificar los valores de \\(p\\) y \\(q\\) a partir de las FAC y FACP de la serie diferenciada (Figura 8).\n\nggtsdisplay(diff(residuos), main = \"\")\n\n\n\n\n\n\n\nFigura 8: Residuos (primera diferencia)\n\n\n\n\n\nObservamos que tanto en la FAC como en la FACP solo la primera autocorrelación sobrepasa las líneas que marcan el intervalo de confianza al 95%. podemos estar ante un proceso AR(1, 1, 0), MA(1, 1, 0) o ARIMA(1, 1, 1). Si queremos hilar fino, podemos aventurar que la FAC presenta decrecimiento y apostar por un proceso ARIMA(1, 1, 0).\nTambién podemos ayudarnos de la función auto.arima, fijando el número de diferenciaciones con d = 1. El argumento trace = TRUE sirve para que en la salida se muestren todos los modelos que se han probado.\n\nauto.arima(residuos, \n           d = 1,\n           trace = TRUE)\n\n\n ARIMA(2,1,2) with drift         : 259.0007\n ARIMA(0,1,0) with drift         : 254.585\n ARIMA(1,1,0) with drift         : 251.1215\n ARIMA(0,1,1) with drift         : 251.5733\n ARIMA(0,1,0)                    : 252.3874\n ARIMA(2,1,0) with drift         : 253.7421\n ARIMA(1,1,1) with drift         : 253.348\n ARIMA(2,1,1) with drift         : 256.7297\n ARIMA(1,1,0)                    : 248.6339\n ARIMA(2,1,0)                    : 251.0241\n ARIMA(1,1,1)                    : 250.6109\n ARIMA(0,1,1)                    : 249.1284\n ARIMA(2,1,1)                    : 253.7815\n\n Best model: ARIMA(1,1,0)                    \n\n\nSeries: residuos \nARIMA(1,1,0) \n\nCoefficients:\n         ar1\n      0.4494\ns.e.  0.1705\n\nsigma^2 = 368.8:  log likelihood = -122.08\nAIC=248.15   AICc=248.63   BIC=250.82\n\n\nLa identificación automática da como mejor modelo \\(p=1\\) y \\(q=0\\) sin constante. Además, observa como el segundo mejor modelo (según AICc) es ARIMA(0, 1, 1) y el tercero un ARIMA(1, 1, 1).\nVamos a asumir que \\(residuos_t \\sim ARIMA(1,1,0)\\) sin deriva (sin constante): \\[\n\\begin{aligned}\n(1 - \\phi_1 L)(1 - L)residuos_t & = \\varepsilon_t \\\\\nresiduos_t &= residuos_{t-1} + \\phi_1(residuos_{t-1} - residuos_{t-2}) + \\varepsilon_t\n\\end{aligned}\n\\]\n\n\nEstimación\nAunque existe la función arima de stats, vamos a usar la función Arima de la librería forecast para estimar el modelo identificado por ser más versátil. El argumento order indica los valores de (p, d, q) como un vector y el argumento lógico include.constant indica si debe incluirse la constante \\(c\\) en el modelo.1.\n\narima110 &lt;- Arima(residuos, \n                  order = c(1, 1, 0), \n                  include.constant = FALSE)\n\narima110\n\nSeries: residuos \nARIMA(1,1,0) \n\nCoefficients:\n         ar1\n      0.4494\ns.e.  0.1705\n\nsigma^2 = 368.8:  log likelihood = -122.08\nAIC=248.15   AICc=248.63   BIC=250.82\n\n\nUna forma rápida, aunque imprecisa, de determinar si un coeficiente es relevante (significativo) es compararlo con su error estándar (standard error, s.e). Si el coeficiente es mayor que dos veces su error estándar, hay evidencia de que es significativo. En la salida de R, en la tabla Coefficients tienes en la primera fila el nombre de los coeficientes; su valor estimado aparece en la segunda fila de la tabla; y los errores estándar en la tercera fila (precedidos por s.e.). Con esta regla, parece que el coeficiente \\(\\phi_1\\) (ar1 en la salida) es significativo.\n\n\nIntervención\nSe analiza si para algún año se observa un error atípico (por ejemplo 3 veces superior al error estándar). La Figura 9 muestra que en los años 1999 y 2004, el residuo sobrepasa los dos errores estándar pero queda lejos de los tres errores estándar así que asumiremos que no hay valores atípicos.\n\nerror &lt;- residuals(arima110)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1995, 2023, 2)) \n\n\n\n\n\n\n\nFigura 9: Error + Intervención\n\n\n\n\n\n\n\nValidación\nCoeficientes significativos\nA fin de poner un poco de objetividad en la decisión de si un coeficiente es significativo (distinto de cero), podemos usar una prueba estadística. Existen varias alternativas (prueba z, prueba t, prueba de Wald…), así que vamos a optar por la más sencilla y cómoda, la prueba z. Esta prueba asume normalidad (asintótica) en la distribución de los coeficientes.\nPara implementar la prueba z usaremos la función coeftest (se precisa la librería lmtest) que contrasta individualmente si cada coeficiente de un modelo es significativo. Esta función requiere por defecto un solo argumento, el que contiene la estimación del modelo Arima.\n\ncoeftest(arima110)\n\n\nz test of coefficients:\n\n    Estimate Std. Error z value Pr(&gt;|z|)   \nar1  0.44945    0.17051   2.636 0.008389 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComo el valor de p es menor que el nivel de significatividad \\(\\alpha = 0.05\\), se concluye que \\(\\phi_1\\) es significativo.\nMedidas de bondad del ajuste\nAdemás, tenemos las diferentes medidas de bondad del ajuste. En media nos equivocamos en 19 kg per cápita (RMSE) y el error porcentual medio (MAPE) es 2.5%. El modelo no presenta sesgo –el error medio es \\(ME=\\) -1.1, relativamente bajo en comparación con el valor medio de la serie– y los intervalos de confianza están correctamente calculados.\n\naccuracy(arima110)\n\n\n\n               ME  RMSE   MAE   MPE MAPE MASE  ACF1\nTraining set -1.1 18.53 14.03 -0.24 2.54 0.87 -0.03\n\n\n\n\nIncorrelación\nLo veremos con el test de Box-Ljung\n\nLa hipótesis nula es \\(H_0: \\rho_1 = ... = \\rho_k = 0\\)\nEl valor de p = 0.829 es mayor que el nivel de significatividad 0.05. No se rechaza la hipótesis de incorrelación, hasta el orden \\(k = 2\\).\n\n\nBox.test(error, lag = 2,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 0.37605, df = 2, p-value = 0.8286\n\n\nLa elección de dos retardos para la prueba, fijado con el parámetros lag = 2, es bastante arbitraria. Sería mejor realizar la prueba para un rango de valores de \\(k\\) (véase Tabla 2).\n\n\n\n\nTabla 2: Prueba de incorrelacion para diferentes valores del retardo\n\n\n\n\n\n\nk\nvalor.de.p\n\n\n\n\n1\n0.845\n\n\n2\n0.829\n\n\n3\n0.211\n\n\n4\n0.330\n\n\n\n\n\n\n\n\n\n\nHomocedasticidad (varianza constante)\nLo veremos con el test de Box-Ljung para el residuo al cuadrado. La hipótesis nula seria que las primeras \\(k\\) autocorrelaciones estimadas sobre el cuadrado del residuo son cero.\nEl valor de p = 0.982 es mayor que el nivel de significatividad 0.05. No se rechaza la hipótesis de homocedasticidad, hasta el orden 2.\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 0.037009, df = 2, p-value = 0.9817\n\n\nDe nuevo, la elección de dos retardos es totalmente arbitraria y sería mejor realizar la prueba para un rango de valores de \\(k\\) (véase Tabla 3).\n\n\n\n\nTabla 3: Prueba de homocedasticidad para diferentes valores del retardo\n\n\n\n\n\n\nk\nvalor.de.p\n\n\n\n\n1\n0.899\n\n\n2\n0.982\n\n\n3\n0.984\n\n\n4\n0.991\n\n\n\n\n\n\n\n\n\n\nNormalidad\nRecuerda que todos las pruebas de normalidad son muy sensibles al tamaño de la muestra. Siempre es recomendable empezar por un análisis gráfico (histograma, gráfico PP, gráfico QQ).\nSin embargo, cuando es necesario un criterio más objetivo o se precisa de un proceso automático, entonces si la muestra es reducida (menos de 30 o 50 observaciones, según autores) se aplica la prueba de Shapiro-Wilk; en otro caso se aplica la prueba de Jarque-Bera (de la librería tseries) o Kolmogorov-Smirnov. En nuestro ejemplo, con 26 datos, lo correcto es aplicar la prueba de Shapiro-Wilk.\n\nshapiro.test(error)\n\n\n    Shapiro-Wilk normality test\n\ndata:  error\nW = 0.97865, p-value = 0.8028\n\n\n\n\n\n\nPredicción\nUna vez validado el modelo podemos pasar a realizar predicciones, por ejemplo a 5 años vista.\n\nparima110 &lt;- forecast(arima110, \n                      h = 5, \n                      level = 95)\n\nparima110\n\n     Point Forecast    Lo 95    Hi 95\n2024       457.3594 419.7202 494.9986\n2025       453.9253 387.6450 520.2056\n2026       452.3819 361.5146 543.2491\n2027       451.6882 339.6295 563.7469\n2028       451.3764 320.7577 581.9951\n\n\n\nautoplot(parima110, \n         xlab = \"\", \n         ylab = \"Kg per cápita\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1994, 2028, 2)) \n\n\n\n\n\n\n\nFigura 10: Residuos (1999-2023) y predicción (2024-2028)\n\n\n\n\n\nLa Figura 10 muestra la serie, la previsión y el intervalo de confianza al 95%. Dado el modelo estimado y que los dos últimos datos muestran una caída en el volumen de residuos generados, la predicción decrece suavemente. Además, en las series diferenciadas el intervalo de confianza de las predicciones crece muy rápidamente porque los errores se van acumulando rápidamente."
  },
  {
    "objectID": "03-04-Tema4.html#aforo-de-vehículos",
    "href": "03-04-Tema4.html#aforo-de-vehículos",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "6.2 Aforo de vehículos",
    "text": "6.2 Aforo de vehículos\nVamos a aplicar las diferentes metodologías vistas en este tema a la serie aforo de vehículos por Oropesa, carretera N-340, km. 996,48 (fuente Ministerio de Fomento). La serie es anual de 1960 a 2024 (65 datos). La serie ofrece el número medio diario de vehículos que pasan por esta carretera.\n\naforo &lt;- read.csv(\"./series/Aforo_oropesa.csv\", \n                   header = TRUE)\n\naforo &lt;- ts(aforo, \n            start = 1960, \n            freq = 1)\n\nautoplot(aforo, \n         xlab = \"\", \n         ylab = \"Vehículos diarios\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 11: Aforo de vehículos en N-340, Oropesa\n\n\n\n\n\nLos puntos de cambio en la tendencia de la serie Aforo están muy relacionados con la autopista AP-7 y las crisis ocurridas en España: la caída del aforo en 1979 se debe a la inauguración en 1978 del tramo de la AP-7 Torreblanca - Castellón; el incremento del Aforo al inicio de la década de los 80 se debe al periodo de expansión económica en España; y la caída en el aforo al partir de 2009 a la crisis financiera que llevó a la Gran Recesión. Además, se observa un clara intervención, la caída puntual en 2020 originada por el confinamiento durante la pandemia de la Covid-19.\nEn este ejemplo incluiremos, por primera vez, intervención y veremos como la presencia de valores atípicos puede distorsionar el proceso de identificación. Por ello, es conveniente realizar en paralelo ambas actividades, identificar el proceso y detectar valores atípicos.\n\nTransformación de la serie\nLa Figura 12 muestra que la serie Aforo no es estacionaria. por lo que el primer paso es transformar la serie original para que lo sea. La serie no es estacionaria, pero sí lo es su primera diferencia. Ten siempre presente que diferenciar más veces de las necesarias puede dificultar la identificación y la interpretación. Por otro lado, la función ndiffs aconseja una diferenciación. Así, optamos por fijar \\(d = 1\\).\n\nautoplot(aforo, xlab = \"\", ylab = \"\", main = \"\")\nautoplot(diff(aforo), xlab = \"\", ylab = \"\", main = \"\")\nggAcf(aforo, xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(aforo), xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n\n\n\n\n\n\n(b) Dif. de la serie\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) FAC serie original\n\n\n\n\n\n\n\n\n\n\n\n(d) FAC dif. de la serie\n\n\n\n\n\n\n\nFigura 12: Gráfica y FAC para Aforo\n\n\n\n\n\nndiffs(aforo)\n\n[1] 1\n\n\n\n\nIdentificación y estimación\nVeamos ahora a identificar los valores de \\(p\\) y \\(q\\) a partir de la FAC y la FACP, que se muestran en la Figura 13. Ni para la FAC ni para la FACP se observan coeficientes fuera del intervalo de confianza (líneas azules de las gráficas). Podría tratarse de un proceso ARIMA(0, 1, 0).\n\nggtsdisplay(diff(aforo))\n\n\n\n\n\n\n\nFigura 13: Aforo (una diferencia)\n\n\n\n\n\n¿Qué recomienda auto.arima?\n\nauto.arima(aforo, \n           d = 1)\n\nSeries: aforo \nARIMA(0,1,0) \n\nsigma^2 = 1019660:  log likelihood = -533.53\nAIC=1069.06   AICc=1069.13   BIC=1071.22\n\n\nSugiere un proceso ARIMA(0,1,0). Vamos a ver la gráfica de los residuos del modelo para identificar los valores extremos (intervención).\n\narima010 &lt;- Arima(aforo, \n                  order = c(0, 1, 0),\n                  include.constant = FALSE)\n\nerror &lt;- residuals(arima010)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1960, 2024, 4)) \n\nfechas &lt;- format(seq(as.Date(\"1960-01-01\"), as.Date(\"2024-01-01\"), \"year\"), \"%Y\")\nfechas[abs(error) &gt; 2.8 * sderror]\n\n[1] \"2011\" \"2020\"\n\n\n\n\n\n\n\n\nFigura 14: Error + Intervención\n\n\n\n\n\nSe identifican dos años atípicos, dos intervenciones, porque los errores supera las 3 desviaciones típicas, uno en el año 2011, relacionado con la Gran Recesión, y otro en 2020. Además, se observa otro potencial año atípico en 1979 (supera las 2.5 desviaciones típicas) ya identificado con el método de Alisado. Cada una de las intervenciones es del tipo pulso porque solo afecta un periodo de la serie y tienen causas identificadas.\nAhora, creamos una variable ficticia asociada a cada intervención, que denominaremos d1979, d2011 y d2020. La forma de definir la variable ficticia asociada a un pulso consiste en crear una variable de ceros, excepto para el periodo atípico en que la variable valdrá 1.\n\nd1979 &lt;- 1*(fechas == 1979)\nd2011 &lt;- 1*(fechas == 2011)\nd2020 &lt;- 1*(fechas == 2020)\n\nPor último, incluimos las tres variables ficticias en la autoidentificación.\n\nauto.arima(aforo,\n           d = 1,\n           xreg = cbind(d1979,  d2011, d2020))\n\nSeries: aforo \nRegression with ARIMA(2,1,0) errors \n\nCoefficients:\n         ar1     ar2       d1979      d2011       d2020\n      0.2023  0.4119  -1670.8067  -1188.599  -2369.1885\ns.e.  0.1171  0.1201    494.4281    458.598    459.5709\n\nsigma^2 = 604969:  log likelihood = -514.47\nAIC=1040.94   AICc=1042.42   BIC=1053.9\n\n\nObserva como la inclusión de intervención modifica la autoidentificación, que ahora es un proceso ARIMA(2, 1, 0).\n\narima210 &lt;- Arima(aforo, \n                  order = c(2, 1, 0),\n                  include.constant = FALSE,\n                  xreg = cbind(d1979, d2011, d2020))\narima210\n\nSeries: aforo \nRegression with ARIMA(2,1,0) errors \n\nCoefficients:\n         ar1     ar2       d1979      d2011       d2020\n      0.2023  0.4119  -1670.8067  -1188.599  -2369.1885\ns.e.  0.1171  0.1201    494.4281    458.598    459.5709\n\nsigma^2 = 604969:  log likelihood = -514.47\nAIC=1040.94   AICc=1042.42   BIC=1053.9\n\n\nTodos los coeficientes estimados, excepto el ar1 (\\(\\phi_1\\)), superan las dos desviaciones estándar y parece que son significativos.\nLa Figura 15 muestra que para ningún año se observa un error atípico. Es decir, no es necesaria más intervención.\n\nerror &lt;- residuals(arima210)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1960, 2024, 4)) \n\n\n\n\n\n\n\nFigura 15: Error + Intervención\n\n\n\n\n\n\n\nValidación\nVariables significativas\nLa identificación de errores atípicos —para la posterior inclusión de sus variables de intervención asociadas— ha sido un tanto arbitraria: ¿es atípico el error que supera las 2 desviaciones típicas, las dos y media, las tres desviaciones típicas?\nDe nuevo, podemos contrastar si sus coeficientes son significativos y dejar solo aquellas variables de intervención cuyos coeficientes lo sean. Aunque si la serie es suficientemente larga, también podríamos saltarnos este paso y dejar las variables de intervención que mejoren las predicciones extramuestrales del modelo o las que recojan efectos conocidos.\nVeamos qué coeficientes estimados son significativos.\n\ncoeftest(arima210)\n\n\nz test of coefficients:\n\n         Estimate  Std. Error z value  Pr(&gt;|z|)    \nar1       0.20233     0.11712  1.7275 0.0840715 .  \nar2       0.41193     0.12011  3.4295 0.0006047 ***\nd1979 -1670.80671   494.42812 -3.3793 0.0007268 ***\nd2011 -1188.59914   458.59796 -2.5918 0.0095472 ** \nd2020 -2369.18850   459.57086 -5.1552 2.533e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLas tres variables de intervención son significativas y el coeficiente \\(\\phi_2\\) (ar2) también. No es significativo el coeficiente \\(\\phi_1\\) (ar1) al 5% pero si al 10%. En cualquier caso, los modelos Arima son modelos jerárquicos donde la presencia de un coeficiente –autorregresivo o de media móvil– significativo de cierto orden exige que los coeficiente de orden inferior estén presentes, sean o no significativos. En nuestro caso, como el coeficiente \\(\\phi_2\\) es significativo, se debe dejar en el modelo el coeficiente \\(\\phi_1\\).\nMedidas de bondad del ajuste\nEl error medio es 741 vehículos por día (RMSE) y el error porcentual medio (MAPE) es 6%. No hay sesgo de predicción y la fórmula empleada para el cálculo del intervalo de confianza de las predicciones es válida.\n\naccuracy(arima210)\n\n\n\n                ME   RMSE    MAE MPE MAPE MASE ACF1\nTraining set 57.17 741.03 552.44 1.3 5.98 0.76 0.05\n\n\n\n\nIncorrelación, Homocedasticidad y Normalidad de los residuos\nVeamos ahora si el residuo es ruido blanco:\n\nBox.test(error, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 0.27509, df = 2, p-value = 0.8715\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\") \n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 2.252, df = 2, p-value = 0.3243\n\njarque.bera.test(error)\n\n\n    Jarque Bera Test\n\ndata:  error\nX-squared = 1.5694, df = 2, p-value = 0.4562\n\n\nLas hipótesis de incorrelación y homocedasticidad se aceptan. También se aceptarían para otros valores de \\(k\\) razonables. Igualmente, se acepta la hipótesis de normalidad.\n\n\nInterpretación del modelo\nHemos validado y estimado el modelo \\(aforo_t \\sim ARIMA(2,1,0) + AI\\). En concreto tenemos que el modelo teórico es:\n\\[(1 - \\phi_1 L - \\phi_2 L^2)(1 - L)aforo_t =  \\varepsilon_t + \\gamma_1 \\cdot d1979 + \\gamma_2 \\cdot d2011 + \\gamma_3 \\cdot d2020.\\]\nSi desarrollamos, queda:\n\\[aforo_t = aforo_{t-1} + \\phi_1(aforo_{t-1}-aforo_{t-2}) + \\phi_2(aforo_{t-2}-aforo_{t-3}) +\\] \\[\\gamma_1 \\cdot d1979 +  \\gamma_2 \\cdot d2011 + \\gamma_3 \\cdot d2020 + \\varepsilon_t.\\]\nFinalmente, el modelo estimado es: \\[\\widehat{aforo}_t = aforo_{t-1} + 0.20(aforo_{t-1}-aforo_{t-2}) + 0.41(aforo_{t-2}-aforo_{t-3})\\] \\[-1671 \\cdot d1979 - 1189 \\cdot d2011 - 2369 \\cdot d2020\\]\nCada año el aforo es el mismo que el aforo del año pasado más un 20% del último incremento observado y un 41% del incremento anterior.\nRespecto de la intervención, en 1979 se redujo el aforo en 1700 vehículos por día respecto de lo esperado debido a la apertura de la autopista AP-7; en 2011 aumentó el aforo en 1200 vehículos por día debido a la Gran Recesión; y en 2020 las restricciones de movilidad debidas a la pandemia redujeron el aforo en 2400 vehículos al día.\n\n\nPredicción\nComo hemos incluido tres variables ficticias en el ajuste, de cara a predecir el aforo hemos de indicar cuáles serán los valores futuros para estas variables. En este caso serán cero puesto que son intervenciones que no responden a un efecto calendario. Las causas detrás de estas intervenciones no se espera que se repitan en el futuro.\nYa hemos visto que para ello se incluye en el comando forecast el argumento xreg = cbind(rep(0, 4), rep(0, 4), rep(0, 4)) que añade cinco ceros por cada variable de intervención porque la predicción va a ser a cinco años vista.\n\nparima210 &lt;- forecast(arima210, \n                      h = 4, \n                      level = 95,\n                      xreg = cbind(d1979=rep(0, 4), d2011=rep(0, 4), \n                                   d2020=rep(0, 4)))\nparima210\n\n     Point Forecast    Lo 95    Hi 95\n2025       10078.48 8554.022 11602.93\n2026       10520.44 8136.436 12904.44\n2027       10742.28 7270.924 14213.64\n2028       10969.22 6515.638 15422.81\n\n\nPara 2025 se espera un paso de 10078 vehículos al día por la N-340 a la altura de Oropesa.\n\nautoplot(parima210, \n         xlab = \"\",\n         ylab = \"Vehículos diarios\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1960, 2028, 4)) \n\n\n\n\n\n\n\nFigura 16: Aforo (1960-2024) y predicción (2025-2028)\n\n\n\n\n\n\n\n\n\nValidación con origen de predicción móvil\nVamos a calcular el error extramuestral según el horizonte temporal de previsión. Asumiremos que se precisan 30 años para estimar el modelo, fijaremos el horizonte temporal en 4 años y calcularemos el error MedAPE, para evitar el efecto de los años atípicos.\n\nk &lt;- 30                  \nh &lt;- 4                  \nT &lt;- length(aforo)     \ns &lt;- T - k - h    \n\nmapeArima &lt;- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set &lt;- subset(aforo, start = i + 1, end = i + k)\n  test.set &lt;-  subset(aforo, start = i + k + 1, end = i + k + h) \n  \n  fit &lt;- Arima(train.set, \n               include.constant = FALSE,\n               order = c(2, 1, 0))\n  \n  fcast &lt;- forecast(fit, h = h)\n\n  mapeArima[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  \n}\n\nmapeArima &lt;- apply(mapeArima, MARGIN = 2, FUN = median)\nmapeArima\n\n[1]  2.075815  6.455624 10.646505 12.474715\n\n\nEl error de previsión extramuestral crece notablemente con el horizonte temporal. El error de las previsiones a un año vista es del 2%, muy inferior al error intramuestral del 6% por ser un error mediano, pero para tres años vista alcanza el 10%."
  },
  {
    "objectID": "03-04-Tema4.html#consumo-de-alimentos-en-el-hogar-per-cápita",
    "href": "03-04-Tema4.html#consumo-de-alimentos-en-el-hogar-per-cápita",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "6.3 Consumo de alimentos en el hogar per cápita",
    "text": "6.3 Consumo de alimentos en el hogar per cápita\nAnalizaremos el consumo alimentario en hogar per cápita en España. Esta serie está construida a partir de la serie de consumo alimentario en hogar (disponible en el Ministerio de Agricultura, Alimentación y Medio Ambiente), y la serie de población (disponible en el Instituto Nacional de Estadística). Es una serie anual de 1990 a 2024 (35 datos) y la unidad es el Kg per cápita. La Figura 17 muestra que es una serie estacionaria.\n\nalimentospc &lt;- read.csv2(\"./series/Alimentacionpc.csv\",\n                         header = TRUE)\n\nalimentospc &lt;- ts(alimentospc,\n                  start = 1990, \n                  freq = 1)\n    \nautoplot(alimentospc, \n         xlab = \"\", \n         ylab = \"Kg per cápita\",\n         main = \"\",\n         ylim = c(0, 700))\n\n\n\n\n\n\n\nFigura 17: Consumo alimentario en hogar\n\n\n\n\n\nEl pico en el año 2020 se debe al aumento del consumo de alimentos en el hogar causado por el periodo de confinamiento por la Covid-19 (marzo a junio) y el aumento del trabajo desde casa. La aparente caída desde 2022 se debe a que como efecto rebote, los españoles ahora comemos y cenamos más fuera del hogar.2\n\nTransformación de la serie\nLa gráfica de la serie original y su FAC (véase la Figura 17 y la Figura 18) indican que la serie original ya es estacionaria y la función ndiffs lo corrobora. Por tanto asumimos que \\(d=0\\) o \\(alimentospc_t \\sim I(0)\\).\n\nggAcf(alimentospc, xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\nFigura 18: FAC para Alimentos\n\n\n\n\n\n\nndiffs(alimentospc)\n\n[1] 0\n\n\n\n\nIdentificación y Estimación\nGráficamente hemos observado posibles intervenciones, en los años 2020, 2022 y 2023. Vamos a incorporar esta información en auto.arima para obtener una identificación mejor.\nCreamos una variable ficticia asociada a la intervención de 2020 cuya causa es el aumento del consumo de alimentos en el hogar durante el confinamiento y solo afecta un año (pulso). La forma de definir la variable ficticia asociada a un pulso consiste en crear una variable de ceros, excepto para el año atípico en que la variable valdrá 1. Denominaremos a esta variable ficticia d2020.\nLa intervención de 2022 se debe a la caída en el consumo de alimentos en el hogar debida al incremento del ocio, pero a diferencia de la anterior intervención, esta caída se ha mantenido durante los años siguientes, tomando forma de cambio permanente (level shift). La forma de definir la variable ficticia asociada a un cambio permanente consiste en crear una variable de ceros, excepto para el periodo de años atípicos en que la variable valdrá 1. Denominaremos a la variable ficticia asociada l2022.\n\nfechas &lt;- format(seq(as.Date(\"1990-01-01\"), as.Date(\"2024-01-01\"), \"year\"), \"%Y\")\nd2020 &lt;- 1 * (fechas == 2020)\nl2022 &lt;- 1 * (fechas &gt; 2021)\n\nPor último, incluimos las dos variables ficticias en la autoidentificación.\n\nauto.arima(alimentospc,\n           d = 0,\n           xreg = cbind(d2020, l2022))\n\nSeries: alimentospc \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    d2020     l2022\n      0.6834   632.7882  55.3717  -69.7211\ns.e.  0.1250     6.2452   9.8367   11.5582\n\nsigma^2 = 158.1:  log likelihood = -136.46\nAIC=282.92   AICc=284.99   BIC=290.7\n\n\nLa identificación automática sugiere un proceso AR(1) con constante (intercept) por ser el que menor AICc tiene. Por tanto, ha estimado los coeficientes \\(\\phi_1\\) y \\(\\mu\\) (que denomina ar1 e intercept) y los coeficientes de la intervención para d2020 y l2022. Todos los coeficientes parecen significativos.\nAhora vamos a estimar el modelo identificado. El análisis gráfico (Figura 19) y numérico de los residuos de este proceso no detecta más valores extremos.\n\narima100 &lt;- Arima(alimentospc, \n                  order = c(1, 0, 0),\n                  include.constant = TRUE,\n                  xreg = cbind(d2020, l2022))\n\narima100\n\nSeries: alimentospc \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    d2020     l2022\n      0.6834   632.7882  55.3717  -69.7211\ns.e.  0.1250     6.2452   9.8367   11.5582\n\nsigma^2 = 158.1:  log likelihood = -136.46\nAIC=282.92   AICc=284.99   BIC=290.7\n\n\n\nerror &lt;- residuals(arima100)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  geom_point() +\n  scale_x_continuous(breaks= seq(1990, 2024, 4)) \n\nfechas[abs(error) &gt; 2.8 * sderror]\n\ncharacter(0)\n\n\n\n\n\n\n\n\nFigura 19: Error + Intervención\n\n\n\n\n\nSi rebajamos el número de desviaciones estándar de 2.8 a 2.5, aparece otro valor extremo en 1999, con un consumo inferior al esperado. Sin embargo, para este año no se identifica ninguna razón para la caída del consumo por lo que damos el modelo previo como definitivo. Toda intervención debe tener detrás una causa identificable; si no, puede ser fruto del simple azar.\n\n\nValidación\nCoeficientes significativos\nTanto \\(\\phi_1\\) como el intercepto \\(\\mu\\) y las variables de intervención son significativas.\n\ncoeftest(arima100)\n\n\nz test of coefficients:\n\n           Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1         0.68338    0.12495   5.4692 4.521e-08 ***\nintercept 632.78823    6.24524 101.3233 &lt; 2.2e-16 ***\nd2020      55.37173    9.83670   5.6291 1.812e-08 ***\nl2022     -69.72109   11.55823  -6.0322 1.618e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nMedidas de bondad del ajuste\nLa precisión del ajuste es 11.8 kg per cápita (RMSE) y el error porcentual medio (MAPE) es 1.4%. No hay sesgo y los intervalos de confianza de las predicciones los vamos a considerar correctos.\n\naccuracy(arima100)\n\n\n\n               ME  RMSE  MAE   MPE MAPE MASE ACF1\nTraining set 0.22 11.83 8.65 -0.01 1.38 0.62  0.1\n\n\n\n\nIncorrelación, Homocedasticidad y Normalidad del residuo\nVeamos ahora si el residuo es ruido blanco:\n\nerror &lt;- residuals(arima100)\nBox.test(error, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 0.60162, df = 2, p-value = 0.7402\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\") \n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 0.26066, df = 2, p-value = 0.8778\n\nshapiro.test(error)\n\n\n    Shapiro-Wilk normality test\n\ndata:  error\nW = 0.94782, p-value = 0.09709\n\n\nLas hipótesis de incorrelación y homocedasticidad se aceptan. También se aceptarían para otros valores de \\(k\\) razonables. La hipótesis de normalidad también se acepta al 5%.\n\n\nInterpretación del modelo\nEl modelo teórico identificado es \\[(1 - \\phi_1 L) alimentospc_t = c +  \\gamma_1 d2020 + \\gamma_2 l2022 + \\varepsilon_t,\\] que desarrollando queda \\[alimentospc_t = c + \\phi_1 alimentospc_{t-1} + \\gamma_1 d2020 + \\gamma_2 l2022  + \\varepsilon_t.\\]\nFinalmente, el modelo estimado es \\[\\widehat{alimentospc}_t = 200.4 + 0.68 \\cdot alimentospc_{t-1} +\\] \\[55.4\\cdot d2020 - 69.7\\cdot l2022.\\]\n\n\n\n\n\n\nLa constante del modelo teórico y la media del modelo estimado\n\n\n\nEl término constante \\(\\mu\\) que estima R no es el valor “c” que hemos visto en la teoría. Para convertir la constante estimada por R en “c” hemos de multiplicarla por el polinomio autorregresivo. En este caso, \\[c = (1 - \\phi_1)\\cdot \\mu = (1 - 0.68338) \\cdot 632.78823 = 200.3534\\]\n\n\nCada año el consumo de alimentos per cápita en el hogar es 200 kilos más un 68% del consumo del año pasado.\nEn 2020, debido al efecto combinado del periodo de confinamiento entre marzo y junio y el incremento del trabajo en casa, se produjo un fuerte aumento del consumo de alimentos en el hogar, estimado en 55 kg per cápita. Por el contrario, a partir de 2022 se redujo el consumo de forma permanente en unos en 70 kg, por el aumento del ocio.\n\n\nPredicciones de la serie\nComo hemos incluido dos variables ficticias en el ajuste, de cara a predecir el consumo de alimentos hemos de indicar cuáles serán los valores futuros para estas variables. No cabe esperar otro periodo de confinamiento en los próximos años, así que podemos fijar a 0 los valores futuros de d2020. Sin embargo, hemos asumido que el consumo de alimentos en el hogar ha caído de forma permanente desde 2022, así que debemos fijar a 1 los valores futuros de l2022.\nEn R esto se hace incluyendo en el comando forecast el argumento xreg = cbind(rep(0, 5), rep(1, 5)) que añade cinco ceros para la primera variable de intervención y cinco unos para la segunda porque la predicción va a ser a cinco años vista.\n\nparima100 &lt;- forecast(arima100, \n                      h = 5, \n                      level = 95,\n                      xreg = cbind(rep(0, 5), rep(1, 5)))\n\nparima100\n\n     Point Forecast    Lo 95    Hi 95\n2025       553.8475 529.2034 578.4917\n2026       556.7666 526.9175 586.6157\n2027       558.7615 526.7704 590.7525\n2028       560.1247 527.1810 593.0684\n2029       561.0563 527.6770 594.4356\n\n\nPodemos ver gráficamente las predicciones (véase la Figura 20).\n\nautoplot(parima100, \n         series = \"Alimentos\",\n         xlab = \"\",\n         ylab = \"Kg per cápita\",\n         main = \"\",\n         PI = FALSE,\n         ylim = c(0, 700))\n\n\n\n\n\n\n\nFigura 20: Consumo de alimentos y predicción\n\n\n\n\n\n\n\nValidación con origen de predicción móvil\nVamos a calcular el error extramuestral según el horizonte temporal de previsión. Asumiremos que se precisan 20 años para estimar el modelo, fijaremos el horizonte temporal en 2 años y calcularemos el error MedAPE, para evitar el efecto de los años atípicos.\n\nk &lt;- 20                  \nh &lt;- 2\nT &lt;- length(alimentospc)     \ns &lt;- T - k - h    \n\nmapeArima &lt;- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set &lt;- subset(alimentospc, start = i + 1, end = i + k)\n  test.set &lt;-  subset(alimentospc, start = i + k + 1, end = i + k + h) \n  \n  fit &lt;- Arima(train.set, \n               include.constant = TRUE,\n               order = c(1, 0, 0))\n  \n  fcast &lt;- forecast(fit, h = h)\n\n  mapeArima[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  \n}\n\nmapeArima &lt;- apply(mapeArima, MARGIN = 2, FUN = median)\nmapeArima\n\n[1] 1.401493 2.181278\n\n\nEl error de previsión extramuestral a un año vista es similar al error de ajuste y el error a dos años vista se mantiene notablemente bajo."
  },
  {
    "objectID": "03-04-Tema4.html#comparación-con-alisado-exponencial",
    "href": "03-04-Tema4.html#comparación-con-alisado-exponencial",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "6.4 Comparación con alisado exponencial",
    "text": "6.4 Comparación con alisado exponencial\nVeamos una comparativa, para los tres ejemplos vistos, entre los resultados obtenidos con ARIMA y con Alisado exponencial.\n\nResiduos:\n\nMAPE ARIMA: \\(2.54\\%\\) - ARIMA(1,1,0) sin deriva\nMAPE ETS: \\(2.77\\%\\) - ETS(M,A,N), \\(\\alpha=1\\), \\(\\beta = 0.45\\)\nAmbos métodos presentan similar calidad de ajuste.\n\n\n\nAforo:\n\nMAPE ARIMA: \\(5.98\\%\\) - ARIMA(2,1,0) sin deriva, con intervención\nMAPE ETS: \\(7.00\\%\\) - ETS(M,A,N), \\(\\alpha=0.97\\), \\(\\beta=0.03\\)\nARIMA tiene menor error al incluir tres variables de intervención\n\n\n\nAlimentos per cápita:\n\nMAPE ARIMA: \\(1.38\\%\\) - ARIMA(1,0,0) con constante e intervención\nMAPE ETS: \\(2.21\\%\\) - ETS(A,N,N), \\(\\alpha = 0.96\\)\nARIMA tiene menor error menor y permite capturar la caída del consumo de alimentos en el hogar tras la pandemia"
  },
  {
    "objectID": "03-04-Tema4.html#footnotes",
    "href": "03-04-Tema4.html#footnotes",
    "title": "Series Temporales: Procesos ARIMA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMira en la ayuda de la función Arima la diferencia entre los argumentos include.mean, include.drift e include.constant↩︎\nSegún el INE, el gasto monetarios per cápita de los españoles en restauración y hoteles cayó ligeramente en 2018 y 2019, y retrocedió un 40% en 2020 a causa de la pandemia. Desde entonces, el gasto per cápita en esta partida ha crecido año tras año: un 30.7% en el 2021 y 2022 y un 13.6% en el 2023.↩︎"
  },
  {
    "objectID": "04-05-Valores_perdidos_Outliers.html#footnotes",
    "href": "04-05-Valores_perdidos_Outliers.html#footnotes",
    "title": "Valores perdidos y valores atípicos",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLos métodos sencillos de previsión y los modelos ARIMA funcionan perfectamente con valores perdidos, pero la función ets para Alisado exponencial o las funciones stlf y tbats, para trabajar con series con más de una componente estacional, no funcionan si hay valores perdidos.↩︎\nEl nombre de la función na.interp viene de unir na de not available (valor perdido) e interp de interpolación.↩︎\nPuedes leer más detalles sobre el proceso de identificación de valores atípicos usado por la función tsoutlier pinchando aquí↩︎"
  },
  {
    "objectID": "03-08-Ejemplo3.html",
    "href": "03-08-Ejemplo3.html",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "",
    "text": "1 Introducción\nConsideremos la serie temporal correspondiente al número de defunciones causadas por enfermedades cerebrovasculares, Esta serie está disponible en el Instituto Nacional de Estadística desde enero de 1980 hasta diciembre de 2024, un total de 45 años o 540 meses.\nLa serie presenta tendencia decreciente y estacionalidad de orden 12 en un claro esquema multiplicativo (véase Figura 1). Ya vimos en el análisis descriptivo que el determinante principal del patrón estacional es la temperatura.\nAdemás, la descomposición revelaba la presencia de varios valores atípicos concentrados al inicio de la serie. Por este motivo vamos a recortar la serie y considerarla solo desde enero de 1990, 35 años o 420 meses.\n\nDefEnfCer &lt;- read.csv2(\"./series/Enfermedades cerebrovasculares.csv\", \n                       header = TRUE)\n\nDefEnfCer &lt;- ts(DefEnfCer[,2], \n                start = 1980, \n                frequency = 12)\n\nDefEnfCer &lt;- window(DefEnfCer, \n                    start = 1990)\n\nautoplot(DefEnfCer,\n         xlab = \"\",\n         ylab = \"Defunciones\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1990, 2024, 2)) \n\n\n\n\n\n\n\nFigura 1: Defunciones causadas por enfermedades cerebrovasculares\n\n\n\n\n\nVeamos que transformaciones son necesarias para lograr que la serie sea estacionaria y ergódica.\n\n\n\n\n\n2 Transformación logarítmica\nEl esquema multiplicativo aconseja el uso del logaritmo para el análisis de la serie. Veamos que transformación de Box-Cox nos sugiere R:\n\n(nl &lt;- BoxCox.lambda(DefEnfCer))\n\n[1] -0.5330294\n\nwDefEnfCer &lt;-BoxCox(DefEnfCer, lambda = nl)\n\nEl valor sugerido de \\(\\lambda=-0.53\\) está alejado de 0 y es difícil de interpretar. Afortunadamente la Figura 2 muestra que la transformación logarítmica (panel inferior) genera una serie muy similar a la transformación de Box-Cox óptima (panel medio); y que ambas series transformadas tienen una varianza más constante que la serie original (panel superior).\n\nseries &lt;- cbind(\"Original\" = DefEnfCer,\n                \"Transformación Box-Cox\" = wDefEnfCer,\n                \"Logaritmo\" = log(DefEnfCer))\nautoplot(series, facets = TRUE,\n         xlab = \"\",\n         ylab = \"\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 2: Defunciones causadas por enfermedades cerebrovasculares y algunas transformaciones\n\n\n\n\n\nConcluimos que se usará el logaritmo de las defunciones causadas por enfermedades cerebrovasculares en lugar de la serie original.\n\n\n\n\n\n3 Diferenciaciones\nUna vez decidida la transformación logarítmica, usaremos la FAC para determinar las diferenciaciones necesarias para alcanzar una serie estacionaria y ergódica.\n\nggAcf(log(DefEnfCer), lag = 48, ylim = c(-1, 1))\nggAcf(diff(log(DefEnfCer)), lag = 48, ylim = c(-1, 1))\nggAcf(diff(log(DefEnfCer), lag = 12),lag = 48, ylim = c(-1, 1))\nggAcf(diff(diff(log(DefEnfCer), lag=12)), lag = 48, ylim = c(-1, 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Dif. regular de la serie\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Dif. estacional de la serie\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(d) Dif. regular y estacional de la serie\n\n\n\n\n\n\n\nFigura 3: FAC para Defunciones (log)\n\n\n\n\nLos dos paneles superiores de la Figura 3 muestran que ni el logaritmo de la serie, ni su tasa de variación mensual (\\(\\nabla \\log(y_t)\\), diferencia regular del logaritmo) son ergódicas. Obsérvese que en ambos casos ni al cabo de 48 retardos los coeficientes de autocorrelación son nulos.\nClaramente, la doble diferencia del logaritmo de la serie es estacionaria y ergódica porque el panel (d) muestra que sólo unos pocos coeficientes de autocorrelación son no nulos.\nLa tasa de variación anual (\\(\\nabla_{12} \\log(y_t)\\), diferencia estacional del logaritmo) no ofrece resultados tan claros. El panel (c) muestra una rápida caída de los valores de la autocorrelación en la parte regular (primeros valores de retardo), pero siguen observándose valores no nulos para valores de retardo altos.\nPor otro lado, las funciones ndiffsy nsdiffs recomiendan la doble diferenciación regular y estacional.\n\nndiffs(log(DefEnfCer))\n\n[1] 1\n\nnsdiffs(log(DefEnfCer))\n\n[1] 1\n\n\nConcluimos que para alcanzar la estacionariedad y verificar la hipótesis de ergodicidad es necesario diferenciar tanto en su parte regular como estacional el logaritmo de la serie: \\(log({y_t}) \\sim I_{12}(1)I(1)\\). Es decir, la serie que verifica todas las hipótesis es \\(\\nabla \\nabla_{12} log({y_t})\\).\nPara finalizar, mostramos gráficamente la serie original y la serie transformada (Figura 4), así como la FAC y FACP de la serie transformada (Figura 5).\n\nseries &lt;- cbind(\"Original\" = DefEnfCer,\n                \"Dif reg. y est. de log\" = diff(diff(log(DefEnfCer), lag = 12)))\nautoplot(series, facets = TRUE,\n         xlab = \"\",\n         ylab = \"\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 4: Defunciones causadas por enfermedades cerebrovasculares. Serie original y transformada\n\n\n\n\n\n\nggtsdisplay(diff(diff(log(DefEnfCer)), lag = 12), lag = 48)\n\n\n\n\n\n\n\nFigura 5: Defunciones por enfermedades cerebrovasculares, FAC y FACP de la serie transformada\n\n\n\n\n\n\n\n\n\n\n4 Contraste KPSS\nVamos a analizar la estacionariedad en media a partir del contraste de raíces unitarias. Como la serie original tiene estacionalidad, vamos a realizar la prueba KPSS a partir de la serie anualizada, ya mostrada en el análisis descriptivo (véase Figura 6).\n\nDefEnfCerAnual &lt;- aggregate(DefEnfCer, FUN = sum)\n\nautoplot(DefEnfCerAnual,\n         xlab = \"\",\n         ylab = \"\",\n         main = \"\") \n\n\n\n\n\n\n\nFigura 6: Defunciones anuales causadas por enfermedades cerebrovasculares\n\n\n\n\n\nDado que la serie muestra una clara tendencia decreciente determinista, procedemos primero con la prueba de KPSS asumiendo que hay tendencia determinista.\n\nsummary(ur.kpss(DefEnfCer, type='tau', lags = 'short'))\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: tau with 5 lags. \n\nValue of test-statistic is: 0.049 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.119 0.146  0.176 0.216\n\n\nPara un nivel de confianza del 5%, si asumimos la presencia de tendencia determinista (type = tau), entonces no hay tendencia estocástica –el estadístico de contraste 0.049 es menor que el valor crítico 0.146.\nDiferenciamos la serie, para eliminar la tendencia determinista, y repetimos el contraste. En este caso, asumimos que la serie ya no tiene tendencia determinista (type = mu). El contraste indica que no se rechaza la hipótesis nula, la serie diferenciada no tiene raíces unitarias y puede considerarse estacionaria.\n\nsummary(ur.kpss(diff(DefEnfCer), type='mu', lags = 'short'))\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 5 lags. \n\nValue of test-statistic is: 0.0209 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nBasta diferenciar la serie una vez para eliminar todo tipo de tendencia, determinista y estocástica. La serie anual de defunciones es integrada de orden uno \\(y_t \\sim I(1)\\)."
  },
  {
    "objectID": "03-03-Tema3.html#definición",
    "href": "03-03-Tema3.html#definición",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "2.1 Definición",
    "text": "2.1 Definición\nUn proceso estocástico \\(Y_t\\) es (sin excesiva precisión) una variable aleatoria que corresponde a momentos sucesivos del tiempo.\nComo toda variable aleatoria, su caracterización puede hacerse a través de su función de distribución o a partir de sus momentos.\n\nLa caracterización de un proceso estocástico mediante los momentos de primer y segundo orden (medias y covarianzas) es más incompleta que cuando se hace mediante funciones de distribución.\nBajo la hipótesis de normalidad el proceso estocástico queda completamente caracterizado a través de los dos primeros momentos.\n\nNosotros contamos con una sola realización del proceso estocástico \\(Y_t\\), la serie temporal observada \\(\\{y_t\\}_{t=1}^T\\).\n\nA partir de \\(\\{y_t\\}_{t=1}^T\\) estimaremos los momentos de primer y segundo orden (medias y covarianzas).\nEstos momentos nos permitirán identificar el PGD.\nUna vez identificado, estimaremos el PGD y podremos realizar inferencia (contrastes de hipótesis y predicciones).\n\nPara que esta línea de razonamiento sea válida es necesario que el proceso sea estacionario y ergódico."
  },
  {
    "objectID": "03-03-Tema3.html#proceso-estacionario",
    "href": "03-03-Tema3.html#proceso-estacionario",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "2.2 Proceso estacionario",
    "text": "2.2 Proceso estacionario\n\nProceso estacionario en sentido estricto\nUn proceso es estacionario en sentido estricto cuando la distribución conjunta no varía al realizar un desplazamiento en el tiempo de todas las variables.\n\nSi \\(F(Y_{t_1},..., Y_{t_k})\\) es la función de distribución conjunta y \\(h&gt;0\\), entonces el proceso es estacionario en sentido estricto si \\[F(Y_{t_1},..., Y_{t_k}) = F(Y_{t_1+h},..., Y_{t_k+h})\\]\n\nIntuitivamente, viendo aisladamente algunos de los datos de la serie es imposible saber a que periodo temporal corresponden.\nComprobar si un proceso es estacionario en sentido estricto es muy difícil, así que vamos a encontrar condiciones suficientes: estacionariedad en media y en sentido amplio (covarianza).\n\n\nProceso estacionario en media\nUn proceso es estacionario en media (o de primer orden) si su nivel se mantiene en el tiempo: \\[E[Y_t] = \\mu \\; \\; \\forall t\\]\n\n\nProceso estacionario en sentido amplio\nUn proceso (ya estacionario en media) es estacionario en sentido amplio, o de segundo orden, si sus momentos de orden dos no dependen del tiempo:\n\nLa (auto)covarianza entre dos periodos de tiempo es finita y sólo depende del intervalo de tiempo transcurrido entre estos dos periodos: \\[Cov[Y_t, Y_{t+k}] = E[(Y_t - \\mu)(Y_{t+k} - \\mu)] = \\gamma_k,\\,\\,\\,\\forall t\\]\n\nObserva que la varianza será entonces \\(Var[Y_t] = E[(Y_t - \\mu)^2] = \\gamma_0\\).\nSi en el contexto de series temporales la autocovarianza la interpretamos como la información de la serie que se transmite entre dos periodos de tiempo \\(t\\) y \\(t'\\), el supuesto de estacionariedad en sentido amplio nos dice que la información transmitida solo depende de la distancia temporal entre los dos periodos \\(t - t'\\) y no los periodos en sí mismos. Por ejemplo, los nacimientos en enero de un año me dan información sobre los nacimientos en enero del año siguiente, y esta información es la misma e independiente del año en consideración. Además, la información transmitida entre dos eneros consecutivos es la misma que entre dos febreros o dos marzos consecutivos porque la distancia es la misma, 12 meses.\n\n\nLa Figura 1 muestra la serie Nacimientos que no es estacionaria ni en media, ni en sentido amplio. No lo es en media porque presenta tendencia y, por tanto, el valor medio de la serie cambia en el tiempo; y no es estacionaria en sentido amplio porque al inicio de la serie los datos presenta más variabilidad que a finales del siglo pasado.\n\n\n\n\n\n\n\n\nFigura 1: Nacimientos mensuales\n\n\n\n\n\n\n\nUn proceso estacionario en sentido estricto también es estacionario en sentido amplio, pero lo contrario no es cierto. Ahora bien, bajo normalidad un proceso estacionario en sentido amplio también lo será en sentido estricto."
  },
  {
    "objectID": "03-03-Tema3.html#proceso-ergódico",
    "href": "03-03-Tema3.html#proceso-ergódico",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "2.3 Proceso ergódico",
    "text": "2.3 Proceso ergódico\nEn el tema siguiente se verá que la estimación de los momentos de primer y segundo orden de la serie temporal permite identificar y estimar el PGD.\nPara que la estimación de los momentos sea consistente el proceso debe ser ergódico: \\[\\lim_{T\\rightarrow \\infty} Var(\\frac{1}{T}\\sum_{t=1}^T Y_t)=0. \\]\nPara que un proceso sea ergódico las observaciones nuevas tienen que aportar suficiente información para que la varianza del valor medio converja a 0. Esto no ocurre si la dependencia entre las variables es muy fuerte.\nUna condición necesaria pero no suficiente para que un proceso estacionario sea ergódico es:\n\\[\\lim_{k\\rightarrow \\infty} \\gamma_k = 0.\\] Es decir, que cuanto más distancia hay entre dos periodos, menos información se transmite. Es decir, cuanto más en el pasado están los datos menos ayudan a entender el presente."
  },
  {
    "objectID": "03-03-Tema3.html#normalidad",
    "href": "03-03-Tema3.html#normalidad",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "2.4 Normalidad",
    "text": "2.4 Normalidad\nAsumiremos que el error del modelo se distribuye como una variable aleatoria normal. Esta hipótesis se puede relajar si la serie tiene suficientes datos."
  },
  {
    "objectID": "03-03-Tema3.html#ideas-generales",
    "href": "03-03-Tema3.html#ideas-generales",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "3.1 Ideas generales",
    "text": "3.1 Ideas generales\nUna serie temporal \\(\\{y_t\\}_{t=1}^T\\) no tiene porque verificar las condiciones de estacionariedad, ergodicidad y normalidad.\nA continuación, veremos una serie de transformaciones que convierten una serie no estacionaria en estacionaria; no ergódica en ergódica; y, de paso, facilitan la verificación de la hipótesis de normalidad, que dejaremos para más adelante.\nEn el panel superior de la Figura 2 vuelves a tener la serie de nacimientos, que denominaremos \\(y_t\\); en el segundo panel la diferencia de la serie, \\(y_t - y_{t-1}\\); y en el panel inferior tienes la diferencia de la transformación logarítmica de la serie, \\(log(y_t) - log(y_{t-1})\\). La serie nacimientos no es estacionaria en media ni en sentido amplio, su diferencia es estacionaria en media, pero no en sentido amplio, Sin embargo, la transformación logarítmica y la diferencia han logrado que sea completamente estacionaria.\n\n\n\n\n\n\n\n\nFigura 2: Serie Nacimientos, su diferencia y la diferencia del logaritmo"
  },
  {
    "objectID": "03-03-Tema3.html#diferenciación",
    "href": "03-03-Tema3.html#diferenciación",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "3.2 Diferenciación",
    "text": "3.2 Diferenciación\nLa diferenciación permite transformar una serie no estacionaria en media en estacionaria en media.\nDiferenciar de orden \\(k\\) consiste en restar a la observación de un periodo la de \\(k\\) periodos antes: \\[\\nabla_k y_t = y_t - y_{t-k}.\\]\n\nDiferenciación regular (\\(k=1\\))\nUn caso concreto es la diferenciación regular o diferenciación de orden uno, que consiste en restar a la observación de un periodo la del periodo precedente:\n\\[\\nabla y_t = y_t - y_{t-1}.\\]\nSi \\(\\nabla y_t\\) no fuera estacionaria, se diferenciaría (regularmente) una segunda vez para obtener una doble diferenciación de primer orden:\n\\[\\nabla^{2} y_t = \\nabla(\\nabla y_t) = \\nabla y_t - \\nabla y_{t-1} = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2}.\\]\nEn la práctica una sola diferenciación suele ser suficiente para obtener la estacionariedad en media; diferenciar dos veces es excepcional; y diferenciar tres o más veces no se da.\nAdemás, en series sin estacionalidad la diferenciación también permite alcanzar la ergodicidad.\n\n\nDiferenciación estacional (\\(k=m\\))\nLa diferencia estacional consiste en restar a la observación de un periodo la observación precedente de la misma estación. Si el orden estacional es \\(m\\), entonces la diferencia estacional de \\(y_t\\) es\n\\[\\nabla_m y_t = y_t - y_{t-m}.\\]\nUna serie no estacionaria en media puede pasar a serlo tras diferenciarla estacionalmente. En general, cualquiera de las dos diferenciaciones vistas (regular o estacional) o ambas a la vez son alternativas para obtener la estacionariedad.\nAdemás, la diferenciación regular y/o estacional también permite alcanzar la ergodicidad.\nLa Figura 3 muestra un ejemplo de diferenciación regular y/o estacional. En el primer panel aparece la serie original Nacimientos \\(y_t\\); el segundo panel muestra la serie diferenciada regularmente \\(\\nabla y_t\\); en el tercer panel la serie diferenciada estacionalmente \\(\\nabla_{12} y_t\\); y en el cuarto panel muestra la serie diferenciada regular y estacionalmente \\(\\nabla\\nabla_{12} y_t\\).\n\n\n\n\n\n\n\n\nFigura 3: Serie Nacimientos y algunas transformaciones por diferenciación\n\n\n\n\n\n¿Qué transformación para nacimientos consideras que genera una serie estacionaria, tanto en media como en sentido amplio? Siempre hay un cierto grado de subjetividad en la elección de las diferencias que hay que aplicar a una serie. En la figura 4 podemos considerar que la diferenciación regular (panel 2) es suficiente para lograr la estacionariedad en media y en sentido amplio y terminar el proceso de diferenciación. Pero también podemos considerar que la serie es estacionaria en media pero no en sentido amplio, y optar por la doble diferenciación, regular y estacional (panel 4). Existen contrastes para ayudarnos en esta decisión (véase epígrafe 5), pero la conclusión puede depender el tipo de contraste elegido o de ciertos parámetros técnicos usados en los contrastes. Muchas veces la decisión final se realiza durante el proceso de modelización o simplemente se opta por la que mejores predicciones genere.\n\n\nDiferenciación con R\nR dispone de la función diff para diferenciar una serie:\n\ndiff(x, lag = k) calcula la diferencia de orden \\(k\\), \\(\\nabla_k y_t\\). Si \\(k = m\\) (orden estacional), calcula una diferencia estacional, \\(\\nabla_m y_t\\).\ndiff(x) calcula la diferencia regular o de orden \\(1\\), \\(\\nabla y_t\\) (el valor por defecto de lag es 1).\ndiff(x, difference = d) calcula \\(d\\) diferencias regulares, \\(\\nabla^d y_t\\).\n\nSi necesitas calcular una diferencia regular y otra estacional, \\(\\nabla\\nabla_m y_t\\), debes usar diff(diff(x, lag = m)). El orden de las diferenciaciones no cambia el resultado.\nAdemás, en la librería forecast están disponibles las funciones ndiffs y nsdiffs que estiman, respectivamente, el número de diferencias regulares y estacionales necesarias. En el primer caso se usa un contraste de raíces unitarias (epígrafe 5) y en el segundo un criterio ad-hoc.\n\nndiffs(nacimientos)\n\n[1] 1\n\nnsdiffs(nacimientos)\n\n[1] 1\n\n\nPara nacimientos estas dos funciones sugieren la doble diferenciación regular y estacional. La doble diferenciación es muy usual."
  },
  {
    "objectID": "03-03-Tema3.html#transformación-de-box-cox",
    "href": "03-03-Tema3.html#transformación-de-box-cox",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "3.3 Transformación de Box-Cox",
    "text": "3.3 Transformación de Box-Cox\nEn el tema 2 vimos el argumento lambda, que, al fijarlo a 0, indicaba que había que transformar la serie logarítmicamente. Veamos en detalle que hay detrás de este argumento.\n\nTransformación logarítmica\nSi la serie original no es estacionaria en sentido amplio porque los datos crecen con el nivel de la serie, es posible obtener la estacionariedad por medio de transformaciones simples.\nLa transformación logarítmica de una serie es una alternativa. La Figura 4 muestra la serie Nacimientos y su logaritmo. La variabilidad estacional con la transformación logarítmica (panel segundo) es menor que en la serie original (panel superior).\n\n\n\n\n\n\n\n\nFigura 4: Serie Nacimientos y algunas transformaciones Box-Cox\n\n\n\n\n\n\n\nBox-Cox\nLa transformación logarítmica es un caso concreto de otra más general, la transformación de Box-Cox: \\[\nz_t =\n    \\begin{cases}\n      (y_t^{\\lambda}-1)/\\lambda & \\,\\,\\,\\lambda \\neq 0 \\\\\n      \\log(y_t) & \\,\\,\\, \\lambda = 0.\n    \\end{cases}\n\\]\nSe puede demostrar que \\(\\lim_{\\lambda \\rightarrow 0} \\;\\; (y_t^{\\lambda}-1)/\\lambda = log(y_t)\\).\n\n\nTransformación de Box-Cox con R\nR dispone de una serie de funciones en el paquete forecast que nos facilitan el uso de la transformación de Box-Cox:\n\nBoxCox(y, lambda) realiza la transformación Box-Cox para un valor de \\(\\lambda\\) determinado.\nInvBoxCox(z, lambda) realiza la transformación inversa\nBoxCox.lambda(y) calcula el valor de \\(\\lambda\\) más adecuado. Usa help para saber más sobre los métodos de estimación de \\(\\lambda\\).\n\nPara la serie Nacimientos el valor óptimo de \\(\\lambda\\) es:\n\nBoxCox.lambda(nacimientos) \n\n[1] 0.1143325\n\n\nEn la Figura 4, el panel inferior muestra la transformación óptima de Box-Cox para Nacimientos usado el valor \\(\\lambda= 0.11\\). Por un lado, el valor obtenido es muy cercano a cero. Por otro lado, no hay grandes diferencias (al menos visuales) entre la transformación logarítmica y la transformación de Box-Cox con \\(\\lambda= 0.11\\). Finalmente, ¿qué interpretación tiene \\((nacimientos_t^{0.11}-1)/0.11\\)? En general, independientemente del resultado de la transformación óptima, se opta por la transformación logarítmica por ser más sencilla y, sobre todo, más interpretable.\nPor último, ten presente que si se estima y predice una serie transformada, luego hay que deshacer la transformación para obtener la predicción de la serie original.\nEl argumento lambda que hemos ya usado en la función ets hace referencia al parámetro \\(\\lambda\\) de la transformación Box-Cox. Siempre lo hemos fijado a cero, indicando la transformación logarítmica."
  },
  {
    "objectID": "03-03-Tema3.html#diferencia-logaritmo-y-tasa-de-variación",
    "href": "03-03-Tema3.html#diferencia-logaritmo-y-tasa-de-variación",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "3.4 Diferencia, Logaritmo y Tasa de variación",
    "text": "3.4 Diferencia, Logaritmo y Tasa de variación\nLa transformación \\(\\nabla y_t\\) se puede interpretar como variaciones en nivel. Sin embargo, cuando una serie tiene que ser diferenciada para conseguir su estacionariedad, vale la pena probar una transformación alternativa que también es interpretable: \\(\\nabla \\log(y_t)\\).\nPara la diferencia regular se tiene que, \\[\\nabla \\log(y_t) = \\log(y_t) - \\log(y_{t-1}) = \\log\\big(\\frac{y_t}{y_{t-1}} \\big) \\approx \\frac{y_t}{y_{t-1}} - 1 = \\frac{y_t - y_{t-1}}{y_{t-1}} =TV y_t.\\]\nEs decir, la diferencia regular del logaritmo es la Tasa de Variación de la serie, que tiene una clara interpretación como variación porcentual. Por ejemplo, para una serie anual la diferencia regular del logaritmo es la Tasa de Variación Anual de la serie \\((\\nabla \\log(y_t)=TVA y_t)\\); y para una serie mensual la diferencia regular del logaritmo es la Tasa de Variación Mensual de la serie \\((\\nabla \\log(y_t)=TVM y_t)\\).\nPara la diferencia estacional se tiene que, \\[\\nabla_m \\log(y_t) \\approx \\frac{y_t - y_{t-m}}{y_{t-m}} =TV_m y_t.\\]\nEs decir, para una serie mensual la diferencia estacional del logaritmo es la Tasa de Variación Anual de la serie \\((\\nabla_{12} \\log(y_t)=TVA y_t)\\); y para una serie diaria la diferencia estacional del logaritmo es la Tasa de Variación Semanal de la serie \\((\\nabla_{7} \\log(y_t)=TVS y_t)\\).\n\n\nCuando una serie tiene que ser diferenciada tanto regular como estacionalmente para conseguir su estacionariedad también vale la pena usar la transformación logarítmica para ganar en interpretabilidad. Es decir, frente a \\(\\nabla \\nabla_m y_t\\) es preferible usar \\(\\nabla \\nabla_m \\log(y_t)\\). Así, podemos escribir\n\\[\\nabla \\nabla_m \\log(y_t) = \\nabla TV_m y_t = TV_m y_t - TV_m y_{t-1},\\]\nque para una serie mensual se puede interpretar como una diferencia regular de tasas anuales \\(TVA y_t - TVA y_{t-1}\\). Es decir, como cambia de mes en mes la tasa de variación anual.\nLa Figura 5 muestra para la serie Nacimientos la serie original (panel superior), las tasas de variación mensual y anual (paneles dos y tres, respectivamente) y la doble diferencia regular y estacional del logaritmo de nacimientos (panel inferior). Parece que la tasa de variación mensual de los nacimientos y la doble diferencia son, de todas las transformaciones probadas en este epígrafe, las series más estacionarias en media y en sentido amplio.\n\n\n\n\n\n\n\n\nFigura 5: Diversas transformaciones de Nacimientos\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa diferenciación y el logaritmo\n\n\n\n\nSi se tiene una serie sin estacionalidad y no estacionaria, bastará diferenciarla regularmente una o, a lo sumo, dos veces para que sea estacionaria en media, en sentido amplio y ergódica.\nSi se tiene una serie con estacionalidad y no estacionaria, lo usual es tener que diferenciarla una vez (regular o estacionalmente) o aplicar ambas diferenciaciones para que sea estacionaria en media, en sentido amplio y ergódica.\nEn ambos casos, la transformación logarítmica se puede usar para ganar en interpretabilidad o para intentar mejorar las predicciones.\nLa transformación logarítmica no es necesaria para alcanzar la estacionariedad de la serie."
  },
  {
    "objectID": "03-03-Tema3.html#función-de-autocorrelación",
    "href": "03-03-Tema3.html#función-de-autocorrelación",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "4.1 Función de autocorrelación",
    "text": "4.1 Función de autocorrelación\nYa hemos indicado que bajo ciertas hipótesis los momentos primero y segundo de la serie caracterizan perfectamente el proceso estocástico. En el contexto de series temporales, si se da la estacionariedad en media, el primer momento de la serie es constante y, por tanto, no informativo. Son los segundos momentos de la serie (covarianzas) los que caracterizan el proceso estocástico.\nRecordemos que \\(\\gamma_k=Cov(y_t,y_{t-k})\\) es la autocovarianza de orden k. Por tanto \\(\\gamma_0\\) es la varianza de la serie \\(y_t\\). Sea \\(\\rho_k\\) la autocorrelación se orden \\(k\\). Se puede verificar que: \\[\\rho_k = cor(y_t, y_{t-k}) =\\frac{\\gamma_k}{\\gamma_0}.\\]\nPodemos interpretar \\(\\rho_1\\) como la información que se transmite de un periodo al siguiente periodo. En general \\(\\rho_k\\) mide la información que se transmite k periodos hacia adelante.\nLas autocorrelaciones caracterizan el proceso estocástico, y la función de autocorrelación o correlograma (FAC, o ACF en inglés) es el gráfico de \\(r_k\\) contra \\(k\\), donde \\(r_k\\) es la estimación de \\(\\rho_k\\) obtenida con las observaciones.\nLa Figura 6 muestra la FAC para la serie Nacimientos y algunas de sus transformaciones. Observa el uso del argumento lag, que en la función ggAcf indica el orden máximo de la autocorrelación mostrada en el gráfico. La primera columna muestra la FAC para Nacimientos y varias diferenciaciones, mientras que la segunda columna muestra la FAC para el logaritmo de Nacimientos y sus diferenciaciones. Se puede observar que:\n\nla FAC de una serie y su transformación logarítmica es muy similar.\nEn los paneles de la primera y tercera fila las autocorrelaciones decrecen muy lentamente. Este es un claro indicativo de que la serie analizada no es estacionaria ni ergódica.\nEn los paneles de la segunda fila las autocorrelaciones de orden estacional (12, 24,…) también decrecen lentamente, indicando que la serie analizada no es ergódica.\nSolo la doble diferenciación regular y estacional de la serie (original o su logaritmo) muestran un rápido descenso en los coeficiente de autocorrelación (paneles de la última fila), indicando que la serie transformada es estacionaria en media y ergódica.\n\n\nggAcf(nacimientos, lag = 48, ylim = c(-1 ,1))\nggAcf(log(nacimientos), lag = 48, ylim = c(-1 ,1))\nggAcf(diff(nacimientos), lag = 48, ylim = c(-1 ,1))\nggAcf(diff(log(nacimientos)), lag = 48, ylim = c(-1 ,1))\nggAcf(diff(nacimientos, lag = 12),lag = 48, ylim = c(-1 ,1))\nggAcf(diff(log(nacimientos), lag = 12), lag = 48, ylim = c(-1 ,1))\nggAcf(diff(diff(nacimientos, lag = 12)), lag = 48, ylim = c(-1 ,1))\nggAcf(diff(diff(log(nacimientos), lag = 12)), lag = 48, ylim = c(-1 ,1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Logaritmo de la serie\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Dif. regular de la serie\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(d) Dif. regular del logaritmo la serie\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Dif. estacional de la serie\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(f) Dif. estacional del logaritmo la serie\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) Dif. regular y estacional de la serie\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(h) Dif. regular y estacional del logaritmo la serie\n\n\n\n\n\n\n\nFigura 6: FAC para diversas transformaciones de Nacimientos\n\n\n\n\nLas bandas azules de la FAC muestran el intervalo de confianza al 95% (IC95). Si \\(\\rho_k = 0\\), la distribución del estimador \\(r_k\\) se distribuye aproximadamente como una normal de media \\(-1/T\\) y varianza \\(1/T\\). Las líneas punteadas de la FAC están dibujadas en las posiciones \\(\\frac{-1}{T} \\pm \\frac{1.96}{\\sqrt{T}}\\).\n\nSi un \\(r_k\\) cae fuera del IC95 hay evidencia para rechazar la hipótesis nula de que \\(\\rho_k = 0\\) a un nivel del 5%. Recordemos que incluso si todos los \\(\\rho_k\\) son cero, cabe esperar que un 5% de sus estimaciones \\(r_k\\) caigan fuera del IC95.\nLos \\(\\rho_k\\) no son independientes. Si uno cae fuera del IC95, es más probable que los valores vecinos caigan también fuera.\n\nSi queremos ver los valores numéricos de las autocorrelaciones debemos añadir a la función ggAfc el argumento plot = FALSE. Para la serie doblemente diferenciada vemos que la autocorrelación más elevada se da para el primer retardo (\\(r_{1}=-0.338\\)). El segundo valor más alto es \\(r_{12} = - 0.311\\).\n\nggAcf(diff(diff(nacimientos, lag = 12)), \n      lag = 24, \n      plot = FALSE)\n\n\nAutocorrelations of series 'diff(diff(nacimientos, lag = 12))', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.336 -0.041 -0.005 -0.115 -0.016  0.067 -0.055  0.055  0.102 -0.060 \n    11     12     13     14     15     16     17     18     19     20     21 \n 0.083 -0.310  0.056  0.053  0.059 -0.066  0.109 -0.031 -0.082  0.072 -0.058 \n    22     23     24 \n 0.002  0.154 -0.153"
  },
  {
    "objectID": "03-03-Tema3.html#función-de-autocorrelación-parcial",
    "href": "03-03-Tema3.html#función-de-autocorrelación-parcial",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "4.2 Función de autocorrelación parcial",
    "text": "4.2 Función de autocorrelación parcial\nLa autocorrelación parcial de orden k, \\(\\phi_k\\), mide la información que se transmite directamente \\(k\\) periodos adelante, eliminada la información que se transmite a través de los periodos intermedios.\n\n\\(\\phi_1\\) mide la información que de un periodo se trasmite directamente al siguiente periodo. Por tanto \\(\\phi_1=\\rho_1\\).\n\\(\\phi_2\\) mide la información que de un periodo se trasmite 2 periodos adelante, eliminando la información que se trasmite a través del periodo intermedio.\n\nLa función de autocorrelación parcial (FACP, o PACF en inglés) es el gráfico de \\(r_{k.1,2,k-1}\\) contra \\(k\\), donde \\(r_{k.1,2,k-1}\\) es la estimación de \\(\\phi_k\\) realizada con las observaciones.\nLa Figura 7 muestra la FACP para la serie Nacimientos y su transformación para que sea estacionaria. La FACP no permite conocer si una serie es estacionaria o ergódica.\n\nggPacf(nacimientos, lag = 36)\nggPacf(diff(diff(nacimientos, la = 12)), lag = 36)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Serie original\n\n\n\n\n\n\n\n\n\n\n\n(b) Serie diff. regular y estacionalmente\n\n\n\n\n\n\n\nFigura 7: FACP para Nacimientos\n\n\n\n\nTambién puedes obtener directamente los valores numéricos de la FACP:\n\nggPacf(diff(diff(nacimientos), lag = 12), lag = 24, plot = FALSE)\n\n\nPartial autocorrelations of series 'diff(diff(nacimientos), lag = 12)', by lag\n\n     1      2      3      4      5      6      7      8      9     10     11 \n-0.336 -0.174 -0.092 -0.184 -0.164 -0.048 -0.098 -0.032  0.103  0.041  0.134 \n    12     13     14     15     16     17     18     19     20     21     22 \n-0.261 -0.151 -0.072  0.024 -0.131 -0.007  0.026 -0.098  0.005  0.037  0.015 \n    23     24 \n 0.191 -0.161 \n\n\nLa función ggtsdisplay de la libraría forecaat muestra la serie temporal, su FAC y su FACP en un único gráfico (véase Figura 8).\n\nggtsdisplay(diff(diff(nacimientos), lag = 12), main = \"\")\n\n\n\n\n\n\n\nFigura 8: FAC y FACP para Nacimientos doblemente diferenciada"
  },
  {
    "objectID": "03-03-Tema3.html#tipos-de-tendencia",
    "href": "03-03-Tema3.html#tipos-de-tendencia",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "5.1 Tipos de tendencia",
    "text": "5.1 Tipos de tendencia\nLa Figura 9 muestra tres posibles tipos de PGD (el residuo \\(\\varepsilon_t\\) es siempre ruido blanco):\n\nPGD con tendencia estocástica, por ejemplo Paseo aleatorio puro,\n\n\\[y_t = y_{t-1} + \\varepsilon_t = y_0 + \\sum_{s=1}^t \\varepsilon_s.\\]\n\nPGD con tendencia determinista, por ejemplo Modelo lineal\n\n\\[y_t = \\alpha + \\mu t + \\varepsilon_t.\\]\n\nPGD con tendencia estocástica y determinista, por ejemplo Paseo aleatorio con deriva\n\n\\[y_t = \\mu + y_{t-1} + \\varepsilon_t = y_0 + \\mu t + \\sum_{s=1}^t \\varepsilon_s.\\]\n\n\n\n\n\n\n\n\nFigura 9: Ejemplos de procesos generadores\n\n\n\n\n\nEn los tres casos la serie no es estacionaria, pero es la presencia de tendencia estocástica lo que determina que el proceso tenga una raíz unitaria. Así, el paseo aleatorio puro y el paseo aleatorio con deriva presentan una raíz unitaria, pero los tres modelos tienen tendencia y deben ser diferenciados una vez para transformarlos en procesos estacionarios.\nContrastar la tendencia estocástica no es algo trivial, y la prueba es que existen múltiples contrastes para ello: Dickey-Fuller aumentado (ADF), Phillips-Perron (PP), Elliot-Rothemberg-Stock (ERS), Schmidt-Phillips (SP), Kwiatkowski-Phillips-Schmidt-Shin (KPSS), entre otros (véase Pfaff (2008)).\nEn muchas de las pruebas la hipótesis nula es que la serie tiene raíces unitarias (tendencia estocástica), cuando lo habitual es que la hipótesis nula sea la conservadora (en este caso que la serie no tenga tendencia estocástica). En este curso veremos la prueba de Kwiatkowski-Phillips-Schmidt-Shin que usa un contraste de Multiplicadores de Lagrange para contrastar la estacionariedad de una serie en torno a una tendencia determinista, frente a la hipótesis alternativa de existencia de una raíz unitaria. Es decir, la prueba KPSS tiene como hipótesis nula que la serie no tiene tendencia estocástica y por tanto es una prueba más conservadora que la mayoría de ellas. Sin embargo, en la prueba KPSS la ausencia de raíz unitaria no es una prueba de estacionariedad, sino de estacionariedad en torno a una tendencia determinista. Por ejemplo, el Modelo lineal (linea azul en la Figura 9) no tiene una raíz unitaria (no tiene tendencia estocástica) pero no es estacionario.\nTambién existen procesos con estacionalidad estocástica del tipo \\(y_t = y_{t-m} + \\varepsilon_t\\), donde m es el orden estacional. Contrastar la existencia de raíces unitarias estacionales es complicado y no lo veremos en este curso (véase Ghysels, Lee, and Noh (1994)).\nEn lo que queda de exposición asumiremos que la serie no presenta estacionalidad."
  },
  {
    "objectID": "03-03-Tema3.html#contraste-de-raíz-unitaria-kpss",
    "href": "03-03-Tema3.html#contraste-de-raíz-unitaria-kpss",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "5.2 Contraste de raíz unitaria KPSS",
    "text": "5.2 Contraste de raíz unitaria KPSS\n\nTeoría\nSea \\(y_t\\) la serie para la que queremos contrastar la existencia de raíces unitarias. Asumimos que podemos descomponer la serie en la suma de una tendencia determinista, un paseo aleatorio y una perturbación aleatoria estacionaria: \\[y_t = \\xi t + r_t + \\varepsilon_t.\\] Aquí \\(r_t\\) es el paseo aleatorio, \\[r_t = r_{t-1} + u_t,\\] donde \\(u_t\\) son i.i.d. \\((0,\\sigma_u^2)\\) y el valor inicial \\(r_0\\) se asume fijo y tiene el papel de un intercepto o constante del modelo.\nSi se cumple la hipótesis \\(H_0: \\sigma_u^2=0\\), entonces \\(u_t=0\\) y \\(r_t=r_0\\) para todo \\(t\\) y la componente estocástica desaparece, quedando solo la tendencia determinista. Como el parámetro de interés es una varianza, \\(H_1: \\sigma_u^2&gt;0\\), es decir, es un contraste de una cola por la derecha.\n\nSi \\(\\xi=0\\), entonces bajo la hipótesis nula \\(y_t = r_0 + \\varepsilon_t\\), es decir \\(y_t\\) no tiene tendencia estocástica ni determinista: es estacionaria en nivel alrededor de \\(r_0\\) (recuerda que por hipótesis \\(\\varepsilon_t\\) es estacionario).\nSi \\(\\xi \\neq 0\\), entonces bajo la hipótesis nula \\(y_t = \\xi t + r_0 + \\varepsilon_t\\), es decir \\(y_t\\) no tiene tendencia estocástica pero si determinista: es estacionaria alrededor de una tendencia determinista \\(\\xi t + r_0\\).\nEn ambos casos, si se rechaza la hipótesis nula es porque la serie tiene una raíz unitaria (tendencia estocástica) y habría que diferenciarla.\n\nNo hay que olvidar que esta prueba descansa bajo los supuestos de \\(\\varepsilon_t\\) es estacionario y \\(u_t\\) i.i.d. \\((0,\\sigma_u^2)\\).\n\n\nProcedimiento de contraste\nPaso 1: Se estima la ecuación \\(y_t = \\xi t + r_0 + \\varepsilon_t\\) para obtener los residuos estimados \\(\\widehat{\\varepsilon}_t\\).\nObserva que este vector de residuos se puede estimar fijando \\(\\xi = 0\\), es decir, regresando la serie \\(y_t\\) simplemente sobre una constante; o bajo es supuesto de que \\(\\xi \\neq 0\\), es decir, regresando la serie sobre una contante y una tendencia lineal.\nPaso 2: Se definen las sumas parciales \\(S_t = \\sum_{i=1}^t \\widehat{\\varepsilon}_i\\), con \\(t=1,2,\\ldots,T\\) y se calcula \\(\\widehat{\\sigma}^2_{\\varepsilon}\\).\nLa forma más sencilla de calcular \\(\\widehat{\\sigma}^2_{\\varepsilon}\\) es \\(\\widehat{\\sigma}^2_{\\varepsilon}= \\sum \\widehat{\\varepsilon}_t^2/T\\), pero una fórmula alternativa es \\(\\widehat{\\sigma}^2_{\\varepsilon}(l) = T^{-1}\\sum_{t = 1}^T \\widehat{\\varepsilon}_t^2 + 2T^{-1}\\sum_{s=1}^l \\Big[ \\Big( 1 - \\frac{s}{l+1} \\Big) \\sum_{t=s+1}^T \\widehat{\\varepsilon}_t \\widehat{\\varepsilon}_{t-s}  \\Big]\\), que dependen el parámetro \\(l\\).\nPaso 3: El estadístico de contraste es \\(LM = \\sum_{t=1}^T S_t^2/\\widehat{\\sigma}^2_{\\varepsilon}\\).\nPaso 4: Fijado un nivel de significatividad, el estadístico de contraste LM es comparado con el valor crítico. Si LM es mayor que el valor crítico, se rechaza la hipótesis nula, la serie tiene raíces unitarias (tendencia estocástica) y es, por tanto, no estacionaria. Si no se rechaza la hipótesis nula, entonces la serie es estacionaria en torno a un nivel (caso \\(\\xi = 0\\)) o a una tendencia determinista (caso \\(\\xi \\neq 0\\)).\nPuedes encontrar los detalles de este contraste en Kwiatkowski et al. (1992).\n\n\nContraste KPSS en R\nR proporciona dos implementaciones de esta prueba, ur.kpss en el paquete urca (que será la que usaremos) y kpss.test en el paquete tseries. En ambos casos el usuario debe decidir el valor adecuado del parámetro \\(l\\) para el cálculo de \\(\\widehat{\\sigma}^2_{\\varepsilon}(l)\\). Como los resultados pueden depender de este parámetro, es conveniente repetir el contraste para diferentes valores.\nAdemás, en forecast está disponible la función ndiffs que estima el número de diferencias necesarias para que una serie sea estacionaria. Por defecto la función ndiffs emplea el contraste KPSS, pero también están disponibles Dickey-Fuller y Phillips-Perron.\nLos principales argumentos de la función ur.kpss son:\n\ntype, puede ser type = \"mu\" para contrastar la presencia de tendencia estocástica en torno a un nivel (\\(\\xi = 0\\)); o type = \"tau\" para contrastar la presencia de tendencia estocástica en torno a una tendencia determinista (\\(\\xi \\neq 0\\)).\nlags, el valor del parámetro \\(l\\) en \\(\\widehat{\\sigma}^2_{\\varepsilon}(l)\\). Puede ser ‘’nil’’ cuando no hay corrección (\\(l=0\\)); ‘’short’’ usa el valor \\((4T/100)^{0.25}\\); y ‘’long’’ para \\((12T/100)^{0.25}\\).\nuse.lag fija un valor numérico para \\(l\\). (La función ndiffs fija este valor a \\(3\\sqrt{T/13}\\).)"
  },
  {
    "objectID": "03-03-Tema3.html#ejemplos",
    "href": "03-03-Tema3.html#ejemplos",
    "title": "Series Temporales: Procesos Estocásticos",
    "section": "5.3 Ejemplos",
    "text": "5.3 Ejemplos\n\nEjemplo con Residuos\nLa Figura 10 muestra que la serie Residuos no presenta tendencia determinista, pero veamos si tiene raíces unitarias (tendencia estocástica).\n\nresiduos &lt;- read.csv2(\"./series/Residuos.csv\", \n                      header = TRUE)\n\nresiduos &lt;- ts(residuos[, 2], \n               start = 1995, \n               frequency = 1)\n\n\nautoplot(residuos, \n         xlab = \"\", \n         ylab = \"kg per cápita\")\n\n\n\n\n\n\n\nFigura 10: Residuos recogidos\n\n\n\n\n\nEl contraste de raíces unitarias KPSS, bajo el supuesto de que no hay tendencia determinista, estima un estadístico de contraste igual a 0.692 y el valor crítico al 5% vale 0.463. Por tanto, se rechaza la hipótesis nula, es decir, la serie tiene tendencia estocástica. Hay que diferenciarla, al menos una vez, para que sea estacionaria.\n\nsummary(ur.kpss(residuos, \n                type = \"mu\", \n                lags = \"short\"))\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 2 lags. \n\nValue of test-statistic is: 0.6919 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nSi aplicamos el contraste para diferentes valores de \\(l\\) desde 1 hasta 5, se rechaza la hipótesis nula sólo para \\(l \\leq 3\\).\n\n\n    1     2     3     4     5 \n1.003 0.692 0.539 0.451 0.395 \n\n\nSi aceptamos que la serie Residuos no es estacionaria, debemos repetir el contraste para la serie diferenciada a fin de determinar si una diferencia es suficiente para alcanzar la estacionariedad. En general, una serie ya diferenciada no suele tener nunca tendencia determinista, por tanto se fija tau = \"mu\". Con el contraste KPSS no se rechaza la hipótesis nula, es decir, la serie Residuos diferenciada es estacionaria.\n\nsummary(ur.kpss(diff(residuos), \n                type = \"mu\", \n                lags = \"short\"))\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 2 lags. \n\nValue of test-statistic is: 0.267 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nOtra opción es usar la función ndiffs que aplica el contraste KPSS reiteradamente para identificar el número adecuado de diferenciaciones. Esta función permite fijar el nivel de significatividad, el tipo de contraste y el tipo de tendencia determinista.\n\nndiffs(residuos, alpha = 0.05, test = \"kpss\")\n\n[1] 1\n\n\n\n\nEjemplo con Nacimientos anuales\nLa serie nacimientos, desde el año 2000, tiene tendencia determinista y ya solo por ello debe ser diferenciada una vez. Veamos si su diferencia tiene raíces unitarias. Recuerda, estamos asumiendo que una serie ya diferencia no tiene tendencia determinista.\n\nnacimientosAnual&lt;-aggregate(nacimientos, \n                            FUN = sum)\n\nnacimientosAnual &lt;- window(nacimientosAnual, start = 2000)\n\nautoplot(nacimientosAnual)\n\n\n\n\n\n\n\n\nEl estadístico de contraste vale 0.525 y el valor crítico al 5% vale 0.463. Como el estadístico de contraste es mayor que el valor crítico, se rechaza la hipótesis nula. Es decir, la primera diferencia de la serie Nacimientos no es estacionaria y la serie tiene tendencia estocástica.\n\nsummary(ur.kpss(diff(nacimientosAnual), \n                type = \"mu\", \n                lags = \"short\"))\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 2 lags. \n\nValue of test-statistic is: 0.5254 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nSi aplicamos el contraste a la segunda diferencia, ya se acepta la hipótesis nula. La serie diferenciada dos veces es estacionaria.\n\nsummary(ur.kpss(diff(nacimientosAnual, differences = 2), \n                type = \"mu\", \n                lags = \"short\"))\n\n\n####################### \n# KPSS Unit Root Test # \n####################### \n\nTest is of type: mu with 2 lags. \n\nValue of test-statistic is: 0.1094 \n\nCritical value for a significance level of: \n                10pct  5pct 2.5pct  1pct\ncritical values 0.347 0.463  0.574 0.739\n\n\nCon la función ndiffs obtenemos el mismo resultado, que hay que diferenciar la serie dos veces para que sea estacionaria.\n\nndiffs(nacimientosAnual)\n\n[1] 2\n\n\nRecordemos que para una serie con estacionalidad también existe la función nsdiffs que identifica el número de diferenciaciones estacionales adecuado. En este caso se basa de una regla de decisión empírica, sin soporte teórico.\n\n\nPruebas alternativas para contraste de raíces unitarias\n\n\n\nPrueba\nFunción\nPackage\n\n\n\n\nKPSS\nur.kpss\nurca\n\n\nKPSS\nkpss.test\ntseries\n\n\nADF\nur.df\nurca\n\n\nADF\nadf.test\ntseries\n\n\nADF\nadftest\nfUnitRoots\n\n\nADF\nADF.test\nuroot\n\n\nPP\nur.pp\nurca\n\n\nPP\npp.test\ntseries\n\n\nERS\nur.ers\nurca\n\n\nSP\nur.sp\nurca"
  },
  {
    "objectID": "03-10-Ejemplo5.html",
    "href": "03-10-Ejemplo5.html",
    "title": "Defunciones por Enfermedades Cerebrovasculares",
    "section": "",
    "text": "1 Introducción\nConsideremos la serie temporal correspondiente al número de defunciones causadas por enfermedades cerebrovasculares, Esta serie está disponible en el Instituto Nacional de Estadística desde enero de 1980 hasta diciembre de 2024, un total de 45 años o 540 meses.\nEn el análisis descriptivo vimos que la descomposición revelaba la presencia de varios valores atípicos concentrados al inicio de la serie. Por este motivo, para su análisis por modelos Arima vamos a recortar la serie, que empezará el enero de 1990 (véase Figura 1).\nTambién hemos visto que para alcanzar la estacionariedad y verificar la hipótesis de ergodicidad es necesario diferenciar la serie tanto en la parte regular como estacional y decidimos usar la transformación logarítmica para linealizar la serie y ganar en interpretabilidad.\n\nDefEnfCer &lt;- read.csv2(\"./series/Enfermedades cerebrovasculares.csv\", \n                       header = TRUE)\n\nDefEnfCer &lt;- ts(DefEnfCer[,2], \n                start = 1980, \n                freq = 12)\n\nDefEnfCer &lt;- window(DefEnfCer, \n                    start = 1990)\n\nautoplot(DefEnfCer,\n         xlab = \"\",\n         ylab = \"\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 1: Defunciones causadas por enfermedades cerebrovasculares\n\n\n\n\n\n\n\n\n\n\n2 Identificación\nVamos a identificar los valores de \\(p\\), \\(q\\), \\(P\\) y \\(Q\\). Para ello, analizaremos la FAC y la FACP, y solicitaremos con auto.arima y seas una identificación automática.\n\nggtsdisplay(diff(diff(log(DefEnfCer), lag = 12)), lag = 48)\n\n\n\n\n\n\n\nFigura 2: Defunciones anuales por enfermedades cerebrovasculares, FAC y FACP de la serie transformada\n\n\n\n\n\nEn la parte regular, la FAC muestra que la primera autocorrelación está por encima del IC95 y en la FACP se observa decrecimiento. En la parte estacional, la FAC muestra una autocorrelación significativa en el orden 12 y la FACP muestra decrecimiento. Así, podemos identificar el proceso como \\(log(Def_t) \\sim ARIMA_{12}(0,1,1)(0,1,1)\\).\nVeamos ahora auto.arima, al que incluiremos variables ficticias para los cuatro valores atípicos ya identificados cuando aplicamos Alisado exponencial: febrero de 1999, mayo de 2001, junio de 2003 y febrero de 2012.\n\nfechas &lt;- format(seq(as.Date(\"1990-01-01\"), as.Date(\"2024-12-01\"), \"month\"), \"%Y-%m\")\n\nd0299 &lt;- 1*(fechas == \"1999-02\")\nd0501 &lt;- 1*(fechas == \"2001-05\")\nd0603 &lt;- 1*(fechas == \"2003-06\")\nd0212 &lt;- 1*(fechas == \"2012-02\")\n\nauto.arima(DefEnfCer, \n           d = 1, \n           D = 1,\n           lambda = 0,\n           xreg = cbind(d0299, d0501, d0603, d0212))\n\nSeries: DefEnfCer \nRegression with ARIMA(1,1,2)(1,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n         ar1      ma1     ma2     sar1     sma1   d0299   d0501   d0603   d0212\n      0.4167  -1.1042  0.1447  -0.1614  -0.8360  0.1833  0.1121  0.1399  0.2198\ns.e.  0.2174   0.2351  0.2160   0.0588   0.0398  0.0419  0.0413  0.0424  0.0416\n\nsigma^2 = 0.002102:  log likelihood = 671.35\nAIC=-1322.7   AICc=-1322.15   BIC=-1282.61\n\n\nLa función auto.arima identifica un complejo proceso \\(ARIMA_{12}(1,1,2)(1,1,1)\\). Además, varios coeficientes estimados no parecen ser significativos (ar1 y ma2) y si los eliminamos, tendríamos un modelo muy parecido al de las aerolíneas.\nPor otro lado, la función seas identifica un proceso \\(ARIMA_{12}(1,0,1)(1,1,1)\\). El resultado muestra la conveniencia de la transformación logarítmica y dos intervenciones en febrero de 1999 y 2012.\n\nsummary(seas(DefEnfCer))\n\n\nCall:\nseas(x = DefEnfCer)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \nConstant          -0.018590   0.000762 -24.396  &lt; 2e-16 ***\nAO1999.Feb         0.181007   0.042353   4.274 1.92e-05 ***\nAO2012.Feb         0.216484   0.042297   5.118 3.08e-07 ***\nAR-Nonseasonal-01  0.660300   0.090023   7.335 2.22e-13 ***\nAR-Seasonal-12    -0.141384   0.054692  -2.585 0.009735 ** \nMA-Nonseasonal-01  0.368515   0.109522   3.365 0.000766 ***\nMA-Seasonal-12     0.826659   0.032838  25.173  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (1 0 1)(1 1 1)  Obs.: 420  Transform: log\nAICc:  5115, BIC:  5146  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 18.24   Shapiro (normality): 0.9874 **\n\n\nVamos a descartar ambas identificaciones y considerar el modelo de las aerolíneas. El AICc para este modelo es solo algo superior al de los modelos sugeridos por auto.arima o seas. Es decir, \\(\\log(Def_t) \\sim ARIMA_{12}(0, 1, 1)(0, 1, 1) + AI\\) donde \\(AI\\) recoge las cuatro variables ficticias que afectan un único mes.\n\n\n\n\n\n3 Estimación\nLa siguiente salida muestra el modelo estimado y la Figura 3 permite analizar la presencia de más valores extremos.\n\nDefEnfCerArima1 &lt;- Arima(DefEnfCer, \n                         order = c(0, 1, 1),  \n                         seasonal = c(0, 1, 1),\n                         lambda = 0,\n                         cbind(d0299, d0501, d0603, d0212))\nDefEnfCerArima1\n\nSeries: DefEnfCer \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1   d0299   d0501   d0603   d0212\n      -0.8858  -0.8860  0.2074  0.1137  0.1565  0.2522\ns.e.   0.0500   0.0272  0.0450  0.0447  0.0455  0.0446\n\nsigma^2 = 0.002269:  log likelihood = 654.46\nAIC=-1294.93   AICc=-1294.65   BIC=-1266.86\n\n\n\nerror &lt;- residuals(DefEnfCerArima1)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1990, 2024, 2))\n\nfechas[abs(error) &gt; 3 * sderror]\n\n[1] \"2003-08\" \"2015-02\"\n\natipicos &lt;- tsoutliers(error)\nfechas[atipicos$index]\n\ncharacter(0)\n\n\n\n\n\n\n\n\nFigura 3: Error + Intervención\n\n\n\n\n\nSe observan dos valores claramente atípicos en agosto de 2003 y febrero de 2015. Además, hay otros candidatos a valor extremo, entre los que destacan enero de 2005 y 2015, y julio de 2022. Procederemos a incluirlos en el modelo.\n\nd0803 &lt;- 1*(fechas == \"2003-08\")\nd0105 &lt;- 1*(fechas == \"2005-01\")\nd0115 &lt;- 1*(fechas == \"2015-01\")\nd0215 &lt;- 1*(fechas == \"2015-02\")\nd0722 &lt;- 1*(fechas == \"2022-07\")\n\nDefEnfCerArima2 &lt;- Arima(DefEnfCer, \n                         order = c(0, 1, 1),  \n                         seasonal = c(0, 1, 1),\n                         lambda = 0,\n                         xreg = cbind(d0299, d0501, d0603, d0803, d0105, \n                                      d0212, d0115, d0215, d0722))\nDefEnfCerArima2\n\nSeries: DefEnfCer \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1   d0299   d0501   d0603   d0803   d0105   d0212   d0115\n      -0.9160  -0.8572  0.2088  0.1107  0.1679  0.1804  0.1638  0.2599  0.1304\ns.e.   0.0276   0.0279  0.0412  0.0411  0.0413  0.0413  0.0411  0.0411  0.0412\n       d0215   d0722\n      0.1798  0.1515\ns.e.  0.0412  0.0419\n\nsigma^2 = 0.001953:  log likelihood = 688.58\nAIC=-1353.15   AICc=-1352.36   BIC=-1305.05\n\n\nSi comparamos los coeficientes con sus errores estándar, observamos que, con seguridad, las variables de intervención incluidas son significativas.\nEl análisis gráfico del residuo indica que aún hay candidatos a valores atípicos (véase Figura 4). Sin embargo, vamos a dar por concluido este proceso.\n\nerror &lt;- residuals(DefEnfCerArima2)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 2, 3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1990, 2024, 2)) \n\nfechas[abs(error) &gt; 3 * sderror]\n\n[1] \"1991-12\" \"2005-02\"\n\n\n\n\n\n\n\n\nFigura 4: Error + Intervención\n\n\n\n\n\nTres de los valores atípicos corresponden al mes de febrero y sus coeficientes estimados toman valores parecidos. También hay dos meses de enero atípicos con similar efecto. Vamos a asumir que la causa que hay detrás del valor anómalo en los meses de febrero es la misma, posiblemente un invierno más frío de lo usual. Lo mismo asumiremos para los valores atípicos en enero. Esto nos permite agrupar variables de intervención y simplificar el modelo.\n\nd01aa &lt;- d0105 + d0115\nd02aa &lt;- d0299 + d0212 + d0215\n\nDefEnfCerArima3 &lt;- Arima(DefEnfCer, \n                         order = c(0, 1, 1),  \n                         seasonal = c(0, 1, 1),\n                         lambda = 0,\n                         xreg = cbind(d01aa, d02aa, \n                                      d0501, d0603, d0803, d0722))\nDefEnfCerArima3\n\nSeries: DefEnfCer \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1   d01aa   d02aa   d0501   d0603   d0803   d0722\n      -0.9169  -0.8571  0.1480  0.2163  0.1107  0.1679  0.1804  0.1516\ns.e.   0.0275   0.0278  0.0295  0.0244  0.0412  0.0414  0.0415  0.0421\n\nsigma^2 = 0.00195:  log likelihood = 687.4\nAIC=-1356.8   AICc=-1356.35   BIC=-1320.72\n\n\n\n\n\n\n\n4 Validación\n\nCoeficientes signnificativos\nVemos que todos los coeficientes del modelo son significativos.\n\ncoeftest(DefEnfCerArima3)\n\n\nz test of coefficients:\n\n       Estimate Std. Error  z value  Pr(&gt;|z|)    \nma1   -0.916939   0.027495 -33.3490 &lt; 2.2e-16 ***\nsma1  -0.857072   0.027782 -30.8504 &lt; 2.2e-16 ***\nd01aa  0.147988   0.029493   5.0178 5.227e-07 ***\nd02aa  0.216322   0.024417   8.8596 &lt; 2.2e-16 ***\nd0501  0.110729   0.041224   2.6861 0.0072301 ** \nd0603  0.167948   0.041425   4.0543 5.029e-05 ***\nd0803  0.180439   0.041462   4.3519 1.349e-05 ***\nd0722  0.151571   0.042050   3.6045 0.0003127 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCalidad de ajuste\nAnalizando los criterios de bondad de ajuste se tiene que el error medio (ME), igual a -7.07, defunciones es prácticamente cero por lo que no parece que haya sesgo en las predicciones; en media nos equivocamos en 127 defunciones (RMSE); y el error porcentual medio es 3.3%, bajo. Sin embargo, los intervalos de las predicciones no están correctamente calculados.\n\naccuracy(DefEnfCerArima3)\n\n\n\n                ME   RMSE   MAE   MPE MAPE MASE ACF1\nTraining set -7.07 126.87 92.58 -0.33 3.33 0.58 0.15\n\n\n\n\n\n\nIncorrelación, Homocedasticidad y Normalidad\nVeamos ahora si el residuo es ruido blanco.\n\nerror &lt;- residuals(DefEnfCerArima3)\nBox.test(error, lag = 2,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 10.71, df = 2, p-value = 0.004724\n\nBox.test(error, lag = 24,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 28.824, df = 24, p-value = 0.2268\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 1.5913, df = 2, p-value = 0.4513\n\nBox.test(error^2, lag = 24, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 26.768, df = 24, p-value = 0.3154\n\njarque.bera.test(error) \n\n\n    Jarque Bera Test\n\ndata:  error\nX-squared = 0.89314, df = 2, p-value = 0.6398\n\n\nLos primeros valores de autocorrelación indican que el error no es incorrelado. Esto puede deberse a un análisis de la intervención incompleto o a una identificación incorrecta. Por otro lado, el error es homocedástico y segue una distribución normal.\n\n\n\n\n\n\n5 Ecuación del modelo identificado\nEl modelo estimado es el de las aerolíneas con intervención: \\[(1-L)(1-L^{12})log(Def_t) =(1+\\theta_1 L)(1 + \\theta_{12}L^{12})\\varepsilon_t +AI.\\] Como la interpretación de la parte estructural del modelo es ya conocida, vamos a centrar la interpretación en la intervención:\n\nEn los dos meses de enero atípicos, posiblemente por ser más frios, la defunciones fueron 14.8% mayores que las observadas en otros meses de enero.\nDe la misma forma, en los tres meses de febrero atípicos, la defunciones fueron un 21.6% mayores que las observadas en otros meses de febrero.\nEn mayo de 2001 hubo un aumento en las defunciones del 11.1% respecto a lo esperado; en junio de 2003 del 16.8%; en agosto de 2003 del 18.0%; y en julio de 2022 del 15.2%.\n\n\n\n\n\n\n6 Predicción de las defunciones por enfermedad cerebrovascular\nUna vez dado por válido el modelo, podemos pasar a realizar predicciones para los próximos años (véase Figura 5). Como la variables de intervención no son efectos calendario, sus valores previstos serán cero. Esto implica, por ejemplo, que estamos asumiendo que la temperatura en los eneros y febreros de los próximos años no será inusualmente baja.\n\npDefEnfCerArima3 &lt;- forecast(DefEnfCerArima3, \n                             h = 60,\n                             xreg = cbind(rep(0, 60), rep(0, 60), rep(0 ,60), \n                                          rep(0 ,60), rep(0, 60), rep(0, 60)), \n                             level = 95)\n\nautoplot(pDefEnfCerArima3, \n         xlab = \"\",\n         ylab = \"Defunciones\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1990, 2030, 4)) \n\n\n\n\n\n\n\nFigura 5: Defunciones (1990-2024) y predicción (2025-2029)\n\n\n\n\n\nA partir de 2026 se espera que el número anual de defunciones por enfermedad cerebrovascular caiga por debajo de los 22,000 casos.\n\naggregate(pDefEnfCerArima3$mean, FUN = sum)\n\nTime Series:\nStart = 2025 \nEnd = 2029 \nFrequency = 1 \n[1] 22318.33 21831.48 21355.26 20889.42 20433.75\n\n\n\n\n\n\n\n7 Comparación con Alisado Exponencial\nEl método de Alisado exponencial, aplicado sobre el logaritmo de las defunciones identifica un proceso (A,A,A) con \\(\\alpha=0.091\\), \\(\\beta = 0\\) y \\(\\gamma = 0.002\\). La raíz del error cuadrático medio (RMSE) es de 150 defunciones y el error porcentual (MAPE) del 3.7%. Estos valores son superiores a los obtenidos con el modelo Arima (127 defunciones y 3.3%, respectivamente).\n\nsummary(ets(DefEnfCer, lambda = 0))\n\nETS(A,A,A) \n\nCall:\nets(y = DefEnfCer, lambda = 0)\n\n  Box-Cox transformation: lambda= 0 \n\n  Smoothing parameters:\n    alpha = 0.091 \n    beta  = 1e-04 \n    gamma = 0.0016 \n\n  Initial states:\n    l = 8.1772 \n    b = -0.0015 \n    s = 0.1196 -0.0045 -0.0452 -0.1461 -0.0811 -0.0507\n           -0.102 -0.0387 -0.0276 0.0912 0.0777 0.2073\n\n  sigma:  0.0508\n\n      AIC      AICc       BIC \n 52.04196  53.56435 120.72629 \n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE      ACF1\nTraining set 5.313471 149.5784 103.8437 -0.1411164 3.662229 0.6556417 0.2482545\n\n\nVamos a determinar si la aplicación de modelos Arima mejora la calidad de las predicciones lo suficiente como para justificar su uso –frente a los métodos de Alisado, mucho más sencillos. Para ello, aplicaremos la metodología de origen de predicción móvil para estimar la capacidad predictiva del modelo Arima y compararla con el modelo de Alisado.\n\nk &lt;- 120                   \nh &lt;- 12                    \nT &lt;- length(DefEnfCer)     \ns &lt;- T - k - h               \n\nmapeArima &lt;- matrix(NA, s + 1, h)\nmapeAlisado &lt;- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set &lt;- subset(DefEnfCer, start = i + 1, end = i + k)\n  test.set &lt;-  subset(DefEnfCer, start = i + k + 1, end = i + k + h) \n  \n  fit &lt;- Arima(train.set, \n               order = c(0, 1, 1),\n               seasonal = c(0, 1, 1), \n               lambda = 0)\n  \n  fcast &lt;- forecast(fit, h = h)\n  mapeArima[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  \n  fit &lt;- ets(train.set, \n             lambda = 0, \n             model = \"AAA\", \n             damped = FALSE)\n  \n  fcast&lt;-forecast(fit, h = h)\n  mapeAlisado[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n}\n\nCalculamos los errores medianos para eliminar el impacto de los meses atípicos sobre la precisión en las predicciones.\n\nerrorArima &lt;- apply(mapeArima, MARGIN = 2, FUN = median)\nerrorArima\n\n [1] 2.973894 3.260939 3.443113 3.520236 3.501047 3.727242 3.781061 3.774683\n [9] 3.610695 3.278842 3.318228 3.292135\n\nerrorAlisado &lt;- apply(mapeAlisado, MARGIN = 2, FUN = median)\nerrorAlisado\n\n [1] 3.202547 3.326366 3.443478 3.623668 3.623171 3.907784 4.084028 3.889111\n [9] 3.776533 3.214803 3.343446 3.499976\n\n\n\ndatos &lt;- data.frame(\n  factor = c(rep(c(\"Arima\", \"Alisado\"), each = 12)),\n  x = c(1:12,1:12),\n  y = c(errorArima, errorAlisado)\n)\n\nggplot(datos, aes(x = x, y = y,  colour= factor)) + \n  geom_line() +\n  labs(title = \"\", x = \"Horizonte temporal de predicción\", y = \"%\") +\n  scale_x_continuous(breaks= 1:12) +\n  labs(colour = \"Métodos\") + \n  theme(legend.position=c(0.1,0.8)) \n\n\n\n\n\n\n\nFigura 6: Error de predicción (MAPE) según horizonte temporal\n\n\n\n\n\nLa Figura 6 indica que Arima es, en general, algo superior a Alisado en calidad de predicciones. Sin embargo, la diferencia máxima en la precisión de las previsiones es de 0.3 p.p. y la media es de 0.12 p.p. Por tanto, la mejora en la precisión no parece justificar la complejidad de los modelos Arima frente a la sencillez de los modelos de Alisado.\nLa ventaja de Arima reside en su capacidad para estimar el efecto de inviernos más crudos sobre las defunciones por enfermedad cerebrovascular y mejorar las predicciones en estos meses."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-tendencia",
    "href": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-tendencia",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.1 Análisis de la tendencia",
    "text": "2.1 Análisis de la tendencia\nSi anualizamos la serie podemos, por un lado, identificar mejor en que años se producen los cambios en la tendencia y, por otro lado, poner cifras al volumen de pasajeros en transporte urbano.\n\nPasajerosAnual &lt;- aggregate(Pasajeros, FUN = sum)\n\nautoplot(PasajerosAnual, colour = \"darkblue\",\n         xlab = \"\",\n         ylab = \"Millones de pasajeros\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\n\n\n\n\n\n\nFigura 2: Pasajeros en transporte urbano (datos anuales)\n\n\n\n\n\nLa Figura 2 muestra el volumen anual de pasajeros en transporte urbano. El crecimiento continuado, posiblemente iniciado antes de 1996 y que permitió superar los 3000 millones de pasajeros en 2007, se ve interrumpido con el inicio de la Gran Recesión. La caída en el número de pasajeros se interrumpe en 2014, año que marca la salida de esta crisis en España y el inicio de la recuperación en el serie. En 2019 se superaron los 3100 millones de pasajeros.\nEl incremento en el uso del transporte urbano observado antes y después de la crisis puede tener distintas causas: un aumento en el número de trabajadores y estudiantes, un uso más intensivo del transporte urbano en detrimento de otros medios de transporte, una reorganización de los servicios de transporte urbano que haya mejorado la conectividad dentro de los municipios, o un aumento en el número de líneas de autobús, tranvía o metro en determinadas ciudades.\nLa causa del repunte aislado observado el año 2011, en plena crisis, fue una ligera recuperación de la economía que tuvo lugar a finales de 2010 y principios de 2011."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-estacionalidad",
    "href": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-estacionalidad",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.2 Análisis de la estacionalidad",
    "text": "2.2 Análisis de la estacionalidad\nLa principal causa de la estacionalidad observada en la serie es la estructura vacacional de la sociedad, especialmente caracterizada por las vacaciones de verano (julio a septiembre) y las vacaciones de Semana Santa (en marzo y/o abril, según el año). Además, debido a que el transporte urbano se usa principalmente para ir a trabajar o al centro de educación, también influye el número de días laborables del mes. Por ejemplo, en 2017 el mes de junio tuvo 22 días laborables, mientras que en 2019 tuvo 20 días laborables. Esta diferencia de dos días tendrá un efecto sobre el volumen de pasajeros.\nEl número de días laborables de un mes viene marcado por los fines de semana del mes y por las festividades nacionales. Es cierto que el sábado se trabaja en diversos sectores (comercio, ocio, distribución) pero la caída en el número de trabajadores respecto de los días entre semana (lunes a viernes) es muy notable. También es cierto que, además de las festividades nacionales, hay muchas festividades autonómicas o municipales que podrían afectar al volumen de pasajeros en transporte urbano. Por ejemplo, las festividades regionales en comunidades como Madrid o Cataluña pueden tener un efecto significativo sobre la serie Pasajeros. Sin embargo, las festividades no nacionales no se van a tener en cuenta.\nPor tanto, para realizar un análisis detallado de la estacionalidad, es necesario crear una serie con el número de días laborables de cada mes. Además, esta serie se usará más adelante para modelizar y predecir la serie Pasajeros.\n\nDías laborables de cada mes\nLa librería timeData proporciona una serie de funciones que permiten definir un calendario de festividades, identificar los fines de semana y, a partir de aquí, crear la serie de días laborables (véase código más abajo).\n\nCon timeCalendar se definen las festividades nacionales que vamos a considerar: Año nuevo (1 de enero), Reyes (6 de enero), Viernes Santo (fecha variable), Día del Trabajo (1 de mayo), Día de la Asunción (15 de agosto), Día de la Hispanidad (12 de octubre), Día de Todos los Santos (1 de noviembre), la Constitución (6 de diciembre) la Inmaculada Concepción (8 de diciembre) y Navidad (25 de diciembre).\nPor claridad, cada festivo se ha definido de forma independiente para después crear una variable con todas las festividades (FestivosNacionales).\nEl rango para todos los cálculos va desde 1996 hasta 2024, que incluye el rango de la serie Pasajeros más cinco años de predicción.\nLa función utilizada Easter de la librería timeDate difiere de la función easter de forecast.\n\n\nAnoNuevo &lt;- timeCalendar(d = 1, m = 1, y = 1996:2024)\nReyes &lt;- timeCalendar(d = 6, m = 1, y = 1996:2024)\nViernesSanto &lt;- Easter(1996:2024, shift = -2)\nDiaTrabajo &lt;- timeCalendar(d = 1, m = 5, y = 1996:2024)\nAsuncion &lt;- timeCalendar(d = 15, m = 8, y = 1996:2024)\nHispanidad &lt;- timeCalendar(d = 12, m = 10, y = 1996:2024)\nTodoSantos &lt;- timeCalendar(d = 1, m = 11, y = 1996:2024)\nConstitucion &lt;- timeCalendar(d = 6, m = 12, y = 1996:2024)\nInmaculada &lt;- timeCalendar(d = 8, m = 12, y = 1996:2024)\nNavidad &lt;- timeCalendar(d = 25, m = 12, y = 1996:2024)\n\nFestivosNacionales &lt;- c(AnoNuevo, Reyes, ViernesSanto,\n                        DiaTrabajo, Asuncion,  Hispanidad, TodoSantos, \n                        Constitucion, Inmaculada, Navidad)\n\n\nA continuación, con timeSequence se crea una serie diaria desde el 1 de enero de 1996 hasta el 31 de diciembre de 2024.\n\n\nfechaDiaria &lt;- timeSequence(from = \"1996-01-01\", to = \"2024-12-31\")\n\n\nLas siguientes líneas eliminan de la serie diaria los festivos y los fines de semana, (función isBizday), para después dar a esta nueva serie el formato año-mes eliminando el día. De esta forma, la serie de días laborales tendrá el mismo identificador para todos los días del mismo mes.\n\n\nbiz &lt;- fechaDiaria[isBizday(fechaDiaria, holidays = FestivosNacionales)]\nbizdays &lt;- format(biz, format = \"%Y-%m\")\n\n\nAhora se crea una tabla que, por la naturaleza de la serie de días laborales, tendrá para cada año-mes el numero de días laborables. Por último fechamos la tabla, que es nuestra serie de días laborables y mostramos algunos datos.\n\n\nDiasLaborables &lt;- table(bizdays)\n\n\nLas últimas líneas de código dividen la serie en el periodo muestral y el de predicción.\n\n\nDiasLaborables &lt;- ts(DiasLaborables, start = 1996, frequency = 12)\n\nsubset(DiasLaborables, start = 289) #Mostramos solo los 5 últimos años\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2020  21  20  22  21  20  22  23  21  22  21  21  21\n2021  19  20  23  21  21  22  22  22  22  20  21  21\n2022  20  20  23  20  22  22  21  22  22  20  21  20\n2023  21  20  23  19  22  22  21  22  21  21  21  18\n2024  22  21  20  22  22  20  23  21  21  23  20  20\n\npDiasLaborables &lt;- subset(DiasLaborables, start = length(DiasLaborables) - 59)\nDiasLaborables &lt;- subset(DiasLaborables, end = length(DiasLaborables) - 60)\n\nEs conveniente insistir que la identificación de las festividades nacionales dista de ser perfecta por varios motivos:\n\nalgunos festivos nacionales si caen en domingo, se pasan a lunes (por ejemplo Reyes y la Inmaculada de 2019), aspecto que no se ha tenido en cuenta.\nalgunos festivos nacionales pueden ser sustituidos por otros días por las Comunidades Autónomas, por ejemplo Reyes o Jueves Santo.\n\n\n\nAnálisis gráfico de la estacionalidad\nVeamos ahora una descriptiva detallada de la estacionalidad de la serie Pasajeros, haciendo especial hincapié en el efecto de las vacaciones (verano y Semana Santa) y el número de días laborables. Para ello, mostraremos gráficamente las subseries definidas por el mes tanto para Pasajeros como para Pasajeros por día laborable, esta segunda resultado de dividir Pasajeros por DiasLaborables.\n\nPasajerosDL &lt;- Pasajeros/DiasLaborables\n\nggsubseriesplot(Pasajeros) +\n  labs(x = \"\", y = \"Millones de pasajeros\", title = \"\")\n\nggsubseriesplot(PasajerosDL) +\n  labs(x = \"\", y = \"Millones de pasajero por día\", title = \"\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Pasajeros\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Pasajeros por día laborable\n\n\n\n\n\n\n\nFigura 3: Estacionalidad de la serie Pasajeros\n\n\n\n\nLa Figura 3 muestra para cada mes la serie de pasajeros (total o por día laborable) y el valor medio (línea azul horizontal). En ambos paneles se identifica perfectamente el efecto de los periodos vacacionales sobre el transporte urbano de pasajeros. En las vacaciones de verano se observa una fuerte caída en el número de pasajeros, especialmente en agosto y, en menor medida, en julio y septiembre. Por otro lado, las subseries de marzo y abril muestran mucha más irregularidad que las de otros meses debido a que el volumen de pasajeros depende de cómo haya caído la Semana Santa. Cada año el mes en que caiga la Semana Santa presentará un volumen de pasajeros inferior al esperado. Diciembre, para ser un mes de 31 días, presenta también un reducido número de pasajeros debido a las vacaciones navideñas.\nLa Figura 3 a) muestra el efecto estacional total debido al número de días del mes y de días laborales. Por ejemplo, en febrero, el mes con menos días y por tanto con menos días laborales, en media se transportan menos pasajeros, comparado con enero o marzo. Octubre, un mes con 31 días, muestra un volumen medio de pasajeros mayor que noviembre de 30 días.\nEn la Figura 3 b) se ha eliminado el efecto de los días laborables al trabajar con la serie de pasajeros transportados por día laborable. Si la comparamos con la Figura 3 a), destaca que las diferencias entre las medias (líneas azules) se han reducido: prácticamente no hay diferencias entre los meses de enero a junio, o entre los meses de octubre a diciembre.\nCabría pensar que al excluir de la serie de días laborables la Semana Santa, en la Figura 3 b) las subseries de marzo y abril serían tan suaves como las observadas para otro meses, pero no es así. Claramente la simple exclusión de los festivos nacionales de Semana Santa no es suficiente para recoger bien su efecto sobre el transporte urbano. La razón hay que buscarla en las vacaciones escolares de este periodo, que en algunas comunidades autónomas tiene lugar durante la propia semana de Semana Santa, mientras que en otras comunidades tiene lugar en la semana posterior. De esta forma, el efecto sobre el transporte urbano de Semana Santa no es homogéneo en el territorio nacional y resulta difícil incluirlo en el análisis de la serie Pasajeros.\n\n\nDescomposición y estimación de la estacionalidad\nPodemos obtener la componente estacional para poder valorarla numéricamente y ver que efecto tiene el número de días laborables. Previamente, debemos determinar el esquema, aditivo o multiplicativo, de la serie.\nLa gráfica media-varianza (Figura 4) refuerza la impresión que se obtenía de la gráfica de la serie (Figura 1), que el esquema es aditivo.\n\nMediaAnual &lt;- aggregate(Pasajeros, FUN = sum)\nDesviacionAnual &lt;- aggregate(Pasajeros, FUN = sd)\n\nggplot() +\n  geom_point(aes(x = MediaAnual, y = DesviacionAnual), size = 2) +\n  labs(x = \"Total pasajeros por año\", \n       y = \"Desviación típica intraanual de pasajerosa\", \n       title = \"\")\n\n\n\n\n\n\n\nFigura 4: Identificación del tipo de esquema\n\n\n\n\n\nAhora vamos a proceder a descomponer tanto la serie original como la serie de pasajeros por día laboral. Dado que la serie presenta un esquema aditivo, usaremos el método de descomposición por regresiones locales ponderadas, asumiendo una componente estacional constante y considerando la presencia de posibles valores extremos.\nLa Tabla 1 pone en cifras el efecto estacional sobre los Pasajeros (primera columna): en agosto la caída en el número de pasajeros, respecto de la media anual, se cifra en 72 millones de pasajeros. En julio, septiembre y en menor medida abril también el uso del transporte urbano es inferior a la media anual, en el caso de los dos primeros meses por las vacaciones de verano y en abril debido a ser el mes en que con más frecuencia cae la Semana Santa. Por otro lado, destaca el elevado número de pasajeros en los meses de marzo, mayo y octubre, por tener 31 días.\n\nPasajerosStl &lt;- stl(Pasajeros, \n                    s.window = \"periodic\", \n                    robust = TRUE)\n\nPasajerosDLStl &lt;- stl(PasajerosDL, \n                    s.window = \"periodic\", \n                    robust = TRUE)\n\ndatos &lt;- cbind(seasonal(PasajerosStl)[1:12], seasonal(PasajerosDLStl)[1:12])\ncolnames(datos) &lt;- c(\"Pasajeros\", \"Pasajeros por día laborable\")\nrownames(datos) &lt;- meses\n\nkable(datos, \n      digits = 2)\n\n\n\nTabla 1: Efecto estacional\n\n\n\n\n\n\n\nPasajeros\nPasajeros por día laborable\n\n\n\n\nEne\n4.70\n0.41\n\n\nFeb\n0.34\n0.53\n\n\nMar\n24.02\n0.53\n\n\nAbr\n-2.17\n0.39\n\n\nMay\n19.67\n0.69\n\n\nJun\n6.45\n0.18\n\n\nJul\n-15.30\n-1.21\n\n\nAgo\n-71.82\n-3.52\n\n\nSep\n-8.31\n-0.53\n\n\nOct\n26.79\n1.02\n\n\nNov\n14.70\n0.94\n\n\nDic\n0.93\n0.55\n\n\n\n\n\n\n\n\nTras la corrección por el número de días laborales, el efecto estacional es más suave (véase la segunda columna en la Tabla 1). Ahora, los meses de febrero y marzo tienen un efecto similar, al igual que octubre y noviembre. También se observa que las diferencias entre marzo y abril se han reducido.\n\n\nAnálisis de la intervención\nYa hemos realizado una descripción detallada de las principales componentes de la serie, tendencia y estacionalidad. Ahora vamos a analizar, aunque sea de forma gráfica, el error y tener ya una primera impresión sobre la relevancia de la componente de intervención en Pasajeros.\n\nerror &lt;- remainder(PasajerosStl)\nsderror &lt;- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(3, 2, -2, -3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"),\n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\nfechasMes &lt;- format(seq(from = as.Date(\"1996-01-01\"), \n                        to = as.Date(\"2019-12-01\"), \n                        \"month\"), \n                    \"%Y-%m\")\n\nfechasMes[abs(error) &gt; 3 * sderror]\n\n[1] \"1997-03\" \"1997-04\" \"2002-03\" \"2002-04\" \"2005-04\" \"2008-03\" \"2008-04\"\n[8] \"2013-03\" \"2013-04\"\n\n\n\n\n\n\n\n\nFigura 5: Error + Intervención. Descomposición de Pasajeros\n\n\n\n\n\nLa Figura 5 muestra el error de la descomposición y los intervalos de confianza al 95% (líneas verdes) y el 99.7% (líneas rojas). Se aprecian claramente múltiples valores extremos (superan las tres desviaciones típicas) en forma de compensación (dos errores extremos consecutivos de signo opuesto) que corresponden a los meses de marzo y abril de 1997, 2002, 2008 y 2013, y otro valor extremo en abril de 2005. También en marzo y abril de 2016 se identifican dos valores atípicos. Todos los valores identificados corresponden a los meses de marzo y abril, y en todos los casos el error negativo tiene lugar en marzo y el positivo en abril. Si miramos un calendario, veremos que tienen lugar en los años en que la Semana Santa cayó en marzo.\nLa prueba de Tukey solo identifica como meses atípicos marzo de 2002 y de 2008.\n\natipicos &lt;- tsoutliers(error)\nfechasMes[atipicos$index]\n\n[1] \"2002-03\" \"2008-03\"\n\n\nSi repetimos este análisis para la serie de Pasajeros por día laborable, los resultados son bien diferentes (véase Figura 6). Ahora solo se detectan dos valores extremos en diciembre de 2000 y 2006. También destaca el error de diciembre de 2017. Los errores en diciembre se dan cuando Navidad cae en lunes, de forma que la caída en el transporte urbano debida a la nochebuena coincide con la de cualquier domingo. Así, estos meses de diciembre presentan más transporte urbano que los meses de diciembre donde la nochebuena cae entre semana.\n\nerror &lt;- remainder(PasajerosDLStl)\nsderror &lt;- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(3, 2, -2, -3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"),\n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\nfechasMes[abs(error) &gt; 3 * sderror]\n\n[1] \"2000-12\" \"2006-12\"\n\n\n\n\n\n\n\n\nFigura 6: Error + Intervención. Descomposición de Pasajeros por día laborable"
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#conclusión",
    "href": "03-11-Ejemplo-Pasajeros.html#conclusión",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "2.3 Conclusión",
    "text": "2.3 Conclusión\nLa serie de pasajeros en transporte urbano muestra una tendencia creciente solo interrumpida entre 2008 y 2014 debido a la Gran Recesión.\nLos principales determinantes de la estacionalidad de la serie Pasajeros son los grandes periodos vacacionales en España (Semana Santa y verano) y el número de días laborales del mes. Así, en la serie Pasajeros corregida por días laborables la componente estacional se ha suavizado y prácticamente queda determinada por las vacaciones.\nLa intervención tiene lugar en los meses de marzo y abril debido al carácter móvil de la Semana Santa, y en diciembre cuando el día de Navidad cae en lunes, de forma que la caída de pasajeros de nochebuena se solapa con la de cualquier domingo."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-serie-pasajeros",
    "href": "03-11-Ejemplo-Pasajeros.html#análisis-de-la-serie-pasajeros",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.1 Análisis de la serie Pasajeros",
    "text": "4.1 Análisis de la serie Pasajeros\n\nEstimación e interpretación\nEl modelo óptimo, estimado con la función ets sin imponer ninguna restricción, es ETS(M,Ad,A): pendiente aditiva con amortiguamiento, estacionalidad aditiva y residuo multiplicativo. \\[y_{t+1} = (l_t + \\phi b_t + s_{t+1-m}) \\cdot (1 + \\varepsilon_{t+1}).\\]\n\nPasajerosEts &lt;- ets(Pasajeros)\nsummary(PasajerosEts) \n\nETS(M,Ad,A) \n\nCall:\nets(y = Pasajeros)\n\n  Smoothing parameters:\n    alpha = 0.1374 \n    beta  = 0.017 \n    gamma = 1e-04 \n    phi   = 0.9438 \n\n  Initial states:\n    l = 203.1964 \n    b = -0.4508 \n    s = 2.0753 14.0423 27.0229 -8.7953 -72.369 -15.0625\n           6.6935 19.5129 4.2863 18.0168 0.373 4.2039\n\n  sigma:  0.0333\n\n     AIC     AICc      BIC \n2823.866 2826.408 2889.799 \n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.5929553 7.492818 5.654414 0.1765693 2.447442 0.6844865\n                   ACF1\nTraining set -0.1434247\n\n\nEl valor de \\(\\phi=\\) 0.94 indica que la inclusión de amortiguamiento en el modelo mejora sensiblemente su ajuste a los datos. Por otro lado, \\(\\gamma\\) es técnicamente cero, indicando que el efecto estacional se mantiene constante en el tiempo. Sin embargo, el valor de \\(\\beta\\), reducido pero no nulo, indica que la pendiente cambia en el tiempo de forma muy lenta.\nLa calidad del ajuste es bastante buena, con un error porcentual del 2.4% o un error de 7.5 millones de pasajeros (RMSE). La aplicación del método de Alisado supone una reducción de un punto en el error porcentual respecto del método ingenuo, o una reducción del RMSE de 3.5 millones de pasajeros. Es decir, el modelo de Alisado exponencial supone una mejora en la calidad del ajuste del 32% respecto del método ingenuo con estacionalidad visto en el epígrafe previo (MASE). El indicador ACF1 revela que las fórmulas usadas para el intervalo de confianza no son válidas.\nEl efecto estacional, que recordemos se mantiene constante en el tiempo, es prácticamente idéntico al estimado en la descriptiva y viene determinado por los periodos vacacionales y el número de días laborables. Véase la Figura 8.\nEn verano (julio a septiembre) el uso del transporte urbano es inferior a la media anual, destacando agosto con un descenso de 72 millones de pasajeros. Por el contrario, octubre destaca por ser el mes con mayor incremento en el volumen de pasajeros (27 millones) respecto de la media anual.\n\nPasajerosEtsEst &lt;- PasajerosEts$states[nrow(PasajerosEts$states), 14:3]\nnames(PasajerosEtsEst) &lt;- meses\n\nround(PasajerosEtsEst, 2)\n\n   Ene    Feb    Mar    Abr    May    Jun    Jul    Ago    Sep    Oct    Nov \n  4.21   0.37  18.02   4.29  19.51   6.69 -15.06 -72.37  -8.79  27.02  14.04 \n   Dic \n  2.07 \n\nggplot() +\n  geom_line(aes(x = 1:12, y = PasajerosEtsEst), colour = \"darkblue\") + \n  geom_hline(yintercept = 0, colour = \"black\", lty = 2) +\n  labs(title = \"\", x = \"\", y = \"Efecto estacional\") +\n  scale_x_continuous(breaks= 1:12, \n                     labels = meses)\n\n\n\n\n\n\n\nFigura 8: Componente estacional estimada con Alisado exponencial\n\n\n\n\n\n\n\nPredicción\nPodemos ahora pedir los valores de predicción para los próximos cinco años. No mostramos los resultados numéricos, pero si gráficos (Figura 9). Las predicciones muestran una tendencia creciente amortiguada y, por tanto, no tan acusada como la observada en los años precedentes.\n\nPasajerosEtsPre &lt;- forecast(PasajerosEts, \n                            h = 60)\n\nautoplot(PasajerosEtsPre,\n         xlab = \"\",\n         ylab = \"Millones de pasajeros\",\n         main = \"\") \n\n\n\n\n\n\n\nFigura 9: Pasajeros (1996-2019) y predicción (2020-2024). Método de Alisado exponencial\n\n\n\n\n\nEn el año 2020 se esperan 3153 millones de pasajeros, un 1.6% más que en 2019.2\n\n\nAnálisis del error\nEl residuo del modelo (Figura 10) muestra varios valores que pueden ser considerados como atípicos y que se dan siempre en los meses de marzo y abril para los años donde la Semana Santa recayó en marzo.\n\nerror &lt;- residuals(PasajerosEts)\nsderror &lt;- sd(error)\n\nautoplot(error,\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\",\n         colour = \"darkblue\") +\n  geom_hline(yintercept = c(-3, -2, 2 ,3)*sderror, \n             colour = c(\"red\", \"green\", \"green\", \"red\"), lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\nfechasMes[abs(error) &gt; 3 * sderror]\n\n[1] \"2008-03\"\n\n\n\n\n\n\n\n\nFigura 10: Error + Intervención. Método de Alisado exponencial"
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#otras-alternativas-de-análisis",
    "href": "03-11-Ejemplo-Pasajeros.html#otras-alternativas-de-análisis",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.2 Otras alternativas de análisis",
    "text": "4.2 Otras alternativas de análisis\nEn la descriptiva se ha visto que la serie de pasajeros por día laborable tiene un comportamiento más suave (la componente estacional era más plana) y presentaba un menor número de valores atípicos que la serie original Pasajeros. Cabe esperar, por tanto, que esta serie presente un mejor ajuste con el método de Alisado exponencial y ofrezca mejores predicciones.\nPor otro lado, siempre vale la pena analizar la transformación logarítmica de la serie y ver si ofrece mejores resultados que la serie original. La transformación logarítmica es especialmente eficaz para series no lineales, así que para Pasajeros posiblemente no suponga ninguna mejora.\nLas transformaciones indicadas en los dos párrafos precedentes son solo dos de las posibles. También se pude analizar la serie de pasajeros por día del mes o la transformación óptima de Box-Cox. La idea es no quedarse con lo inmediato –la serie tal cual nos la han ofrecido–, sino probar otras alternativas. Por ejemplo, la serie de Pasajeros es el agregado del número de pasajeros que viajan en transporte urbano según el tipo de transporte (autobús, metro, tranvía…). Se podría proceder a analizar cada serie por separado (pasajeros en autobús, pasajeros en metro, etc.), para luego agregar los resultados y ver si este enfoque da mejores resultados que el análisis directo de la serie agregada Pasajeros.\nEn este epígrafe se analizarán tres de las transformaciones indicadas: la transformación logarítmica, los pasajeros por día laborable y los pasajeros por día del mes. El objetivo es ver si es posible mejorar la calidad de las predicciones obtenidas para Pasajeros. Se usará como criterio de bondad el error de las predicciones extramuestrales según el horizonte temporal, obtenido con el procedimiento origen de predicción móvil. Asumiremos que son necesarios 12 años para obtener una buena estimación del modelo y el horizonte temporal se fijará en 12 meses (\\(k = 144, h = 12\\)). Previamente, hay que crear la serie Pasajeros por día del mes, e identificar el mejor modelo para las series transformadas. Además, para evitar el efecto de meses atípicos sobre el cálculo de la precisión de las previsiones, usaremos como medida de calidad la mediana del error porcentual absoluto.\n\nPasajerosDM &lt;- Pasajeros/monthdays(Pasajeros)\nets(Pasajeros, lambda = 0)$method\n\n[1] \"ETS(A,Ad,A)\"\n\nets(PasajerosDL)$method\n\n[1] \"ETS(M,Ad,A)\"\n\nets(PasajerosDM)$method\n\n[1] \"ETS(A,Ad,A)\"\n\n\n\n\n\nk &lt;- 144                 \nh &lt;- 12                  \nTT &lt;- length(Pasajeros)  \ns &lt;- TT - k - h          \n\nmapeAlisadoPas &lt;- matrix(NA, s + 1, h)\nmapeAlisadoLogPas &lt;- matrix(NA, s + 1, h)\nmapeAlisadoPasDL &lt;- matrix(NA, s + 1, h)\nmapeAlisadoPasDM &lt;- matrix(NA, s + 1, h)\n\nfor (i in 0:s) {\n  train.set &lt;- subset(Pasajeros, start = i + 1, end = i + k)\n  test.set &lt;-  subset(Pasajeros, start = i + k + 1, end = i + k + h)\n  \n  trainDL.set &lt;- subset(PasajerosDL, start = i + 1, end = i + k)\n  testDL.set &lt;-  subset(PasajerosDL, start = i + k + 1, end = i + k + h)\n  \n  trainDM.set &lt;- subset(PasajerosDM, start = i + 1, end = i + k)\n  testDM.set &lt;-  subset(PasajerosDM, start = i + k + 1, end = i + k + h)\n  \n  fit &lt;- ets(train.set, model = \"MAA\", damped = TRUE)\n  fcast &lt;- forecast(fit, h = h)\n  mapeAlisadoPas[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  \n  fit &lt;- ets(train.set, model = \"AAA\", damped = TRUE, lambda = 0)\n  fcast &lt;- forecast(fit, h = h)\n  mapeAlisadoLogPas[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  \n  fit &lt;- ets(trainDL.set, model = \"MAA\", damped = TRUE)\n  fcast &lt;- forecast(fit, h = h)\n  mapeAlisadoPasDL[i + 1,] &lt;- 100*abs(testDL.set - fcast$mean)/testDL.set\n  \n  fit &lt;- ets(trainDM.set, model = \"AAA\", damped = TRUE)\n  fcast &lt;- forecast(fit, h = h)\n  mapeAlisadoPasDM[i + 1,] &lt;- 100*abs(testDM.set - fcast$mean)/testDM.set\n}\n\nerrorAlisadoPas &lt;- apply(mapeAlisadoPas, MARGIN = 2, FUN = median)\nerrorAlisadoLogPas &lt;- apply(mapeAlisadoLogPas, MARGIN = 2, FUN = median)\nerrorAlisadoPasDL &lt;- apply(mapeAlisadoPasDL, MARGIN = 2, FUN = median)\nerrorAlisadoPasDM &lt;- apply(mapeAlisadoPasDM, MARGIN = 2, FUN = median)\n\ndatos &lt;- data.frame(\n  factor = c(rep(c(\"Pasajeros\", \"Pasajeros por día laborable\", \n                   \"Pasajeros por día del mes\", \"Pasajeros (log)\"), each = 12)),\n  x = rep(1:12, 4),\n  y = c(errorAlisadoPas, errorAlisadoPasDL, errorAlisadoPasDM, errorAlisadoLogPas)\n)\n\n\nggplot(datos, aes(x = x, y = y,  colour= factor)) + \n  geom_line() +\n  labs(x = \"Horizonte temporal de predicción\", y = \"%\", title = \"\") +\n  scale_x_continuous(breaks= 1:12) +\n  scale_y_continuous(breaks= seq(2, 4, .2)) +\n  labs(colour = \"Métodos\") + \n  theme(legend.position=c(0.15, 0.7))\n\n\n\n\n\n\n\nFigura 11: Error de predicción (MedAPE) según horizonte temporal y enfoque. Método de Alisado exponencial\n\n\n\n\n\nAntes de pasar al análisis de los resultados, indicar que en las predicciones sobre el logaritmo no se ha pedido corrección por sesgo, y que al trabajar con errores porcentuales no es necesario pasar la predicción de pasajeros por día (laborable o del mes) a predicción de pasajeros.\nLa Figura 11 muestra los errores de predicción según el horizonte temporal para las cuatro aproximaciones. En todos los casos el error aumenta con el horizonte temporal de predicción, de forma que las predicciones a doce meses vista tienen un error un punto porcentual superior a las predicciones a un mes vista.\nPor otro lado, para horizontes temporales de hasta 9 meses vista las predicciones realizadas sobre el logaritmo de la serie original Pasajeros son de las más precisas, seguidas de las predicciones realizadas con Pasajeros por día del mes. Para predicciones de 10 a 12 meses, en general las mejores predicciones se obtienen con Pasajeros. En contra de lo esperado, las predicciones a partir de la serie de pasajeros por día laborable son de las que mayor error porcentual presentan."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#conclusión-1",
    "href": "03-11-Ejemplo-Pasajeros.html#conclusión-1",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "4.3 Conclusión",
    "text": "4.3 Conclusión\nLos modelos de Alisado exponencial resultan excelentes para predecir la serie Pasajeros. El error de ajuste, del 2.4%, es un punto inferior al error obtenido con el método ingenuo con estacionalidad. Además, en las predicciones extramuestrales a 12 meses vista el error porcentual sigue manteniéndose bajo, no superando el 4%.\nLa transformación logarítmica de la serie Pasajeros y la serie Pasajeros por día del mes han dado mejores predicciones por Alisado exponencial que el análisis directo de la serie."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#transformación-de-la-serie-pasajeros",
    "href": "03-11-Ejemplo-Pasajeros.html#transformación-de-la-serie-pasajeros",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.1 Transformación de la serie Pasajeros",
    "text": "5.1 Transformación de la serie Pasajeros\nPor un lado, el análisis por Alisado exponencial ha puesto de relieve el carácter lineal de Pasajeros y la efectividad que podría tener usar la transformación logarítmica para mejorar la calidad de las predicciones. Por otro lado, tras un análisis preliminar por modelos Arima se puede ver que la transformación logarítmica mejora la identificación del modelo y facilitaba su interpretación. Así, optamos por aplicar la transformación logarítmica a Pasajeros.\nLas FAC del logaritmo de la serie y algunas de sus diferenciaciones (Figura 12) indican que es necesaria la doble diferenciación regular y estacional para alcanzar las hipótesis de estacionariedad y ergodicidad: \\(\\log(Pasajeros) \\sim I(1)I_{12}(1)\\). Los resultados ofrecidos por las funciones ndiffs y nsdiffs apoyan esta conclusión.\n\nggAcf(log(Pasajeros), lag = 48, ylim = c(-1, 1), \n      xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(Pasajeros)), lag = 48, ylim = c(-1, 1), \n      xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(log(Pasajeros), lag = 12), lag = 48, ylim = c(-1, 1), \n      xlab = \"\", ylab = \"\", main = \"\")\nggAcf(diff(diff(log(Pasajeros), lag=12)), lag = 48, ylim = c(-1, 1), \n      xlab = \"\", ylab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Log serie\n\n\n\n\n\n\n\n\n\n\n\n(b) Dif. regular log serie\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Dif. estacional log serie\n\n\n\n\n\n\n\n\n\n\n\n(d) Dif. regular y estacional log serie\n\n\n\n\n\n\n\nFigura 12: FAC para Pasajeros\n\n\n\n\n\nndiffs(log(Pasajeros))\n\n[1] 1\n\nnsdiffs(log(Pasajeros))\n\n[1] 1\n\n\nLa Figura 13 muestra la serie original \\(y_t\\) y la serie transformada \\(\\nabla \\nabla_{12} \\log(y_t)\\). En la serie transformada destacan las compensaciones asociadas a la intervención de Semana Santa.\n\nseries &lt;- cbind(\"Original\" = Pasajeros,\n                \"Dif reg. y est. de log\" = diff(diff(log(Pasajeros), lag = 12)))\n\nautoplot(series, facets = TRUE,\n         xlab = \"\",\n         ylab = \"\",\n         main = \"\")\n\n\n\n\n\n\n\nFigura 13: Serie original de Pasajeros y su transformación"
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#identificación-de-la-serie-pasajeros",
    "href": "03-11-Ejemplo-Pasajeros.html#identificación-de-la-serie-pasajeros",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.2 Identificación de la serie Pasajeros",
    "text": "5.2 Identificación de la serie Pasajeros\nVamos a identificar los valores de \\(p\\), \\(q\\), \\(P\\) y \\(Q\\) del proceso Arima. Para ello, solicitaremos con auto.arima y seas una identificación automática, en el primer caso incluyendo todos los efectos calendario ya identificados.\n\nIdentificación automática con auto.arima\nPara ayudar a la función auto.arima en el proceso de identificación vamos a definir previamente todas las variables de intervención que en el desarrollo del análisis de la serie hemos ido identificando: días del mes, días laborables del mes, meses de diciembre con el día de Navidad en lunes y Semana Santa:\n\nLa variable Días laborables del mes ya ha sido definida previamente como DiasLaborables.\nLa variable Días del mes se puede definir directamente con la función monthdays de la librería forecast. En lugar de días del mes, consideraremos la variable días no laborables del mes, resultante de restar a los días del mes los días laborables.\nLos meses de diciembre en que el día de Navidad cae en lunes requiere un poco más de trabajo. La idea general es generar un rango de fechas diarias que cubra todo el periodo de análisis (variable fechas), identificar los lunes de Navidad (variable dicotómica lunesNavidad), eliminar el identificador del día del rango de fechas con la función format y, por último, con tapply sumar para cada mes-año los lunes de Navidad, que lógicamente solo tendrán lugar algunos meses de diciembre y una sola vez. En los objetos definidos con la función as.POSIXlt los meses van de 0 a 11 (enero a diciembre) y los días de la semana de 0 a 6 (domingo a sábado).\nLa creación de las variables de intervención que estiman el efecto de la Semana Santa es aún más complejo. El efecto del viernes de Semana Santa ya queda recogido en la variable DiasNoLaborables. Lo que vamos a hacer ahora es crear una variable que permita estimar el efecto de las vacaciones escolares (y de muchos padres y madres) de lunes a Jueves Santo en aquellas comunidades donde así es; y otra variable para estimar el efecto de las vacaciones escolares que tienen lugar la semana posterior al Domingo de Resurrección, de lunes a viernes tras Semana Santa. Estas nuevas variables (DiasPreSanta y DiasPascua) para marzo y abril valdrán la proporción de días vacacionales que recaen en el correspondiente mes, y valdrán cero para los demás meses del año.\n\nDías no laborables\nGeneramos la variables para DiasNoLaborables y se muestra su valor para los últimos 5 años.\n\nDiasNoLaborables &lt;- monthdays(DiasLaborables) - DiasLaborables\npDiasNoLaborables &lt;- monthdays(pDiasLaborables) - pDiasLaborables\ntail(DiasNoLaborables, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015  11   8   9   9  11   8   8  10   8  10   9  10\n2016  12   8   9   9   9   8  10   9   8  11   9  11\n2017  10   8   8  11   9   8  10   9   9  10   9  13\n2018   9   8  10   9   9   9   9   9  10   9   9  12\n2019   9   8  10   9   9  10   8  10   9   8  10  11\n\n\nNavidad cae en lunes\nGeneramos la variable que identifica los mes de diciembre en los que la Navidad cayó en lunes.\n\nfechas &lt;- as.POSIXlt(seq(from = as.Date(\"1996-1-1\"), \n                         to = as.Date(\"2024-12-31\"), \n                         by = \"day\"))\nLunesNavidad &lt;- 1*(fechas$wday == 1 & fechas$mon == 11 & fechas$mday == 25)\nfechas &lt;- format(fechas, format = \"%Y-%m\")\nLunesNavidad &lt;- tapply(LunesNavidad, fechas, sum)\nLunesNavidad &lt;- ts(LunesNavidad, start = 1996, frequency = 12)\npLunesNavidad &lt;- subset(LunesNavidad, start = length(LunesNavidad) - 59)\nLunesNavidad &lt;- subset(LunesNavidad, end = length(LunesNavidad) - 60)\n\nLunesNavidad[LunesNavidad == 1]\n\n2000-12 2006-12 2017-12 \n      1       1       1 \n\n\nSemana Santa\nSe generan las variables DiasPreSanta y DiasPascua y se muestra su valor para los últimos 5 años.\n\nLunSanto &lt;- Easter(1996:2024, shift = -6)\nMarSanto &lt;- Easter(1996:2024, shift = -5)\nMieSanto &lt;- Easter(1996:2024, shift = -4)\nJueSanto &lt;- Easter(1996:2024, shift = -3)\n\nPreSanta &lt;- c(LunSanto, MarSanto, MieSanto, JueSanto)\nbiz &lt;- fechaDiaria[isBizday(fechaDiaria, holidays = PreSanta, wday = 0:6)]\nbizdays &lt;- format(biz, format = \"%Y-%m\")\n\nDiasPreSanta &lt;- table(bizdays)\nDiasPreSanta &lt;- ts(DiasPreSanta, start = 1996, frequency = 12)\nDiasPreSanta &lt;- (monthdays(DiasPreSanta) - DiasPreSanta)/4\n\npDiasPreSanta &lt;- subset(DiasPreSanta, start = length(DiasPreSanta) - 59)\nDiasPreSanta &lt;- subset(DiasPreSanta, end = length(DiasPreSanta) - 60)\n\nLunPascua &lt;- Easter(1996:2024, shift = 1)\nMarPascua &lt;- Easter(1996:2024, shift = 2)\nMiePascua &lt;- Easter(1996:2024, shift = 3)\nJuePascua &lt;- Easter(1996:2024, shift = 4)\nViePascua &lt;- Easter(1996:2024, shift = 5)\n\nPascua &lt;- c(LunPascua, MarPascua, MiePascua, JuePascua, ViePascua)\nbiz &lt;- fechaDiaria[isBizday(fechaDiaria, holidays = Pascua, wday = 0:6)]\nbizdays &lt;- format(biz, format = \"%Y-%m\")\n\nDiasPascua &lt;- table(bizdays)\nDiasPascua &lt;- ts(DiasPascua, start = 1996, frequency = 12)\nDiasPascua &lt;- (monthdays(DiasPascua) - DiasPascua)/5\n\npDiasPascua &lt;- subset(DiasPascua, start = length(DiasPascua) - 59)\nDiasPascua &lt;- subset(DiasPascua, end = length(DiasPascua) - 60)\n\ntail(DiasPreSanta, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2016 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2017 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2018 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2019 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n\ntail(DiasPascua, n = 60)\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n2015 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2016 0.0 0.0 0.8 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2017 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2018 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n2019 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n\n\nAhora tenemos todos los elementos para pedir la identificación automática con auto.arima.\n\nauto.arima(Pasajeros, \n           lambda = 0,\n           d = 1, \n           D = 1,\n           xreg = cbind(DiasLaborables, DiasNoLaborables, \n                        LunesNavidad, DiasPreSanta, DiasPascua))\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5775  -0.4194          0.0352            0.0159         0.028\ns.e.   0.0496   0.0531          0.0057            0.0057         0.008\n      DiasPreSanta  DiasPascua\n           -0.0577     -0.0248\ns.e.        0.0042      0.0070\n\nsigma^2 = 0.0003214:  log likelihood = 718.25\nAIC=-1420.51   AICc=-1419.97   BIC=-1391.58\n\n\nLa identificación automática muestra el proceso de las aerolíneas. Además, parece que todas las variables de intervención son significativas.\n\n\nIdentificación automática con seas\nLa función seas de seasonal incluye automáticamente durante la identificación las variables de intervención necesarias.\n\nsummary(seas(Pasajeros, transform.function = \"log\"))\n\n\nCall:\nseas(x = Pasajeros, transform.function = \"log\")\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \nWeekday            0.0053245  0.0002834  18.787  &lt; 2e-16 ***\nEaster[1]         -0.0863100  0.0034777 -24.818  &lt; 2e-16 ***\nAR-Nonseasonal-01 -0.9566747  0.2791436  -3.427  0.00061 ***\nAR-Nonseasonal-02 -0.5939742  0.1383640  -4.293 1.76e-05 ***\nAR-Nonseasonal-03 -0.2879049  0.0881848  -3.265  0.00110 ** \nMA-Nonseasonal-01 -0.4666487  0.2885044  -1.617  0.10578    \nMA-Seasonal-12     0.3907450  0.0555134   7.039 1.94e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (3 1 1)(0 1 1)  Obs.: 288  Transform: log\nAICc:  1635, BIC:  1663  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 52.54 *** Shapiro (normality): 0.9846 **\n\n\nEn este caso el proceso identificado en la parte regular es más complejo que el obtenido con auto.arima, ARIMA(3,1,1)(0,1,1). Además, se han incluido variables de intervención asociadas a la Semana Santa y días laborables. Conjuntamente estas variables de intervención recogen los mismos efectos considerados por nosotros.\nConcluimos que el modelo de partida para Pasajeros será \\(\\log(Pasajeros) \\sim ARIMA_{12}(0, 1, 1)(0, 1, 1) + AI\\)."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#estimación-del-modelo-e-identificación-de-otras-componentes-de-intervención",
    "href": "03-11-Ejemplo-Pasajeros.html#estimación-del-modelo-e-identificación-de-otras-componentes-de-intervención",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.3 Estimación del modelo e identificación de otras componentes de intervención",
    "text": "5.3 Estimación del modelo e identificación de otras componentes de intervención\nVamos a estimar el modelo identificado y a analizar la presencia de otros valores atípicos en el residuo.\n\nPasajerosAri &lt;- Arima(Pasajeros, \n                      lambda = 0,\n                      order = c(0, 1, 1),  \n                      seasonal = c(0, 1, 1),\n                      xreg = cbind(DiasLaborables, DiasNoLaborables, \n                                   LunesNavidad, DiasPreSanta, DiasPascua))\nPasajerosAri\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5775  -0.4194          0.0352            0.0159         0.028\ns.e.   0.0496   0.0531          0.0057            0.0057         0.008\n      DiasPreSanta  DiasPascua\n           -0.0577     -0.0248\ns.e.        0.0042      0.0070\n\nsigma^2 = 0.0003214:  log likelihood = 718.25\nAIC=-1420.51   AICc=-1419.97   BIC=-1391.58\n\n\n\nerror &lt;- residuals(PasajerosAri)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 0, 2, 3)*sderror, \n             colour = c(\"red\", \"blue\", \"black\", \"blue\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\nfechasMes[abs(error) &gt; 3 * sderror]\n\n[1] \"2005-08\"\n\n\n\n\n\n\n\n\nFigura 14: Error + Intervención. Modelo Arima\n\n\n\n\n\nEn la Figura 14 identificamos claramente agosto de 2005 como atípico, con un error que supera las cinco desviaciones típicas.\nTras incluir la correspondiente variable artificial en el modelo y estimarlo, identificamos otros valores extremos, por superar las 3 desviaciones típicas, en agosto de 2006, abril de 2002 y marzo de 2010. Procedemos a incluir las variables artificiales en el modelo y repetir el análisis. En esta ocasión ya no identificamos más valores atípicos (véase Figura 15).\n\nd0402 &lt;- 1*(fechasMes == \"2002-04\")\nd0805 &lt;- 1*(fechasMes == \"2005-08\")\nd0806 &lt;- 1*(fechasMes == \"2006-08\")\nd0310 &lt;- 1*(fechasMes == \"2010-03\")\n\nPasajerosAri &lt;- Arima(Pasajeros,\n                      lambda = 0,\n                      order = c(0, 1, 1),  \n                      seasonal =  c(0, 1, 1),\n                      xreg = cbind(DiasLaborables, DiasNoLaborables, \n                                   LunesNavidad, DiasPreSanta, DiasPascua,\n                                   d0402, d0805, d0806, d0310))\nPasajerosAri\n\nSeries: Pasajeros \nRegression with ARIMA(0,1,1)(0,1,1)[12] errors \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1  DiasLaborables  DiasNoLaborables  LunesNavidad\n      -0.5276  -0.3752          0.0343            0.0158        0.0259\ns.e.   0.0539   0.0591          0.0051            0.0052        0.0072\n      DiasPreSanta  DiasPascua   d0402   d0805   d0806   d0310\n           -0.0579     -0.0253  0.0333  0.0644  0.0284  0.0377\ns.e.        0.0042      0.0067  0.0130  0.0126  0.0126  0.0126\n\nsigma^2 = 0.0002842:  log likelihood = 737.61\nAIC=-1451.23   AICc=-1450.03   BIC=-1407.82\n\n\n\nerror &lt;- residuals(PasajerosAri)\nsderror &lt;- sd(error)\n\nautoplot(error, series=\"Error\",\n         colour = \"black\",\n         xlab = \"\",\n         ylab = \"Error\",\n         main = \"\") +\n  geom_hline(yintercept = c(-3, -2, 0, 2, 3)*sderror, \n             colour = c(\"red\", \"blue\", \"black\", \"blue\", \"red\"), \n             lty = 2) + \n  scale_x_continuous(breaks= seq(1996, 2020, 2))\n\nfechasMes[abs(error) &gt; 3 * sderror]\n\ncharacter(0)\n\n\n\n\n\n\n\n\nFigura 15: Error + Intervención. Modelo Arima\n\n\n\n\n\nAntes de finalizar el proceso de identificación vamos a confirmar la significatividad de todos los parámetros estimados.\n\ncoeftest(PasajerosAri)\n\n\nz test of coefficients:\n\n                   Estimate Std. Error  z value  Pr(&gt;|z|)    \nma1              -0.5275620  0.0538773  -9.7919 &lt; 2.2e-16 ***\nsma1             -0.3751743  0.0591070  -6.3474 2.190e-10 ***\nDiasLaborables    0.0342656  0.0051246   6.6865 2.285e-11 ***\nDiasNoLaborables  0.0157855  0.0051814   3.0466 0.0023146 ** \nLunesNavidad      0.0258983  0.0072072   3.5934 0.0003264 ***\nDiasPreSanta     -0.0579159  0.0041802 -13.8548 &lt; 2.2e-16 ***\nDiasPascua       -0.0252714  0.0066643  -3.7921 0.0001494 ***\nd0402             0.0332966  0.0130224   2.5569 0.0105619 *  \nd0805             0.0644211  0.0126345   5.0988 3.418e-07 ***\nd0806             0.0283617  0.0126198   2.2474 0.0246149 *  \nd0310             0.0377276  0.0126289   2.9874 0.0028135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#validación-del-modelo",
    "href": "03-11-Ejemplo-Pasajeros.html#validación-del-modelo",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.4 Validación del modelo",
    "text": "5.4 Validación del modelo\nEn el proceso de validación verificaremos si se cumplen las hipótesis básicas sobre el vector de residuos y analizaremos la calidad de ajuste y predicción del modelo estimado.\n\nIncorrelación, Homocedasticidad y Normalidad\nVeamos si el residuo es ruido blanco.\n\nBox.test(error, lag = 2,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 0.020802, df = 2, p-value = 0.9897\n\nBox.test(error, lag = 24,type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error\nX-squared = 25.067, df = 24, p-value = 0.4021\n\nBox.test(error^2, lag = 2, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 3.5249, df = 2, p-value = 0.1716\n\nBox.test(error^2, lag = 24, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  error^2\nX-squared = 25.854, df = 24, p-value = 0.3606\n\njarque.bera.test(error) \n\n\n    Jarque Bera Test\n\ndata:  error\nX-squared = 0.40002, df = 2, p-value = 0.8187\n\n\nEl error muestra ser incorrelado, homocedástico y seguir una distribución normal.\n\n\nCalidad del ajuste\nAnalizando los criterios de bondad de ajuste (sobre el error de predicción intramuestral a un periodo vista) se tiene un error medio (ME) de -0.05 millones de pasajeros, prácticamente cero por lo que no parece que haya sesgo en las predicciones; en media nos equivocamos 3.7 millones de pasajeros (RMSE) y el error porcentual medio (MAPE) es 1.3%, muy bajo. Para ambos indicadores de bondad de ajuste el error obtenido es la mitad que el visto con Alisado exponencial. Al contrario que para Alisado, el ajuste con modelos Arima generará predicciones por intervalo válidas.\n\n\n                ME RMSE  MAE  MPE MAPE MASE ACF1\nTraining set -0.05 3.72 2.92 0.01 1.26 0.35 0.02\n\n\n\n\nCalidad de las predicciones\nSe completará el proceso de validación estimado el error de predicción extramuestral según el horizonte temporal. Se considerarán 12 años para el periodo de estimación y un año para el de predicción.\n\nk &lt;- 144                  \nh &lt;- 12                   \nT &lt;- length(Pasajeros)    \ns&lt;-T - k - h            \n\nmapeArima &lt;- matrix(NA, s + 1, h)\n\nX &lt;- data.frame(cbind(DiasLaborables, DiasNoLaborables, \n                      LunesNavidad, DiasPreSanta, DiasPascua))\n\nfor (i in 0:s) {\n  train.set &lt;- subset(Pasajeros, start = i + 1, end = i + k)\n  test.set &lt;-  subset(Pasajeros, start = i + k + 1, end = i + k + h) \n  \n  X.train &lt;- as.matrix(X[(i + 1):(i + k),])\n  X.test &lt;- as.matrix(X[(i + k + 1):(i + k + h),])\n  \n  fit &lt;- try(Arima(train.set, \n                   lambda = 0,\n                   order = c(0, 1, 1),\n                   seasonal = c(0, 1, 1),\n                   xreg=X.train), \n             silent = TRUE)\n  \n  if (!is.element(\"try-error\", class(fit))) {\n    fcast &lt;- forecast(fit, h = h, xreg = X.test)\n    mapeArima[i + 1,] &lt;- 100*abs(test.set - fcast$mean)/test.set\n  }\n}\n\nerrorArima &lt;- apply(mapeArima, MARGIN = 2, FUN = median, na.rm = TRUE)\nround(errorArima, 2)\n\n [1] 1.09 1.12 1.32 1.52 1.51 1.46 1.66 1.87 2.12 2.02 2.21 2.59\n\n\nEl error es creciente en el horizonte temporal de predicción. Para predicciones extramuestrales a un periodo vista vale 1.1%, asimilar al error de estimación. Incluso para predicciones a doce meses vista, el error sigue siendo reducido, 2.6%. Recordemos que para Alisado exponencial el error a 12 meses vista era de 2.9%, muy similar."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#interpretación-del-modelo-estimado",
    "href": "03-11-Ejemplo-Pasajeros.html#interpretación-del-modelo-estimado",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.5 Interpretación del modelo estimado",
    "text": "5.5 Interpretación del modelo estimado\nEl modelo estimado y validado corresponde al modelo de las aerolíneas con intervención: \\(ARIMA_{12}(0,1,1)(0,1,1) + AI\\). La ecuación teórica completa del modelo es:\n\\[(1-L)(1-L^{12})\\log(Pasajeros) = (1+\\theta_1 L)(1 + \\theta_{12} L^{12})\\varepsilon_t+\\]\n\\[\\gamma_1 DiasLaborables +\\gamma_2 DiasNoLaborables +\\gamma_3 LunesNavidad+\\]\n\\[\\gamma_4 DiasPreSanta + \\gamma_5 DiasPascua +\\]\n\\[\\gamma_6 d0402 +\\gamma_7 d0805 +\\gamma_8 d0806 +\\gamma_9 d0310.\\]\nSi se desarrolla el modelo y se deja en función de la tasa de variación anual del número de pasajero, queda (la parte de intervención no cambia):\n\\[TVAPasajeros_t = TVAPasajeros_{t-1} + \\theta_1 \\varepsilon_{t-1} + \\theta_{12} \\varepsilon_{t-12}+ \\theta_1 \\theta_{12} \\varepsilon_{t-13}+\\varepsilon_t + AI.\\]\nFinalmente, el modelo estimado es:\n\\[\\widehat{TVAPasajeros}_t = TVAPasajeros_{t-1} -0.53 \\varepsilon_{t-1} -0.37 \\varepsilon_{t-12}+ 0.20 \\varepsilon_{t-13} +\\]\n\\[0.034\\cdot DiasLaborables +0.016\\cdot DiasNoLaborables+ 0.026\\cdot LunesNavidad\\]\n\\[- 0.058\\cdot DiasPreSanta - 0.025\\cdot DiasPascua +\\]\n\\[0.033\\cdot d0402 +0.064\\cdot d0805 +0.028\\cdot d0806 +0.038\\cdot d0310.\\] Interpretación:\n\nLa tasa de variación anual del número de pasajeros en transporte urbano para un mes dado es la misma que la observada en el mes previo.\nSi hace uno, doce o trece meses se observó un número atípico de pasajeros, se debe tener en cuenta para corregir la predicción.\nCada día laborable adicional en un mes supone un incremento en el número de pasajeros del 3.4% y cada día no laborable un incremento adicional del 1.6%.\nSi la Navidad cae en lunes y por tanto Nochebuena en domingo, ese mes de diciembre el número de pasajeros será un 2.6% superior al de un mes de diciembre donde la Navidad no cae en lunes.\nEl mes (marzo o abril) en que caigan los días laborables (lunes a jueves) de la Semana Santa el número de pasajeros se reducirá un 5.8% respecto de lo esperado.\nDe la misma forma, el mes (marzo o abril) en que caigan los días laborables (lunes a viernes) de la semana posterior a Domingo de Resurrección el número de pasajeros se reducirá un 2.5% respecto de lo esperado.\nAdemas, para cuatro meses se observó una tasa de variación anual atípicamente superior a la esperada."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#predicción-del-número-de-pasajeros-en-transporte-urbano",
    "href": "03-11-Ejemplo-Pasajeros.html#predicción-del-número-de-pasajeros-en-transporte-urbano",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "5.6 Predicción del número de pasajeros en transporte urbano",
    "text": "5.6 Predicción del número de pasajeros en transporte urbano\nUna vez dado por válido el modelo, podemos pasar a realizar predicciones para los próximos años. Para las variables de intervención sujetas a fecha de calendario ya hemos ido creando sus valores previstos, para las demás los fijaremos a cero.\n\npPasajerosAri &lt;- forecast(PasajerosAri, \n                          h = 60,\n                          xreg = cbind(pDiasLaborables, pDiasNoLaborables, \n                                       pLunesNavidad, pDiasPreSanta, pDiasPascua,\n                                       rep(0, 60), rep(0, 60), \n                                       rep(0 ,60), rep(0, 60)), \n                          level = 95)\n\nautoplot(pPasajerosAri, \n         xlab = \"\",\n         ylab = \"\",\n         main = \"\") +\n  scale_x_continuous(breaks= seq(1996, 2024, 4))\n\n\n\n\n\n\n\nFigura 16: Pasajeros (1996-2019) y predicción (2020-2024). Modelo Arima\n\n\n\n\n\nAsí, en 2020 se esperan 3180 millones de pasajeros y para 2021 un total de 3247 millones de pasajeros."
  },
  {
    "objectID": "03-11-Ejemplo-Pasajeros.html#footnotes",
    "href": "03-11-Ejemplo-Pasajeros.html#footnotes",
    "title": "Series Temporales: Análisis de la serie Pasajeros en transporte urbano",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRecordemos que hemos recortado la serie hasta 2019 y vivimos en un mundo donde nada sabemos de la Covid-19.↩︎\nRealmente en 2020 el número de Pasajeros fue de 1682 millones, un 45% menos que en 2019.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Iván Arribas",
    "section": "",
    "text": "Iván Arribas es licenciado en Ciencias Matemáticas con la especialidad de Estadística e Investigación Operativa, y doctor en Ciencias Económicas. Actualmente es Profesor Titular en el Departamento de Análisis Económico de la Universitat de València, donde lleva más de 25 años como docente e investigador.\nIván Arribas es, también, investigador del Instituto Valenciano de Investigaciones Económicas (Ivie) y de la Estructura de Investigación Interdisciplinar Comportamiento Económico-Social (ERI CES).\nAdemás, estuvo más de 10 años como director del área de Análisis de Datos en las empresas de investigación de mercados Idesa y Eratema."
  },
  {
    "objectID": "04-02-Multiples_CS.html#descomposición",
    "href": "04-02-Multiples_CS.html#descomposición",
    "title": "Múltiples componentes estacionales",
    "section": "Descomposición",
    "text": "Descomposición\nPodemos descomponer la serie de forma análoga a como se hacia para series con una componente estacional usando la función mstl.\n\ndescomposicion &lt;- mstl(electricidad)\nautoplot(descomposicion) \n\n\n\n\n\n\n\nFigura 2: Descomposición de Consumo eléctrico por hora\n\n\n\n\n\nEn la Figura 2 aparecen los paneles de datos originales, tendencia y residuo, más dos paneles correspondientes a las dos componentes estacionales, de orden 24 y 168 (Seasonal24 y Seasonal168).\nPara poder interpretar adecuadamente cada serie de descomposición hay que fijarse en la escala de los ejes verticales. La tendencia apenas cambia en el periodo de análisis. Las dos componentes estacionales oscilan sobre un rango de valores mayor. Dentro de cada día el consumo de electricidad oscila aproximadamente 20 GW entre las horas pico y las valle (Seasonal24): en los picos se consumen unos 7.5 GW más que la media diaria, y en los valles unos 12.5 GW menos que la media diaria. Por otro lado, el rango de variación semanal en el consumo también es de aproximadamente 20 GW (Seasonal168): de lunes a viernes se consumen como máximo unos 5 GW más que la media semanal y los domingo unos 15 GW menos.\nVamos a mostrar un detalle de la componente estacional diaria, semanal y su composición para una semana cualquiera de la serie (véase Figura 3).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Componente estacional diaria\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Componente estacional semanal\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Componente estacional diaria + semanal\n\n\n\n\n\n\n\nFigura 3: Componentes estacionales para Consumo eléctrico\n\n\n\n\nEl panel a de la Figura 3 muestra la estacionalidad de orden 24 diaria, que se repite de forma sistemática cada día de la semana de lunes a domingo. Cada día el consumo de electricidad tiene dos momentos de máximo consumo y otros dos de bajo consumo relativo. El panel b muestra la estacionalidad semanal de orden 168. Aunque solo se muestra una semana, este patrón se repetiría de forma sistemática semana tras semana. Se observa un elevado consumo eléctrico las 120 primeras horas de la semana (lunes a viernes), un caída en el consumo de la hora 121 a la 144, que corresponde al sábado, y un descenso más acusado del consumo las últimas 24 horas de la semana, el domingo. El panel c combina ambas componentes estacionales de forma aditiva para mostrar el patrón estacional completo del consumo eléctrico una semana cualquiera."
  },
  {
    "objectID": "04-02-Multiples_CS.html#predicción-a-partir-de-la-descomposición-y-alisado-exponencial",
    "href": "04-02-Multiples_CS.html#predicción-a-partir-de-la-descomposición-y-alisado-exponencial",
    "title": "Múltiples componentes estacionales",
    "section": "Predicción a partir de la descomposición y Alisado exponencial",
    "text": "Predicción a partir de la descomposición y Alisado exponencial\nExisten varios métodos para poder estimar series con estacionalidad múltiple. Uno de los más sencillos consiste en descomponer de la serie. Después, predecir las componentes estacionales por simple repetición y predecir la componente de la tendencia usando Alisado exponencial. En último lugar, se combinan la predicción de la tendencia con las predicciones de las estacionalidades para obtener una predicción de la serie.\nLa función stlf hace todas estas operaciones de forma automática. Por defecto la tendencia se predice usando Alisado exponencial (“ets”), pero con el argumento method se pueden especificar otros modelos alternativos, “arima”, “naive” o “rwdrift”.\nEn la Figura 4 se muestra el resultado de aplicar stlf a la serie de consumo eléctrica. La línea negra representa la serie y la línea azul su predicción para las dos semanas siguientes (dos primeras semanas de marzo de 2021). El título por defecto de la figura indica que la tendencia de la serie se ha ajustado usando la función ets y presenta una pendiente aditiva amortiguada y error multiplicativo.\n\npdatos_stfl &lt;- stlf(electricidad)\n\nautoplot(pdatos_stfl, PI = FALSE) + \n  ylab(\"GW\") + \n  xlab(\"Semanas\") + \n  scale_x_continuous(breaks = seq(1, 7, by = 1))\n\n\n\n\n\n\n\nFigura 4: Consumo eléctrico y predicción. Descomposición + Alisado"
  },
  {
    "objectID": "04-02-Multiples_CS.html#predicción-a-partir-de-asilado-exponencial-y-series-de-furier",
    "href": "04-02-Multiples_CS.html#predicción-a-partir-de-asilado-exponencial-y-series-de-furier",
    "title": "Múltiples componentes estacionales",
    "section": "Predicción a partir de Asilado Exponencial y series de Furier",
    "text": "Predicción a partir de Asilado Exponencial y series de Furier\nUno de los inconvenientes del método visto es que estima todos los elementos de cada componente estacional (\\(24 + 168\\) elementos en nuestro ejemplo) como si fueran independientes, sin tener en cuenta que, por lo general, evolucionan siguiendo una suave curva. Véase en el panel (a) de la Figura 3 la curva que sigue la estacionalidad diaria, donde la componente de una hora determinada está muy relacionada con la componente de la hora precedente y posterior.\nAlgunos métodos alternativos de predicción usan la dependencia observada entre los elementos de una componente estacional para ajustarlos a una curva paramétrica, por ejemplo funciones trigonométricas o series de Fourier.\nEntre los métodos que usan funciones trigonométricas está el implementado en Livera, Hyndman, and Snyder (2011). El método de estimación que emplean estos autores es complejo, requiere tiempo de computación y no siempre el ajuste obtenido es el más adecuado, así que su resultado puede ser en ocasiones muy malo.\nVeamos un ejemplo de la implementación de este método con la función tbats. La Figura 5 muestra la predicción para dos semanas.\n\ntmp &lt;- Sys.time()\n\ndatos_tbats &lt;- tbats(electricidad)\n\npdatos_tbats &lt;- forecast(datos_tbats, \n                         h = 14 * 24,\n                         level = 95)\n\nautoplot(pdatos_tbats, PI = FALSE) + \n  ylab(\"GW\") + \n  xlab(\"Semanas\") + \n  scale_x_continuous(breaks = seq(1, 7, by = 1))\n\n(tiempo &lt;- Sys.time() - tmp)\n\nTime difference of 33.67035 secs\n\n\n\n\n\n\n\n\nFigura 5: Consumo eléctrico y predicción. Ajuste de las componentes estacionales por funciones trigonométricas\n\n\n\n\n\n\n\nEl título de la figura indica el modelo TBATS estimado. Su interpretación precisa la podéis encontrar en Livera, Hyndman, and Snyder (2011). Observa que la estimación del modelo ha requerido de 34 segundos.\nEntre los métodos que involucran series de Fourier una propuesta reciente es el modelo Prophet, disponible a través del paquete fable.prophet. Este modelo fue introducido por Facebook (Taylor and Letham (2018)) originalmente para pronosticar datos diarios con estacionalidad semanal y anual, además de efectos calendario. Posteriormente se amplió para cubrir más tipos de datos con estacionalidad."
  },
  {
    "objectID": "04-04-Series_acotadas.html#predicciones-positivas",
    "href": "04-04-Series_acotadas.html#predicciones-positivas",
    "title": "Series acotadas",
    "section": "Predicciones positivas",
    "text": "Predicciones positivas\nPara imponer que las predicciones sean positivas basta trabajar con la transformación logarítmica. Por ejemplo, consideremos la serie anual de nacimientos desde 1975 hasta 2019. Vamos ha realizar predicciones a muy largo plazo (30 años) usando Alisado de Holt con y sin transformación logarítmica.\nEn el panel superior de la Figura 1, donde se ha usado la transformación logarítmica, no solo la predicción, sino también el intervalo es siempre positivo. Por el contrario, en el panel inferior de la Figura 1, donde no se ha usado la transformación logarítmica, las predicciones a partir de 2042 ya son negativas y el extremo inferior del intervalo de confianza es negativo desde el año 2028.\n\nConLog &lt;- forecast(ets(nacimientos,\n                       model = \"AAN\", \n                       damped = FALSE, \n                       lambda = 0),\n                   h = 30,\n                   level = 95)\n\nSinLog &lt;- forecast(ets(nacimientos, \n                       model = \"AAN\", \n                       damped = FALSE),\n                   h = 30,\n                   level = 95)\n\nautoplot(ConLog, main = \"\", xlab = \"\", ylab = \"Nacimientos (miles)\")\n\nautoplot(SinLog, main = \"\", xlab = \"\", ylab = \"Nacimientos (miles)\") + \n  geom_hline(yintercept=0, linewidth = .3, linetype = 2)\n\n\n\n\n\n\n\n\n\n\n\n(a) Con transformación logarítmica\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Sin transformación logarítmica\n\n\n\n\n\n\n\nFigura 1: Ajuste y predicción de Nacimientos con Holt"
  },
  {
    "objectID": "04-04-Series_acotadas.html#predicciones-dentro-de-un-intervalo",
    "href": "04-04-Series_acotadas.html#predicciones-dentro-de-un-intervalo",
    "title": "Series acotadas",
    "section": "Predicciones dentro de un intervalo",
    "text": "Predicciones dentro de un intervalo\nSupongamos que el valor de la serie es un porcentaje y que debe estar comprendido entre \\(a = 0\\) y \\(b = 100\\), como por ejemplo la serie anual consistente en el porcentaje de nacimientos de mujeres con nacionalidad española. La transformación que garantiza que las predicciones se mantendrán dentro del intervalo \\([a,\\;b]\\) es\n\\[z_t = \\log\\Big(\\frac{y_t - a}{b - y_t}\\Big),\\] donde \\(y_t\\) es la serie original y \\(z_t\\) la serie transformada. Una vez tenemos las predicciones de la serie \\(z_t\\), tenemos que deshacer la transformación con\n\\[y_t = \\frac{a +b\\, e^{z_t}}{1 + e^{z_t}}.\\]\nEn este caso no hay un argumento lambda que nos facilite el trabajo y hay que escribir más código.\n\na &lt;- 0\nb &lt;- 100\n\nz &lt;- log((serie - a) / (b - serie))\n\nmodelo &lt;- ets(z, \n              model = \"AAN\", \n              damped = FALSE)\n\npz &lt;- forecast(modelo, \n               h = 30,\n               level = 95)\n\npz[[\"mean\"]] &lt;-  (a + b * exp(pz[[\"mean\"]]) ) / (1 + exp(pz[[\"mean\"]]))\npz[[\"lower\"]] &lt;- (a + b * exp(pz[[\"lower\"]])) / (1 + exp(pz[[\"lower\"]]))\npz[[\"upper\"]] &lt;- (a + b * exp(pz[[\"upper\"]])) / (1 + exp(pz[[\"upper\"]]))\npz[[\"x\"]] &lt;- serie\n\nautoplot(pz, \n         main = \"\",\n         xlab = \"\",\n         ylab = \"Nacimientos de mujeres españolas (%)\")\n\n\n\n\n\n\n\nFigura 2: Predicción con Holt. Valores acotados entre 0% y 100%\n\n\n\n\n\nHemos solicitado una previsión a 30 años vista para poder ver mejor el efecto de acotar la serie. En la Figura 2 se observa que no solo la predicción, sino también el intervalo está siempre entre 0% y 100%."
  },
  {
    "objectID": "05-Recursos-R.html",
    "href": "05-Recursos-R.html",
    "title": "Series Temporales",
    "section": "",
    "text": "Durante el curso usaremos diferentes ficheros para los datos de los ejemplos y para el código en R. Desde esta página puedes descargarte todo el material\n\n\n\n\nFicheros de datos\n\nResiduos recogidos: Residuos recogidos por o en nombre de las autoridades municipales y eliminados a través del sistema de gestión de residuos (fuente Instituto Nacional de Estadística). Es una serie anual de 1995 a 2023.\nNacimientos: Nacimientos en España (fuente Instituto Nacional de Estadística). Serie mensual de enero de 1975 a junio de 2025.\nConsumo eléctrico: Consumo eléctrico en España en GWh (fuente Red Eléctrica de España). Es una serie diaria desde el 1 de enero de 2024 hasta el 31 de diciembre de 2024.\nConsumo eléctrico por hora: Consumo eléctrico en España en GW (fuente Red Eléctrica de España). Es una serie horaria desde las 00:00 horas del 1 de febrero de 2021 hasta las 23:00 horas del 28 de febrero de 2021.\nAforo de vehículos en Oropesa: Aforo de vehículos por Oropesa (número medio de vehículos por día), carretera N-340, km. 996,48 (fuente Ministerio de Fomento). La serie es anual de 1960 a 2024.\nConsumo de alimentos per cápita: Consumo alimentario en hogar per cápita en España. Esta serie está construida a partir de la serie de consumo alimentario en hogar (fuente Ministerio de Agricultura, Alimentación y Medio Ambiente), y la serie de población (fuente Instituto Nacional de Estadística). Es una serie anual de 1990 a 2024 y la unidad es el kg per cápita.\nExportaciones de España a la UE-27: Volumen de exportaciones de bienes, en millones de euros, desde España hacía la UE-27 (conjunto de 27 países de la Unión Europea), desde enero de 1999 hasta julio de 2025 (fuente Eurostat).\nExportaciones de España a la UE-27 de productos químicos: Volumen de exportaciones de productos químicos, en millones de euros, desde España hacía la UE-27 (conjunto de 27 países de la Unión Europea), desde enero de 1999 hasta diciembre de 2020 (fuente Eurostat).\nDefunciones por enfermedades cerebrovasculares: Número de defunciones causadas por enfermedades cerebrovasculares (fuente Instituto Nacional de Estadística). Es una serie mensual de enero de 1980 a diciembre de 2024.\nPasajeros en transporte urbano: Número de pasajeros en transporte urbano en España (fuente Instituto Nacional de Estadística). Serie mensual de enero 1996 a julio de 2025.\nTemperatura: Temperatura media diaria en España medida en el aeropuerto de Madrid (fuente Agencia Estatal de Meteorología, AEMET). Serie diaria del 1 enero de 2024 al 31 de diciembre de 2024.\n\n\n\n\nTodos los ficheros de datos en un único fichero comprimido\n\n\n\n\n\n\nCódigo R\nTienes el código para los temas de teoría aquí:\n\nCódigo para el tema 1\nCódigo para el tema 2\nCódigo para el tema 3\nCódigo para el tema 4\nCódigo para el tema 5\n\n\n\nPara el ejemplo de Defunciones por enfermedades cerebrovasculares tienes el código aquí:\n\nCódigo para el ejemplo del tema 1\nCódigo para el ejemplo del tema 2\nCódigo para el ejemplo del tema 3\nCódigo para el ejemplo del tema 4\nCódigo para el ejemplo del tema 5\n\nY para el ejemplo de Pasajeros en transporte urbano aquí.\n\n\n\nTodos los ficheros de código en un único fichero comprimido"
  },
  {
    "objectID": "04-03-Redes_neuronales.html",
    "href": "04-03-Redes_neuronales.html",
    "title": "Autorregresión con redes neuronales",
    "section": "",
    "text": "1 Antecedentes\nEn las dos grandes familias de modelos que permiten ajustar y predecir series temporales –Alisado exponencial y modelos Arima– se ajusta un modelo a una serie temporal y el resultado del ajuste nos permite no solo predecir, sino aprender y entender el comportamiento de la serie. Por ejemplo, el resultado del ajuste por alisado nos permite saber si la pendiente de la serie cambia en el tiempo (parámetro \\(\\beta\\) del ajuste) o el tipo de esquema de la serie según que la estacionalidad sea aditiva o multiplicativa. Con los modelos Arima podemos estimar el impacto en la serie de un efecto calendario (Semana Santa, días laborables…).\nOtra familia de modelos muy versátiles que permiten predecir con todo tipo de datos –transversales, series temporales, imágenes, espacio-temporales…– son las redes neuronales. Estos modelos son el embrión del Deep Learning y el motor de muchas AI.\nVamos a ver muy, pero que muy por encima en que consisten las redes neuronales y como se pueden aplicar para predecir series temporales. Esto es una pequeña píldora.\n\n\n2 Arquitectura de una red neuronal de una capa\nUna red neuronal puede ser entendida como una red de neuronas dispuestas en capas. Siempre hay una capa de entrada de los datos y una capa de salida de la respuesta. Entre estas dos capas se puede disponer de tantas capas intermedias (ocultas) como se considere necesario.\nCada capa está formada por un número determinado y potencialmente diferente de neuronas o nodos. Los nodos de una capa están conectadas a los nodos de la siguiente. Por simplicidad asumiremos que todos los nodos de una capa se conectan con los nodos de la capa siguiente.\nAquí vamos a considerar solo redes neuronales con una capa intermedia y donde la capa de salida tiene solo una neurona. La Figura 1 es un ejemplo de este tipo de redes neuronales.\n\n\n\n\n\n\n\n\nFigura 1: Red neuronal con una sola capa intermedia\n\n\n\nEn esta red cada nodo de una capa recibe entradas de los nodos de la capa previa. Dicho de otra forma, las salidas de los nodos de una capa son las entradas de los nodos de la siguiente capa. Es lo que se denomina una multilayer feed-forward network.\nLas entradas que recibe cada nodo se combinan usando una función lineal ponderada. Por ejemplo, un nodo \\(j\\) de la capa intermedia recibe las dos entradas \\(x_1\\) y \\(x_2\\) de los nodos de la capa de entrada y los combina linealmente\n\\[z_j = b_j + \\sum_{i=1}^2 w_{i,j}x_i\\] Para los nodos de la capa intermedia el valor \\(z_j\\) se transforma usando una función no lineal, por ejemplo la sigmoidea:\n\\[s_j = \\frac{1}{1 + e^{-z_j}}\\] y este valor \\(s_j\\) es la salida del nodo \\(j\\) que va al nodo de la capa de salida.\nLos valores de los pesos \\(b_1\\), \\(b_2\\), \\(w_{1,1}\\), \\(w_{1,2}\\)…\\(w_{2,5}\\) se deben ajustar a partir de los datos. Estos valores suelen estar restringidos para evitar que sean demasiado grandes. El parámetro que restringe las ponderaciones se conoce como parámetro de decaimiento, y suele ser igual a \\(0.1\\).\nLos pesos toman valores aleatorios al principio y luego se actualizan con los datos observados en un proceso de aprendizaje. Por lo tanto, hay un elemento de aleatoriedad en las predicciones producidas por una red neuronal. Por este motivo, la red suele entrenarse varias veces utilizando diferentes puntos de partida aleatorios, y los resultados se promedian.\n\n\n3 Autoregresión de redes neuronales\nEn el contexto de series temporales, los valores de entrada pueden ser valores retardados de la serie y el valor de salida deseado el valor contemporáneo. De la misma forma que en un modelo AR usamos los datos pasados para predecir el futuro.\n\n\n\n\n\n\nFigura 2: Red neuronal para predecir una serie temporal. El dato del periodo \\(t\\) se predice a partir de los dos datos previos.\n\n\n\nVamos a extender estas ideas e ir añadiendo algo de notación.\nEstamos considerando solo redes simples con una capa intermedia y una capa de salida de un solo nodo, que denominaremos \\(NNAR\\). Para una serie sin estacionalidad, la notación \\(NNAR(p, k)\\) indica que hay \\(p\\) valores desfasados en la capa de entrada y \\(k\\) nodos en la capa intermedia. Por ejemplo, la red de la Figura 2 es modelo \\(NNAR(2,5)\\), donde \\(y_{t-1}\\) e \\(y_{t-2}\\) son usados para predecir \\(y_t\\). Así, un modelo \\(NNAR(p, 0)\\) sería equivalente a un modelo \\(ARIMA(p,0,0)\\).\nSi la serie tiene estacionalidad es conveniente que entre los datos de entrada estén las observaciones pasadas de la misma estación que se desea predecir. Tenemos que ampliar un poco nuestro modelo \\(NNAR\\) para indicar los valores desfasados estacionales de la capa de entrada. Por ejemplo, para la serie diaria de consumo eléctrico un modelo \\(NNAR(2, 1, k)_7\\) usaría como datos de entrada \\(y_{t-1}\\), \\(y_{t-2}\\) e \\(y_{t-7}\\) para predecir \\(y_t\\). En general, \\(NNAR(p,P,k)_m\\) usa como datos de entrada \\(y_{t-1}\\),\\(y_{t-2}\\),…,\\(y_{t-p}\\),\\(y_{t-m}\\), \\(y_{t-2m}\\),…,\\(y_{t-Pm}\\) y una capa intermedia de \\(k\\) neuronas. Por lo tanto, \\(NNAR(p,P,0)_m\\) es equivalente a \\(ARIMA(p,0,0)(P,0,0)_m\\).\n\n\n4 Aplicación\nLa función nnetar de la librería forecast permite estimar modelos \\(NNAR(p,P,k)_m\\). En su forma más sencilla el usuario no tiene que especificar los valor de los parámetros \\(p\\), \\(P\\) y \\(k\\) ya que la función los identifica según ciertos criterios.\nLa siguiente gráfica muestra el consumo eléctrico en España en GWh para 17 semanas desde febrero hasta mayo de 2024. Hay una fuerte componente estacional diaria de orden \\(7\\), donde el consumo es alto de lunes a viernes, algo mas reducido el sábado y aún menor el domingo.\n\nelectricidad &lt;- read.csv(\"./series/Consumo electrico.csv\", \n                         header = TRUE)\n\nelectricidad &lt;- ts(electricidad[, 1], \n                   start = c(1, 1),\n                   frequency = 7)\n\nelectricidad &lt;- window(electricidad,\n                       start = c(6, 1),\n                       end = c(22, 7))\n\nautoplot(electricidad) + \n  ggtitle(\"\") +\n  ylab(\"GWh\") + \n  xlab(\"Semana\")\n\n\n\n\n\n\n\nFigura 3: Consumo eléctrico (febrero a mayo de 2024)\n\n\n\n\n\nLa Figura 4 muestra la serie y su predicción para los siguientes 14 días. El modelo ajustado es \\(NNAR(3,1,2)_7\\). Es decir, la capa de entrada tiene 4 nodos porque para predecir el consumo del día \\(t\\), \\(y_t\\), se usa el consumo de los tres días previos \\(y_{t-1}\\), \\(y_{t-2}\\) y \\(y_{t-3}\\), y el consumo de hace una semana \\(y_{t-7}\\). La capa intermedia tiene dos nodos.\n\nfit &lt;- nnetar(electricidad)\n\naccuracy(fit)\n\n                      ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.008059542 20.07986 14.77356 -0.1144686 2.259785 0.6683302\n                  ACF1\nTraining set 0.2209664\n\n\nEl error de ajuste es de 2.3% o 21 GWh y el ajuste no presenta sesgo. Por otro lado, el ACF1 muestra una elevada autocorrelación de orden 1. Sin embargo, esto no afecta a la calidad de la predicción por intervalos porque con redes neuronales no se utiliza una fórmula cerrada que precise de la hipótesis de incorrelación. Se utiliza un proceso complejo y costoso temporalmente.\n\ntiempo &lt;- Sys.time()\n\npfit &lt;- forecast(fit, \n                 h = 14,\n                 level = 95,\n                 PI = TRUE)\n\nSys.time() - tiempo\n\nTime difference of 13.08543 secs\n\nautoplot(pfit) +\n  ylab(\"GWh\") + \n  xlab(\"Semana\")\n\n\n\n\n\n\n\nFigura 4: Consumo eléctrico (febrero a mayo de 2024) y predicción\n\n\n\n\n\n\npfit\n\n         Point Forecast    Lo 95    Hi 95\n23.00000       661.0622 622.7904 700.5708\n23.14286       677.0893 637.7800 720.9458\n23.28571       678.7566 606.1524 744.9179\n23.42857       681.0452 611.4127 749.9405\n23.57143       671.9841 599.3398 738.4751\n23.71429       608.6991 545.4929 697.7460\n23.85714       563.6131 520.8266 672.7672\n24.00000       662.0676 576.7760 698.7945\n24.14286       677.0331 617.3587 718.6242\n24.28571       677.7435 595.4526 741.7868\n24.42857       677.5963 569.9730 747.3442\n24.57143       669.2573 568.4686 739.4775\n24.71429       604.4866 540.0289 723.3689\n24.85714       560.7779 521.5164 700.5539\n\n\nLa función nnetar admite la inclusión de variables de intervención de la forma usual a través del argumento xreg."
  }
]